\chapter{Planteamiento del proyecto}

En esta capítulo se expone cuál ha sido el planteamiento incial del proyecto, un análisis de tiempos, las tecnologías y equipos utilizados y el hardaware que forma parte del proyecto en sí mismo.

\section{Planteamiento preliminar}
El robot sobre el que se ha trabajado es el Pioneer 3 AT, de la empresa Adept Mobile Robots, cuyas características se detallarán más adelante. La configuración del sistema motriz es de tipo skid-steer y será determinante a la hora de realizar el control del desplazamiento.

Para realizar la teleoperación del robot, se han utilizado las herramientas de comunicación de ROS, que hacen que la ejecución de los diferentes nodos de forma distribuida entre equipos se realice de forma transparente para el usuario. Con esta característica es posible desarrollar con facilidad un sistema de telecontrol sin preocuparnos en exceso por la implementación de la comunicación entre equipos.

Para la navegación se pretende que el robot base sus movimientos en un sistema reactivo, es decir, que el robot base su navegación principalmente en la información captada por sus sensores y no en un mapa preestablecido. El control en navegación del robot se basa en la funcionalidad ''Navigation Stack'' de ROS, que también será explicada en detalle más adelante.

El desarrollo principal para la navegación utiliza la información aportada por el sensor Kinect y el sensor láser, ya que ambos proporcionan información muy valiosa debido a sus diferentes características.

Seguidamente, se han realizado los ajustes pertinentes en la navegación del robot, para la cual se ha seguido el concepto de mapas de coste y descomposición en mapas de celdillas. También se ha valorado la disposición de ambos sensores para capturar el entorno, así como el tratamiento dispar de los datos capturados por cada uno de ellos. De esta forma logramos que no se produzcan detecciones de objetos de manera duplicada y que no haya discrepancias entre los obstáculos que detecta un sensor respecto al otro \footnote{De especial interés en la incorporación de obstáculos al mapa mediante el uso de Costmaps en el sistema de navegación de ROS}.

Finalmente, se han incorporado características adicionales que aportan valor al desarrollo del proyecto, como el uso del simulador Gazebo o la interacción con el robot mediante comandos de voz y sintetizado de voz.

\section{Planificación del proyecto}

En este apartado se desarrollan las fases por las que ha pasado este proyecto y realizaremos un análisis de tiempos.\\

\textbf{Fases de desarrollo:}
\begin{itemize}
\item Fase inicial: Familiarización con el entrono ROS y elección de herramientas.

\begin{enumerate}[i.]
  \item Herramientas proporcionadas por ROS para evaluar los datos que pueda manejar el robot: \textit{RViz}, \textit{rqt\_grapth}, \textit{map\_server}, \textit{Topics}...
  
  \item Para el control del movimiento del robot se ha utilizado el nodo RoSAria debido a sus posibilidades.
  
  \item Para acceder a la información de la Kinect se utilizan los drivers \textit{libfreenect} por ser librerías de código libre integradas en ROS y de las cuales se ha hecho uso extenso en otros proyectos.
\end{enumerate}

\item Segunda fase: Realización del nodo de teleoperación y comunicación entre equipos conectados a la misma red.

\begin{enumerate}[i.]
  \item Configuración de equipos en red para acceder a la información publicada por nodos que se ejecuten en varias máquinas \cite{network-ros}.
  
  \item Partiendo del nodo de teleoperacion de \textit{turtlesim} \cite{turtlesim}, se ha realizado un nodo similar para nuestro robot.
\end{enumerate}

\item Tercera fase: Incorporación de los sensores al robot y acceso a los datos.

\begin{enumerate}[i.]
  \item Para el sensor Kinect, se ha realizado un adaptador para conectarlo a la alimentación del robot. Utilizando el nodo \textit{freenect\_stack} \cite{freenect}, accedemos a la nube de puntos y la imagen.
  
  \item Se ha utilizado el nodo \textit{LMS1xx} \cite{lms1xx} para la puesta en marcha del láser y el acceso a los datos.
  
  \item Incorporación de sistemas de referencia \textit{"base\_link"}, \textit{"laser"}, \textit{''camera\_link''} y sus transformadas mediante el paquete \textit{tf} \cite{tf}. 
  
  \item Visualización del conjunto de datos junto con los ejes de referencia en \textit{RViz}.
\end{enumerate}

\item Cuarta fase: Incorporación del sistema de navegación y ajuste de los parámetros

\begin{enumerate}[i.]
  \item Calibrado de los encoders de las ruedas del robot y ajuste de la odometría mediante \textit{RosAria}.
  
  \item Incorporación del sistema de navegación ROS de forma básica.
  
  \item Navegación utilizando el sensor Kinect y el sensor Sick y generado de mapas mediante SLAM.
  
  \item Ajuste de los planificadores de trayectoria del robot y parámetros de giro y control.
\end{enumerate}

\item Quinta fase: Ajuste de la navegación y simulación mediante \textit{Gazebo}.

\begin{enumerate}[i.]
  \item Navegación en modo global (utilizando un mapa guardado) y en modo local (completamente reactivo).
  
  \item Puesta en marcha del simulador Gazebo y configuración del robot en el entorno.
\end{enumerate}

\item Sexta fase: Remodelado de la estructura del robot
\begin{enumerate}[i.]
  \item Disposición de los sensores de manera óptima e integración de los mismos.
  \item Remodelado de la estructura física del robot.
  \item Incorporación del ordenador compacto Intel NUC.
\end{enumerate}

\item Séptima fase: Nuevas funcionalidades y toma de datos.

\begin{enumerate}[i.]
  \item Incorporación de la funcionalidad de guiado (follower), adaptada a partir del robot \textit{Turtlebot}.
  
  \item Interfaz de comandos por voz y sintetizador de texto a voz.
  
  \item Pruebas reales, recogida y análisis de los datos.
\end{enumerate}
\end{itemize}

\textbf{Análisis de tiempos:}

Este proyecto fin de grado comenzó en Noviembre de 2014 y terminó en Febrero de 2015.

Durante el primer mes de Noviembre se estuvo recopilando información sobre ROS y su funcionamiento, los desarrollos existentes aplicados a robots reales y la filosofía del sistema.

En el mes de Diciembre se comenzó a trabajar con el robot, comprobando que todos los elementos se encontraban en correcto funcionamiento y se instaló el sistema operativo en su ordenador de abordo

Durante el mes de Enero se pudo avanzar menos debido a los exámenes y trabajos de las últimas asignaturas.

En el mes de Febrero se retomó el trabajo, empezando por una primera toma de contacto con la librería Aria y la ejecución de movimientos desde un ordenador externo conectado vía puerto serie.

Durante el mes de Marzo, el robot comenzó a funcionar con ROS, realizando los primeros movimientos con control por teclado. Seguidamente se realizó el nodo de telecontrol y un nodo para realizar movimientos basados tan solo en la odometría.

En el mes de Abril, se comenzaron a probar la compatibilidad con ROS de la cámara Kinect y el sensor Láser. Acto seguido, comenzaron las primeras pruebas de navegación autónoma.

En el mes de Mayo se realizaron las pruebas de navegación con el portátil incorporado en el cuerpo del robot. Durante ese mes se realizaron diferentes ajustes de navegación así como mapas mediante SLAM.

En los meses de Junio y Julio, siguieron los ajustes en la navegación, tanto en el planificador de trayectoria como en los mapas de coste, así como en el sensor Kinect para la detección de obstáculos a diferente altura. Además se incorporó la funcionalidad de seguimiento.

En Julio también comenzaron las primeras pruebas de comandos de voz y la sintetización de voz.

Durante ese mes y el mes de Agosto, se comenzó a redactar gran parte del trabajo en esta memoria, donde se organizó la estructura del proyecto y la información a incluir.

En el mes de Septiembre se decidió incorporar un ordenador más potente al robot y reestructurar su chasis para dejar el sistema desarrollado integrado de manera permanente. También se utilizó el array de micrófonos del sensor Kinect para los comandos de voz.

Durante el mes de Octubre se organizó la estructura del proyecto y se puso en marcha el simulador Gazebo. A continuación se realizó el ajuste de los sensores y la optimización del sistema de navegación. En paralelo se realizaron las modificaciones mecánicas y estructurales para la integración de los sensores y el ordenador en el robot.

Durante el mes de Noviembre se realizó un pequeño parón a nivel de software, se continuó con la parte mecánica y con la redacción de la memoria de este proyecto.

A continuación, comenzaron a realizarse las pruebas reales con la nueva configuración en el robot y la redacción de la memoria.

A continuación se muestra un diagrama de Gantt (Figura \ref{fig:gantt}) con el análisis de tiempos de las diferentes tareas.
\vspace{15 cm}
\pagebreak

\begin{figure}[H]
\centering
\includegraphics[width=0.95\textwidth]{figuras/gantt.png}
\caption{Diagrama de Gantt de la evolución del proyecto.}
\label{fig:gantt}
\end{figure}

\pagebreak

\section{Tecnologías y herramientas empleadas en el proyecto}

En esta sección se describen tanto las tecnologías como las herramientas utilizadas en el desarrollo del proyecto. 

\subsection{Robot Operating System}

El Sistema Operativo Robótico \cite{ROS} (conocido en inglés como Robot Operating System o ROS) es un framework para el desarrollo de software para robots que provee la funcionalidad de un sistema operativo \cite{quigley2009ros}. ROS fue desarrollado originalmente en 2007 por el Laboratorio de Inteligencia Artificial de Stanford para dar soporte al proyecto del Robot con Inteligencia Artificial de Stanford \cite{stair}. Desde 2008, el desarrollo continua principalmente en Willow Garage, un instituto de investigación robótico con más de veinte instituciones que colaboran conjuntamente.

ROS provee los servicios estándar de un sistema operativo como abstracción del hardware, control de dispositivos de bajo nivel, implementación de funcionalidad de uso común, paso de mensajes entre procesos y mantenimiento de paquetes. Está basado en una arquitectura de nodos interconectados que pueden mandar, recibir y multiplexar mensajes de sensores, control, estados, planificaciones y actuadores, entre otros. La librería está orientada para un sistema UNIX (Ubuntu (Linux)) aunque también se está adaptando a otros sistemas operativos como Fedora, Mac OS X, Arch, Gentoo, OpenSUSE, Slackware, Debian o Microsoft Windows, considerados como 'experimentales'.

\begin{figure}[htp]
\centering
\includegraphics[width=0.4\textwidth]{figuras/ros.png}
\caption{Logo de ROS.}
\label{fig:ros}
\end{figure}

ROS consta de dos partes básicas: la parte del sistema operativo, ros, como se ha descrito anteriormente y ros-pkg, una suite de paquetes aportados por la contribución de usuarios (organizados en conjuntos llamado en inglés "stacks") que implementan las funcionalidades tales como localización y mapeo simultáneo, planificación, percepción, simulación, etc. Este tipo de paquetes favorecen el desarrollo rápido de otros robots, consiguiendo que el código pueda reutilizarse gracias a su sistema de nodos, que mantienen cada funcionalidad desacoplada.

ROS ofrece principalmente dos lenguajes de programación para acceder a su API (Application Programming Interface) completa. Esos lenguajes son C++ y Python \cite{rosapi}. 

ROS es software libre bajo términos de licencia BSD. Esta licencia permite libertad para uso comercial e investigador. Las contribuciones de los paquetes en ros-pkg están bajo una gran variedad de licencias diferentes.

Actualemtente ROS es mantenido y desarrollado por Open Source robotics Foundation \cite{osrf}, una organización independiente sin ánimo de lucro fundada por miembros de la comunidad robótica a nivel global.

\subsection{Lenguaje de programación C++}

El lenguaje C++ es un lenguaje orientado a objetos, y como tal, tiene como objetivo la reducción del tiempo de desarrollo aumentando la eficacia del proceso de generación de los programas gracias a la reutilización de código.

Como consecuencia, los programas tienden a tener menos líneas de código y con más facilidad de introducir elementos nuevos escritos
por otras personas.

Al tratarse de un lenguaje compilado, presenta una buena eficiencia en tiempo de ejecución frente a los lenguajes interpretados.

En sistemas operativos basados en Linux, el lenguaje C++ se compila bajo el compilador GCC (GNU Compiler Collection).

Dentro del desarrollo software en C++ para ROS (roscpp \cite{roscpp}), existe una amplia interfaz para acceder a las diferentes funcionalidades y comunicarse con nodos desarrollados tanto en C++ como en Python.

\subsection{Lenguaje de programación Python}
Python es un lenguaje de programación interpretado cuya principal característica es que utiliza una sintaxis que favorece el código legible.

Se trata de un lenguaje de programación multiparadigma, ya que soporta orientación a objetos, programación imperativa y, en menor medida, programación funcional. Es un lenguaje interpretado, usa tipado dinámico y es multiplataforma.

Gracias a sus características, su uso es totalmente flexible y permite un tiempo de desarrollo menor principalmente por su tipado dinámico y su sintaxis. Sin embargo, al tratarse de un lenguaje interpretado, el tiempo de ejecución es más alto lo cual no lo hace adecuado para tareas que requieran altos niveles de eficiencia.

Dentro del desarrollo en Python para ROS (rospy \cite{rospy}), existe una interfaz completa para comunicarse con los nodos y otras funcionalidades de ROS desarrolladas en Python o C++.

\subsection{Controlador de versiones git y repositiorios GitHub}

Git es un software de control de versiones creado por Linus Torvalds. Git
gestiona los archivos y directorios y los cambios hechos en ellos a lo largo del tiempo. Esto permite recuperar antiguas revisiones del proyecto o ver el historial de cambios.

Git fue creado pensando en la eficiencia y la confiabilidad del mantenimiento de versiones cuando estas tienen un gran número de archivos de código fuente. Tiene la capacidad de poder trabajar varias personas con el mismo paquete siempre que no modifiquen el mismo archivo. Además en ese caso, sería posible ver las diferencias entre ambas versiones, y unirlas o crear una rama del proyecto principal si fuera necesario tener las dos versiones.

GitHub\footnote{\url{www.github.com}} es un sistema de almacenamiento público de código fuente (de cualquier tipo) o un servicio de repositorios. Su principal característica es la de ofrecer una plataforma de interacción social \cite{dabbish2012social}en la que distintas personas pueden trabajar conjuntamente. Esto permite que varios desarrolladores contribuyan a un proyecto y trabajen de manera coordinada.

Tanto para el desarrollo software de este proyecto como para la redacción de esta memoria se han utilizado estas herramientas, y el acceso a los repositorios se encuentran en las siguientes direcciones:

\begin{itemize}
	\item Desarrollo software del proyecto \url{https://github.com/danimtb/pioneer3at_ETSIDI}
	\item Memoria del proyecto \url{https://github.com/danimtb/TFG_pioneer3at}
\end{itemize}

\subsection{Simulador de robótica Gazebo}

Gazebo \cite{gazebo} es un simulador de robótica en tres dimensiones que ofrece la simulación de complejos entornos de diversas características, así como robots de todo tipo, su interacción con el entorno y la representación visual de datos obtenidos por diversos sensores como cámaras, láseres, ultrasonidos...

Un buen simulador de robótica es esencial para cualquier tipo de desarrollo robótico, ya que podemos realizar las pruebas software o la viabilidad de un sistema antes de construirlo. Gazebo cuenta con un potente motor de física simulada, interacción con objetos y dinámica de los mismos \cite{koenig2004design}.

\begin{figure}[htp]
\centering
\includegraphics[width=0.8\textwidth]{figuras/gazebo.png}
\caption{Entorno de simulación Gazebo.}
\label{fig:gazebo}
\end{figure}

Gazebo permite una integración completa con ROS, gestiona modelos físicos de robots utilizando el formato URDF (Unified Robot Description Format) \cite{urdf} y añade características específicas como el tipo de material, los momentos de inercia o el modelo de colisión. Además, incorpora plug-ins (funcionalidades añadidas) que permiten la simulación de robots de tipo diferencial, simulación de sensores y el cálculo de transformadas entre los distintos sistemas de referencia.

Gazebo es mantenido y desarrollado actualmente por la Open Source Robotics Foundation \cite{osrf}.

\subsection{RViz: Herramienta de visualización robótica}

RViz es una herramienta para la visualización de datos en 3 dimensiones de forma gráfica que trabaja dentro del entorno ROS (Figura \ref{fig:rviz}). Esta aplicación nos permite ver lo que está ocurriendo en nuestra plataforma robótica a tiempo real.

RViz puede usarse para mostrar lecturas de sensores, datos devueltos por sensores de percepción en 3 dimensiones (nubes de puntos), visualizar mapas, visualizar un modelo de nuestro robot, su posición, etc. También puede utilizarse para interactuar con nuestro robot, utilizando marcadores interactivos o su interfaz de usuario.

RViz no es un simulador de robótica, si no una herramienta que nos muestra la información que maneja nuestro sistema robótico ROS de manera visual.

\begin{figure}[htp]
\centering
\includegraphics[width=0.8\textwidth]{figuras/rviz_model.png}
\caption{Entorno gráfico RViz.}
\label{fig:rviz}
\end{figure}

Al uso, RViz es un nodo más dentro de ROS que se suscribe o publica mensajes a otros nodos.

Su uso está muy extendido en ROS ya que nos permite entender lo que ocurre alrededor del robot y la información que manejan los nodos dentro del sistema de una manera intuitiva.

\subsection{Impresión 3D}
La impresión 3D es un grupo de tecnologías de fabricación por adición donde un objeto tridimensional es creado mediante la superposición de capas sucesivas de materia.

Las impresoras 3D de uso popular utilizan un extrusor preparado para plásticos procesados en forma de filamento de pocos milímetros de grosor que deposita material capa a capa hasta que el objeto deseado alcanza su forma final.

El diseño de los objetos se realiza mediante una herramienta de diseño asistido por ordenador y los objetos deseados se exportan en formato de mallas.

En este proyecto la impresión 3D ha servido de ayuda para crear piezas específicas para colocar los sensores del robot, anclar elementos de la estructura del robot o servir de soporte a equipos provisionales.

El software utilizado para el modelado 3D ha sido la herramienta FreeCAD y la impresora 3D Makerbot thing-o-matic con su software ReplicatorG para generar las capas y controlar la impresora.

\section{Hardware}
En esta parte se explica detalladamente el hardware empleado en el desarrollo del proyecto.

\subsection{Pioneer 3 AT}

El robot Pioneer 3 AT (Figura \ref{fig:pioneer3at}), perteneciente a la empresa Adept MobileRobots, es un robot de cuatro ruedas en configuración skid-steer y todo terreno (AT, All Terrain) de operación e investigación en laboratorio.\\

\begin{figure}[htp]
\centering
\includegraphics[width=0.4\textwidth]{figuras/pioneer_3_at.jpg}
\caption{Robot Pioneer 3-AT.} \label{fig:pioneer3at}
\end{figure}

Su configuración en skid-steer permite un control relativamente simple utilizando el modo diferencial para poder realizar giros con gran maniobrabilidad, sin embargo, esta configuración depende mucho del tipo de suelo, con lo que se pierde precisión.

Este robot dispone de baterías, interruptor con parada de emergencia, dos motores de corriente continua para cada par de ruedas con transmisión mediante correa, encoders para leer la odometría y un microcontrolador con firmware ARCOS.

Ademas cuenta con un pequeño computador interno conectado al microcontrolador que puede utilizarse para realizar operaciones de manera autónoma.

El cuerpo del robot es de aluminio y su parte delantera así como superior es fácilmente desmontable para realizar las conexiones pertinentes y acceder al ordenador de a bordo y la placa microcontroladora. En la plataforma superior se sitúa el panel de control (Figura \ref{fig:panel_control})para acceder al ordenador de a bordo conectando un monitor, teclado y ratón, puerto serial RS-232, botones de encendido y reset varios leds indicadores de estado y de envío y recepción de datos.

\begin{figure}[htp]
\centering
\includegraphics[width=0.6\textwidth]{figuras/panel_control.png}
\caption{Panel de control del robot Pioneer 3-AT.} \label{fig:panel_control}
\end{figure}

En la siguiente tabla (Tabla \ref{tabla_pioneer3at}) se describen las principales características del robot.

% Please add the following required packages to your document preamble:
% \usepackage{graphicx}
\begin{table}[!h]
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{c c}
\hline
{\bf Especificaciones} & {\bf Pioneer 3 AT} \\ \hline
Largo & 508 mm \\
Ancho & 497 mm \\
Alto & 277 mm \\
Distancia al suelo & 80 mm \\
Peso & 12 kg \\
Carga útil & 32 kg \\
Cuerpo & Aluminio de 1.6 mm \\
Baterías & 3 de 12 V, estancas, plomo-ácido \\
Autonomía & 4-8 horas \\
Sistema motriz & 4 ruedas motrices \\
Ruedas & Neumáticos de Nylon \\
Diámetro de rueda & 222 mm (Ruedas todoterreno) / 190 mm (Ruedas interior)\\
Ancho de rueda & 88 mm \\
Sistema de giro & Diferencial \\
radio máxima curvatura & 40 cm \\
Radio de giro & 0 cm \\
Máxima velocidad de avance & 1.2 m/s \\
Máximo escalón & 10 cm \\
Máximo hueco & 15.2 cm \\
Terreno & Asfalto, Tierra, Césped, etc. \\
Encoders & 500 pulsos \\
Procesador & Hitachi H8S \\ \hline
\end{tabular}
}
\caption{Especificaciones del robot Pioneer 3 AT.}
\label{tabla_pioneer3at}
\end{table}


\subsection{Sensor Kinect}

Kinect es un conjunto de sensores de bajo coste que lo convierte en una
herramienta excepcional (Figura \ref{fig:sensor_kinect1}). Este dispositivo incluye una cámara de vídeo RGB, una cámara infrarroja de profundidad, un array de micrófonos y altavoces, un acelerómetro y un pequeño motor que le permite hacer movimientos de inclinación.

\begin{figure}[htp]
\centering
\includegraphics[width=0.5\textwidth]{figuras/sensor_kinect.png}
\caption{Sensor Kinect.}
\label{fig:sensor_kinect1}
\end{figure}

Su función principal es la de percibir el entorno captando una serie de puntos que se ubican en las tres dimensiones. Su funcionamiento a grandes rasgos se basa en un emisor de infrarrojos a 830 nm que interactúa con los objetos y una cámara infrarroja que etecta la diferencia entre la proyección anterior y la actual, obteniendo la distancia a cada objeto.

En primer lugar, el laser infrarrojo es emitido por Kinect con un patrón determinado (Projected textures), el cual no es simétrico sino que tiene puntos aleatorios que se dispersa gracias a unas lentes de proyección. Estos puntos aleatorios se reflejan en los objetos, los cuales sería posible verlos con una cámara externa.

\begin{figure}[!htp]
\centering
\includegraphics[width=0.65\textwidth]{figuras/proyecciones_kinect.png}
\caption{Proyección de infrarrojos y obtención de la nube de puntos.}
\label{fig:proyecciones_kinect}
\end{figure}

A continuación, al sensor de Kinect MT9M001C12STM, que no es más que el sensor CMOS de una cámara en la que se le trata para que observe solo el infrarrojo, obteniendo los puntos infrarrojos en el plano 2D. El motivo por el que podemos medir la profundidad de los objetos (su distancia) es porque sabemos el patrón de cómo emite el laser emisor \cite{konolige2010projected}, por tanto sabremos que si un punto no está en el sitio que corresponde, se ha trasladado respecto al punto inicial y se le aplica la correspondiente transformación (Figura \ref{fig:proyecciones_kinect}), obteniendo finalmente los puntos de toda la nube en coordenadas cartesianas XYZ.

La siguiente tabla (Tabla \ref{tabla_kinect}) muestra las especificaciones del sensor Kinect.

\begin{table}[!h]
\centering
\resizebox{\textwidth}{!}{
\begin{tabular}{c c}
\hline
{\bf Especificaciones} & {\bf Sensor Kinect} \\ \hline
Dimensiones del conjunto & 270mm x 50mm x 70mm \\
Fuente infrarroja & 830nm \\
Potencia & 60 mW \\
Cámara Infrarroja & MT9M001C12STM \\
Resolución cámara infrarroja & 1200x960 pixeles \\
Frecuencia & 30 Hz \\
Tamaño pixel & 5.2um x 5.2um  \\
Pixeles activos & 1280H x 1024V \\
Campo de visión & 58º H, 45º V, 70º D \\
Resolución espacial & 3mm (a 2 metros de distancia) \\
Resolución de profundidad & 1cm (a 2 metros de distancia) \\
Distancia de operación & 0.45m ? 6.5m \\
Cámara RGB & MT9M112 \\
Resolución cámara RGB & 640 x 480) \\
Audio & TAS1020B (Controlador de Audio) \\
Formato & 16kHz, 16-bit mono, modulación por
codificación de pulso (PCM)\\
Entrada de audio & 4 micrófonos con conversión analógico
digital de 24bits \\
Acelerómetro & KXSD9-2050\\ \hline
\end{tabular}
}
\caption{Características del sensor Kinect.}
\label{tabla_kinect}
\end{table}

\subsection{Láser SICK LMS100}

El sensor Sick LMS100 es un sensor láser por infrarrojos de clase I (Inofensivo para el ojo humano), que obtiene la medida de distancias con gran preción y rapidez en un solo plano y realizando un barrido de 270º (Figura \ref{fig:sensor_sicklms100}).

\begin{figure}[!h]
\centering
\includegraphics[width=0.45\textwidth]{figuras/SICK_LMS100.jpg}
\includegraphics[width=0.45\textwidth]{figuras/sicklms100_rango.png}
\caption{Sensor escaner láser Sick LMS100.}
\label{fig:sensor_sicklms100}
\end{figure}

Tras un análisis realizado durante este proyecto este sensor se ha situado en la parte trasera del robot, enfocando hacia atrás para cubrir un mayor rango y conocer todo el entorno alrededor del robot.

Destaca por su amplio rango, alcance y precisión. En la siguiente tabla (Tabla \ref{tabla_sicklms100}) se recogen sus características principales.

\begin{table}[!h]
\centering
\resizebox{0.8\textwidth}{!}{
\begin{tabular}{c c}
\hline
{\bf Especificaciones} & {\bf Sick LMS100} \\ \hline
Campo de aplicación & Interiores \\
Fuente infrarroja & 905 nm \\
Clase Láser & 1 (IEC 60825-1) \\
Campo de visión & 270º \\
Frecuencia de escaneo & 25Hz/50Hz \\
Resolución angular & 0.25º/0.5º \\
Distancia de operación & 0.05 - 20 m  \\
Tiempo de respuesta & 20 ms \\
Error & 30 mm \\
Interfaz de datos & Ethernet \\
Tensión de operación & 10.8V - 20V DC \\
Consumo & 20 W \\
Peso & 1.1 Kg \\
Dimensiones & 105mm x 102mm x 152mm\\ \hline
\end{tabular}
}
\caption{Características del sensor láser Sick LMS100. Basado en \cite{sicklms100}.}
\label{tabla_sicklms100}
\end{table}

\subsection{Intel NUC NUC5i7RYH}

El ordenador Intel NUC NUC5i7RYH, es un ordenador compacto de altas prestaciones y de tamaño compacto que ofrece unas buenas características para procesar datos y realizar la algoritmia adecuada para tareas de robótica.

Está equipado con un procesador Intel i7-5557U de quinta generación que ofrece una frecuencia de reloj de 3.1 GHz. Dispone de un disco duro de estado sólido que permite una alta velocidad de lectura y escritura en disco, así como una tarjeta RAM de tipo DDR3L de 8GB que permitirá el intercambio de información entre los nodos ROS de una manera fluida.

\begin{figure}[!h]
\centering
\includegraphics[width=0.4\textwidth]{figuras/intelnuc.jpg}
\caption{ordenador compacto Intel NUC.}
\label{fig:intelnuc}
\end{figure}

Su cometido será el de procesar la información de los sensores, generar los mapas incorporando los obstáculos, generar las trayectorias de navegación, comandar los motores del robot para realizar movimientos y realizar las funciones de servidor de datos.

Dispone de tamaño compacto y un consumo bajo, junto con una alimentación a partir de los 12 voltios, lo que lo hace ideal para incorporarlo en robots móviles que requieran realizar tareas con alto procesamiento sin depender de una infraestructura.

En la tabla \ref{tabla_intelnuc} pueden consultarse sus características principales.

\begin{table}[!h]
\centering
\resizebox{\textwidth}{!}{
\begin{tabular}{c c}
\hline
{\bf Especificaciones} & {\bf Intel NUC NUC5i7RYH} \\ \hline
Procesador & Intel Core i7-5557U, dual-core \\
Frecuencia de reloj & 3.1 GHz hasta 3.4 GHz \\
Memoria RAM & DDR3L1 8 GB \\
Disco duro & M.2 SSD 120 GB \\
Gráficos & Iris Graphics 6100 \\
 & 2 x USB 3.0 en el panel posterior\\
 Conectividad de periféricos & 2 x USB 3.0 en el panel frontal\\
 & 2 x USB 2.0 internos vía colector\\
 & Intel 10/100/1000 Mbps\\
 Conectividad de red & Intel® Wireless-AC 7265 M.2\\
 & Antenas inalámbricas (IEEE 802.11ac)\\
Alimentación & 12-19V DC \\
Consumo & 65 W \\
Dimensiones & 115mm x 111mm x 48.7mm\\ \hline
\end{tabular}
}
\caption{Características del ordenador Intel NUC NUC5i7RYH.}
\label{tabla_intelnuc}
\end{table}
