\selectlanguage{spanish}

%chapter introduce un nuevo capítulo
\chapter{Resumen}

Muchos robots autónomos surgen como herramienta para acceder a lugares donde el ser humano no puede o no conviene que acceda porque se encontraría en riesgo.

Un robot autónomo es por tanto una pieza fundamental en tareas de rescate, salvamento, inspección, exploración de entornos peligrosos o inaccesibles, como la exploración en la superficie de otros planetas. Además, tareas sociales como la asistencia a humanos en entornos públicos, la interacción con el entorno o una navegación más segura, como es el caso de los coches autónomos, cada vez están tomando más relevancia en nuestro día a día.

Este proyecto de fin de grado trata sobre el guiado y control de un robot móvil de cuatro ruedas, con un sistema motriz en configuración skid-steer, equipado con una serie de sensores que permiten su orientación y posicionado en el entorno así como un sensor capaz de captar este en tres dimensiones y un sensor adicional que lo hace tan solo en dos dimensiones.

Los datos de los sensores sirven tanto para construir mapas en dos dimensiones del entorno del robot como para navegar por él evitando obstáculos de manera dinámica. El robot es capaz de generar mapas de celdas en los que situar tanto los objetos estáticos como los móviles, calcular una trayectoria adecuada y dirigirse hasta un punto indicado evitando obstáculos interpuestos en su camino.

Todo esta información, procesado de datos, cálculo de trayectorias y ejecución de movimientos se realiza en un ordenador de a bordo integrado en el propio robot utilizando el software Robot Operating System (conocido en robótica por sus siglas ROS), que nos ofrece una interfaz común para interconectar nuestro robot con los sensores y con los algoritmos de navegación.

A parte de la navegación autónoma, también se ha incluido un sistema de telecontrol del robot mediante otro ordenador  externo y de un algoritmo de detección frontal de objetos en 3 dimensiones (nubes de puntos) que puedan servirle como guía. De esta forma, el robot es capaz de navegar siguiendo el movimiento de una persona o de un robot que le preceda.

El robot Pioneer 3 AT es el robot móvil que se ha empleado en este proyecto (Figura \ref{fig:esquema_robot1}) y sobre el que se ha trabajado de manera específica para realizar las pruebas reales de este proyecto. A este robot se le incorporan un sensor láser de dos dimensiones (sensor Sick LMS100) y un sensor de tres dimensiones (sensor Kinect). El cómputo de la navegación se realiza en un ordenador compacto incorporado en el robot (Intel NUC NUC5i7RYH).


\begin{figure}[htp]
\centering
\includegraphics[width=0.8\textwidth]{figuras/esquema_robot.jpg}
\caption{Esquema del sistema robótico utilizado en el proyecto} \label{fig:esquema_robot1}
\end{figure}

Las consignas de navegación se realizan mediante un ordenador externo cualquiera conectado a una red inalámbrica o mediante consignas de voz, en las que se indica al robot las tareas de navegación a realizar (avanzar, girar, seguir a una persona...) o un punto del entorno al que dirigirse.

Todas estas implementaciones están desarrolladas bajo el entorno ROS, lo cual permite añadir funcionalidades de manera más rápida y menos laboriosa, como es el caso del control mediante comandos de voz o la interacción mediante sonidos. Es el caso también del simulador de robótica Gazebo, que se integra como funcionalidad en ROS y que ha servido para testar el sistema y aportar las pruebas teóricas pertinentes para luego aplicarlas en el robot real.

Para concluir, podemos decir que este proyecto se encarga de integrar ROS como sistema en un ordenador de a bordo incorporado en el robot que permita conectarse con los sensores y realizar la construcción de mapas y navegación autónoma mediante el cáculo de mapas y trayectorias globales y locales, realizar los movimientos del robot, así como reconocer consignas de voz o de teleoperación.

\paragraph{Palabras clave:} robot móvil, ROS, navegación reactiva, cálculo de trayectorias.

\chapter{Abstract}

Achieving navigation and guidance of mobile robot emerges as a tool with the purpose to access to places where human beings cannot either because of the difficulties posed by the terrain or because the access may involve a high risk for life. Many of those repetitive, dangerous and fatiguing tasks could be done with a robust and capable mobile robot.

Consequently, an autonomous robot is, an essential element in rescue, inspection and exploration tasks that are developed in dangerous or hard to access places, such as the surface of other planets. Moreover, autonomous robots can help with social tasks such as assitance for humans in public places, interaction with the environment or ensuring a safer navigation in the cities. Autonomous cars are a good example of the latter.

This final degree project focuses on a practical guidance and control approach of a four-wheel mobile robot with a skid-steer configuration. It is equipped with a sort of sensors that allow it to make positioning and orientation in the environment. It also contains a main sensor capturing the enviaronment in three dimensions and an additional one that does it in two dimensions.

Sensor data is used to build two dimensional maps of the exploration place as well as avoid dynamical obstacles. The robot can build maps formed by cells where to incorporate or raytrace static and dynamic obstacles, to calculate the proper trajectory plan and to move towards a destination point avoiding the obstacles that it may on its way.

All this information, data processing, trajectory calculation and movement execution is done in an onboard computer inside the robot. It uses the Robot Operating System software (known as ROS), which offers a common interface to communicate the robot with sensors and navigation algorithms.

Likewise, besides autonomous navigation, the robot also has a telecontrol system from an outside computer and an algorithm to detect frontal objects in three dimensions (pointclouds) that can guide the robot. This is how it can navigate following a person when he or she is walking or follow another robot in front of it.

Pioneer 3 AT robot is the one used in this project (Figure \ref{fig:esquema_robot2}). It has been the specific platform used for all real tests conducted during this project. This robot is equipped with a two-dimensional laser scanner (Sick LMS100 sensor) and a three-dimensional sensor (Kinect sensor), while navigation computation is done in an onboard compact computer (Intel NUC NUC5i7RYH).

\begin{figure}[htp]
\centering
\includegraphics[width=0.8\textwidth]{figuras/esquema_robot.jpg}
\caption{Diagram of the robotic system used for this project} \label{fig:esquema_robot2}
\end{figure}

The navigation commands are sent from an outside computer connected to the same wireless network or from voice navigation commands speaking directly to the robot (go forward, backward, turn right...) or pointing a goal in the map.

All those implementations are developed under the ROS framework. This is why additional features can be added in a faster and effortless way. That is the case of the robot simulator Gazebo, which integrates as an add-on within ROS. Gazebo has been used to perform tests in navigation and to check theoretical concepts to lately incorporate them in the real robotic system.

To conclude, it can be said that this project integrates ROS as a robotic system in an onboard computer and connects to sensors to perform tasks such as building maps or navigating from one point to another. The system calculates local and global maps and trajectories, makes movements according to them, as well as recognises voice or teleoperation commands.

\paragraph{Keywords:} mobile robot, ROS, reactive navigation, trajectory calculation.