\chapter{Implementación del sistema}\label{chapter:implementacion}

Este capítulo describe los cambios, ajustes y modificaciones que, basados en la información anterior expuesta, las características de ROS y el hardware del que disponemos, se han realizado para alcanzar los objetivos del proyecto.

\section{Configuraciones hardware}
Como ya se ha descrito, la navegación se basa en el sensor Kinect pero también se ha considerado integrar el sensor láser debido a la valiosa información que aporta y su disponibilidad.

Por ello, el robot deberá llevar incorporados estos sensores proporcionándoles alimentación y una interfaz de conexión adecuada.

\subsection{Pioneer 3 AT}
El robot Pioneer 3 AT de Adept Mobile Robots es la base de la plataforma robótica. El modelo disponible en el laborarotio de la Escuela Técnica Superior de Ingeniería y Diseño Industrial llevaba incorporado un ordenador de tipo **ORDENADOR PIONEER**. También, al comienzo de este proyecto ya existían algunas adaptaciones como la incorporación de un altavoz frontal, acceso a los puestos USB del ordenador interno y conexión para el sensor láser (Figura \ref{fig:robot_inicial}).

\begin{figure}[!htp]
\centering
\includegraphics[width=0.8\textwidth]{figuras/robot_inicial.png}
\caption{Estado del robot al comienzo del proyecto \cite{}.}
\label{fig:robot_inicial}
\end{figure}

El robot había sido utilizado anteriormente mediante el software MRCore en el proyecto **PROYECTO DE ALEJANDRO** y el sistema operativo del ordenador interno era Ubuntu Server 10.

Para integrar la versión Indigo de ROS lo más recomendable era partir de la versión estable más actualizada de Ubuntu, por lo que se sustituyó el sistema operativo por Ubuntu 14.04 LTS en su versión de escritorio.

Una vez integrado el sistema operativo, la primera toma de contacto con el robot fue a partir de la librería Aria **referencia** para controlar el movimiento de los motores y comprobar que el robot se encontraba en buen estado.

**Añadir el puerto al grupo de dialout**

A continuación, tras instalar ROS Indigo, se procedió a las pruebas mediante el paquete Rosaria de ROS. La conexión con el microcontrolador de la placa de motores fue exitosa y se comprobó que los valores de la odometría también funcionaban.

**cambiar el puerto de conexion*?*


Llegados a este punto, ya disponíamos del robot preparado para realizar las primeras pruebas.

\subsection{Sensor Láser}\label{subsection:implementacion_laser}
El sensor láser Sick ya había sido integrado en un proyecto anterior y sus conexiones de alimentación y datos vía Ethernet ya estaban preparadas para utilizarlo.

Para conectarlo a través del puerto Ethernet fue necesario ajustar su dirección IP a través del software del fabricante y ajustar la IP del ordenador del robot Pioneer (más información en el apéndice **TAL**).

El agarre mecánico del sensor se dejó tal y como había sido utilizado en ocasiones anteriores, situado en la parte frontal agarrado mediante un par de tornillos al chasis con tuercas de palometa para su fácil manipulación.

El sensor láser se conecta a la interfaz ROS mediante el paquete LMS1xx tal y como se describió en el apartado \ref{subsection:sicklms100}.

\subsection{Sensor Kinect}\label{subsection:implementacion_kinect}
La integración del sensor Kinect fue relativamente sencilla debido a que las entradas de los puestos USB del ordenador habían sido cableadas previamente. La adaptación a realizar era sobre la parte de alimentación, ya que este sensor trabaja a una tensión de 12 voltios.

En el manual del robot se encuentra una descripción detallada de la placa de alimentación a la cual pueden conectarse diferentes periféricos. Esta placa ofrece tomas de conexión de 5 voltios controlados por unos botones auxiliares y tomas de 12 voltios (ver apéndice **TAL**).

El sensor Kinect dispone de un adaptador USB, preparado para trabajar con la videoconsola XBOX 360, el cual suministra 12 voltios mediante un transformador conectado a una toma de corriente alterna de 220v e incorpora los cables de datos del propio sensor Kinect.

**Esquema conexion usual**.

Para integrar el sensor Kinect en el robot, se cortó el cable de alimentación del cable adaptador y se soldaron unas clavijas tipo Jack **REVISAR** macho-hembra para conectar el adaptador directamente a los 12 voltios de la placa del robot. También se realizaó lo oportuno en el adaptador de corriente, para poder usar el sensor Kinect de la manera habitual (Figura \ref{fig:cables_kinect}).

\begin{figure}[!htp]
\centering
\includegraphics[width=0.45\textwidth]{figuras/adaptador_kinect.png}
\includegraphics[width=0.45\textwidth]{figuras/cable_kinect.png}
\caption{Adaptación de cables para la alimentación del sensor Kinect (izq.) y cable a 12V de la placa de alimentación del robot (dcha.).}
\label{fig:cables_kinect}
\end{figure}

Para anclar el sensor Kinect al robot se optó por situarlo en la parte superior del sensor Láser, para lo cual se diseñó una pieza que encajase en la base de la Kinect y en el sensor láser (figura **REF**).

**Imagen del diseño 3D**

El sensor Kinect se conecta a la interfaz ROS mediante el paquete freenect\_stack tal y como se describió en el apartado \ref{subsection:kinect}.

\subsection{Primera configuración hardware}
La primera configuración del robot consistió en ambos sensores situados en la parte frontal del mismo. Los sensores de encontraban colocados de manera vertical otro, de tal forma que no existieran interferencias entre uno y otro.

\begin{figure}[!htp]
\centering
\includegraphics[width=\textwidth]{figuras/primera_configuracion.jpg}
\caption{Primera configuración hardware del robot}
\label{fig:primera_configuracion}
\end{figure}

De esta forma conseguíamos una vista frontal despejada y contábamos con la información del láser para detectar obstáculos laterales.

\subsubsection{Primera configuración del sistema}
El ordenador interno corría todos los nodos de ROS, de modo que se disponía de la información de los sensores, el control sobre los motores y la lectura de la odometría para realizar las primeras pruebas con el paquete de navegación de ROS (Sección \label{section:navigation_stack}).

Sin embargo, la primera implementación con los primeros ajustes a nuestro hardware de los sensores no fue posible debido a la sobrecarga de la CPU del ordenador interno del robot Pioneer y a problemas de memoria en la ejecución de nodos como AMCL.

\subsubsection{Segunda configuración del sistema}
La siguiente opción fue utilizar un ordenador externo que realizase los cálculos de navegación y enviase al robot las consignas de movimiento a través de una red WLAN. Esta idea no era la solución más ideal, ya que desde el principio la idea era que el robot fuese lo más autónomo posible sin depender de una infraestructura, sin embargo era una posibilidad directa que no suponía mucho esfuerzo.

\begin{figure}[!htp]
\centering
\includegraphics[width=\textwidth]{figuras/segunda_configuracion_sistema.png}
\caption{Esquema de la segunda configuración del sistema. Basado en **REFERENCIA ALENJANDRO**.}
\label{fig:segunda_configuracion_sistema}
\end{figure}

Gracias a la filosofía de ejecución distribuida de nodos de ROS, ejecutar nodos en máquinas diferentes y compartir la información entre procesos es una tarea sencilla. Para configurarlo, tan solo es necesario indicar a las máquinas la IP del nodo master. De esta forma, los nodos que se ejecuten en cada una de las máquinas tratarán de realizar la comunicación a través de IPs dentro de la misma red.

Este ajuste fue puesto en marcha utilizando un ordenador portátil con suficiente capacidad de procesamiento y memoria como para ejecutar la navegación, sin embargo aparecieron algunos inconvenientes. El primero de ellos fueron las direcciones IP en el ordenador interno del Pioneer.

\begin{itemize}

\item \textbf{Problemas con el sensor Láser: LM1xx}\\\\
Debido a que el sensor láser se conecta vía Ethernet a este ordenador, el nodo LMS1xx debe obtener información a través de la IP del láser y enviarla a través del adaptador Wifi a la IP del nodo máster. El problema residía en que el nodo se saturaba al tener que lidiar con ambas interfaces de conexión y provocaba su detención.

Tras varias consultas a Clearpath Robotics a través de su repositorio de GitHub y preguntas en el foro ROS Answers **Enlaces de referencia**, la solución no estaba implementada en código y lo más inmediato era hacer un bridge en el ordenador del Pioneer 3 AT entre la interfaz Ethernet y la Wifi.

Los resultados de esta solución no fueron satisfactorios ya que el comportamiento era el mismo: el nodo LMS1xx se saturaba e interrumpía a los pocos minutos de su ejecución.

Trantando de resolver este problema, se hicieron pruebas generando una red Wifi Ad-hoc desde el ordenador del robot, a la cual se conectaba el ordenador externo. Los resultados fueron buenos siempre y cuando las IPs del nodo master y del sensor Láser se encontrasen en el mismo subrango **revisar nomenclatura**.

En la implementación final del sistema esta solución se sigue utilizando para conectarse desde un ordenador externo al ordenador que incorpora el robot Pioneer.
\end{itemize}

Una vez se pudo conectar el ordenador externo, la ejecución del nodo de navegación era la correcta y las consignas de movimiento se enviaban correctamente al robot, pudiendo realizar las primeras pruebas de navegación autónoma.

**Imagen primeras navegaciones**

Sin embargo en ocasiones la recepción y envío de datos era demasiado alta y esto provocaba que existiese mucho retraso en la comunicación, haciendo que el robot reaccionase tarde para esquivar los obstáculos y el control del robot fuera impracticable.

\subsubsection{Tercera configuración del sistema}
Finalmente se optó por montar un ordenador más potente en el robot, para lo cual se utilizó un portátil externo al que se conectaba tanto e sensor Kinect como el sensor Láser y se utilizaba un convertidor de puesto USB a puerto serie RS-232 para controlar el movimiento del robot y leer la odometría. El ordenador del robot quedaba sustituido y así se mantuvo hasta la versión final.

**imagen del robot con el portátil**

Llegados a este punto, ahora sí disponíamos de la plataforma robótica completa sobre la que trabajar en la navegación del robot. Las primeras pruebas fueron satisfactorias, logrando correr todos los nodos en el portátil, el cual se incorporó de manera provisional al robot por medio de unos agarres realizados con una impresora 3D.

**imagen de los agarres**

\subsection{Segunda configuración hardware}

La segunda configuración hardware vino dada tras las pruebas satisfactorias con el ordenador portátil como encargado de la ejecución del sistema ROS. Debido a ello se optó por utilizar el ordenador compacto Intel NUC de manera dedicada en el robot, sustituyendo al portátil, dejando más espacio para colocar los sensores y hacer un sistema más integrado.

Una de las principales desventajas con las que contaba la primera configuración hardware era la posición tan adelantada del sensor Kinect, ya que debido a sus características, si un objeto se situa a medio metro o menos delante de la cámara infrarroja, las proyecciones de su emisor de infrarrojos no pueden ser captadas y por tanto obtenemos una nube de puntos vacía (la cámara Kinect se queda ''ciega"). Es por tanto que esta nueva configuración hardware del robot vino motivada por el hecho de retrasar la posición de la cámara y obtener cierta distancia de margen para evitar el efecto anterior.

Tras varias pruebas en el simulador Gazebo (Sección \ref{Gazebo}) cambiando la posición del sensor Kinect, el ordenador Intel NUC y del sensor láser SICK, se aprovechó todo el área del robot de manera que ningún elemento estorbase a los haces de infrarrojos de los dos sensores. El sensor Kinect se situó más retrasado, hacia la mitad del robot, dejando tan solo sitio en la parte trasera para incorporar el resto de elementos. El sensor láser se situó en la parte trasera mirando hacia atrás ya que gracias a su amplio rango de 270º, podíamos obtener lecturas de prácticamente todo el perímetro del robot.

**imagen de la disposición del robot en freecad y pruebas reales**.

El ordenador Intel NUC se situó al lado izquierdo del sensor láser dejando el lado derecho para situar el cableado de los sensores y del ordenador.

Finalmente, para dejar todo el sistema integrado, se diseñaron unos paneles laterales, un sistema de varillas roscadas atornilladas a la base del robot y un panel superior para esconder el cableado interno. Se diseñó un modelo en 3D en el simulador Gazebo para comprobar su correcta compatibilidad con la disposición de los sensores.

\begin{figure}[!htp]
\centering
\includegraphics[width=0.45\textwidth]{figuras/modelo_3D_1.png}
\includegraphics[width=0.45\textwidth]{figuras/modelo_3D_2.png}
\caption{Modelo 3D de la segunda configuración hardware hecho en Gazebo}
\label{fig:modelo_3D}
\end{figure}

Tras comprobar que el diseño funcionaba y cumplía las caracteríticas necesarias se mecanizó la parte superior del robot para anclar el sensor láser y las varillas roscadas, como se ve en la figura TAL

**figura de robot con varillas roscadas y láser**

Para evitar las vibraciones y que el sensor Kinect permaneciese inmóvil y situado a la altura necesaria, se diseñaron e imprimieron en 3D unas piezas de soporte (figura TAL)

**figura de las piezas y figura de su disposición**

Los paneles laterales y el panel superior se mecanizaron en metacrilato de 3 y 4 milímetros respectivamente con una fresadora y se sujetaron al robot mediante remaches, tornillos y escuadras en forma de L, algunas de las cuales fueron diseñadas en 3D.

**Imagen de los paneles laterales, remaches, escuadra 3D**

El panel de control del robot quedó dividido en dos partes, la destinada al ordenador de abordo y la destinada al microcontrolador del robot. Ambas partes fueron resituadas en los laterales del robot.

**imagenes de los paneles de control laterales**

A continuación se muestran algunas imágenes del robot completo rediseñado.

**Imágenes del robot**

\textbf{Intel NUC}\\
El ordenador compacto Intel NUC se eligió como unidad de procesamiento y ejecución del sistema ROS debido a sus altas capacidades de procesador y memoria y a su bajo consumo. Éste fue alimentado a 12 voltios de la misma forma que el sensor Kinect y se fijó al chasis del robot mediante tiras del velcro adhesivo.

A él se conectaron el vía puerto USB el sensor Kinect y el cable convertidor de control del robot, el jack de auriculares al altavoz frontal del robot y vía ethernet el sensor láser.


\subsubsection{Configuración del sistema final}

La configuración final del sistema quedó definida como se indica en la figura \ref{fig:esquema_robot_final}. El ordenador Intel NUC se configuró para que generase en su arranque una red Wifi ad-hoc propia, dentro del mismo subrango **revisar nomenclatura** que las direcciones IP del sensor láser para evitar el malfuncionamiento del mismo tal y como se ha indicado anteriormente.

\begin{figure}[htp]
\centering
\includegraphics[width=0.8\textwidth]{figuras/esquema_robot.jpg}
\caption{Esquema del sistema robótico final utilizado en el proyecto.} \label{fig:esquema_robot_final}
\end{figure}

El suministro de energía a todos los elementos que incorpora el robot se realiza a través de la placa de alimentación del robot, la cual se conecta al pack de 3 baterías alojado en su interior.

\section{Navegación}
La navegación es el propósito central de este proyecto. Como ya se ha indicado anteriormente la navegación se centra en el Navigation Stack de ROS.

En el capítulo \ref{chapter:navegacion} vimos una descripción de cada una de sus partes y unos ajustes generales para nuestro robot. En este apartado veremos unos ajustes más específicos y elaborados que son los ajustes finales para la navegación del robot.

El robot ha sido adaptado y configurado de manera óptima basándonos en las características de la navegación en dos dimensiones que ofrece ROS, como hemos visto en el apartado anterior.

\subsection{Configuración de los costmaps y los sensores}
La navegación del robot se basa en la información recogida en los llamados "costmaps".

Para realizar la navegación disponemos de dos costmaps, el llamado "global\_costmap'' y el llamado "local\_costmap". El primero sirve de base al planificador de trayectoria global y el segundo lo hace para el planificador de trayectoria local.

Su comportamiento y posibilidades de configuración son las mismas con la excepción de que el mapa global toma información del mapa que se cargue para realizar navegación (en caso de utilizar uno) además de la información aportada por los sensores.

La misión de los mapas de coste es la tomar la información de los sensores e incorporarla a un mapa de celdillas y marcar o borrar los obstáculos pertinentes. A partir de esa información se calcula un gradiente de coste que asigna un valor a cada una de ellas.

Los mapas de coste tal y como está implementados en ROS integran la información de los sensores en una misma capa, de tal forma que no existe distinción entre el tipo de información que está tomando un sensor u otro. Esto supone un problema añadido en el caso de este proyecto, ya que se pretendía utilizar la información de los dos sensores: Kinect y láser.

\begin{itemize}
\item \textbf{La problemática de los dos sensores}\\\\
El problema consiste en que el sensor Kinect es capaz de tomar información de la posición de os obstáculos a diferente altura pero con un alcance más reducido. Sin embargo, el sensor láser dispone de un alcance mayor, pero no puede detectar obstáculos que queden por encima o por debajo de su haz.

Esto hacía que si se configuraban los mapas de coste con una sola capa de obstáculos, en caso de que el sensor Kinect incorporase un obstáculo al mapa que quedaba por debajo del haz láser, si ese obstáculo dejaba de ser visto por el sensor Kinect pero se encontraba dentro del rango del láser, al quedar por debajo del haz de este último no se detectaba ningún obstáculo y se borraba del mapa.

Esta problemática se producía tanto para el mapa de coste global como para el local.

Esta problemática es bien conocida dentro del mundo de la robótica y se denomina "Fusión sensorial" **referencia** donde una de las técnicas más conocidas es la de fusión mediante Filtro de Kalman **refrencia**.

La solución a esta problemática más adecuada era realizar una composición de los puntos obtenidos por el sensor Láser y el sensor Kinect. Sin embargo, el coste computacional de crear una nueva nube de puntos a partir de dos tipos de datos diferentes a una frecuencia adecuada se antojaba elevado, por lo que la opción más adecuada consistió en utilizar capas de obstáculos diferentes (\textit{costmap\_2d::VoxelLayer}) para cada uno de los sensores.

Así, cada sensor sería capaz de incorporar o borrar obstáculos del mapa solo si eran detectados o no por ese mismo sensor y no por el otro. Si bien es cierto que de esta forma existen duplicidades de los obstáculos al tener que ser incorporados o borrados del mapa por cada sensor de manera independiente, esto nos permite salvar el caso en el que exista un obstáculo y este no se tenga en cuenta por interferencias de los sensores.

\item \textbf{La problemática de la nube de puntos}\\\\
Otro de los problemas a solventar fue la manera en la que gestionar los obstáculos del sensor Kinect.

El gran número de puntos disponibles incrementa mucho el cálculo de los obstáculos si se analiza toda la nube directamente, por lo que es mejor recurrir a analizarla por partes.

Para hacer esta operación se probaron nodos de ros que realizaban la conversión del dato tipo PointCloud2 a tipo LaserScan definiendo parámetros como la altura, distancia...

El nodo \textbf{\textit{PointCloud\_to\_LaserScan}} **referencia** realiza este filtrado y conversión de tipo de datos a partir del análisis de la nube de puntos basándose en la librería PCL \cite{PCL}. Su funcionamiento es correcto y el coste computacional se reduce, sin embargo no se consigue la frecuencia adecuada para que los datos se actualicen a tiempo a medida que el robot navega.

Otro de los nodos utilizados para este propósito es \textbf{\textit{DepthImage\_to\_LaserScan}} **referencia**. En este caso su enfoque es diferente, ya que utiliza los píxeles de la imagen para analizar la nube de puntos de tal modo que solo analiza los puntos correspondientes a un determinado rango de píxeles. Este nodo es mucho más eficiente en el cálculo pero no tiene en cuenta aspectos como la inclinación del sensor o la detección del suelo como obstáculo.

Las pruebas realizadas utilizando este nodo fueron correctas, ya que el procesado de la nube de puntos era más rápido y permitía realizar los cáculos a una frecuencia adecuada. Sin embargo requiere más de una instancia de estos nodos para poder detectar obstáculos bajos a corta (hasta 1.5 metros) y media distancia (hasta unos 3 metros) para que no se produzcan interferencias con el suelo.

De este modo, se crearon 3 instancias diferentes de este nodo: uno para obstáculos situados a una distancia mayor, un segundo para obstáculos a distancias medias, y un tercero para obstáculos a distancias cortas.

\begin{code}[!htp]
\begin{lstlisting}[style=launch]
<launch>
    <node pkg="depthimage_to_laserscan" type="depthimage_to_laserscan" name="depthimage_to_laserscan_long">
        <remap from="image" to="/camera/depth/image_raw"/>
        <remap from="camera_info" to="/camera/depth/camera_info"/>
        <remap from="scan" to="camera/scan_depth_long"/>
        <rosparam>
            scan_height: 10
            scan_time: 0.167
        </rosparam>
    </node>
</launch>
\end{lstlisting}
\caption{Launchfile \textit{DepthImage\_to\_LaserScan} para obstáculos lejanos\footnotemark.}
\end{code}
\footnotetext{\hypersetup{urlcolor=black}
Fuente: \href{https://github.com/danimtb/pioneer3at_ETSIDI/blob/master/pioneer_utils/sensors/kinect_to_laser_long.launch}{\textit{pioneer\_utils/sensors/kinect\_to\_laser\_long.launch}}
\hypersetup{urlcolor=blue}}

\begin{code}[!htp]
\begin{lstlisting}[style=launch]
<launch>
    <node pkg="depthimage_to_laserscan" type="depthimage_to_laserscan" name="depthimage_to_laserscan_long">
        <remap from="image" to="/camera/depth/image_raw"/>
        <remap from="camera_info" to="/camera/depth/camera_info"/>
        <remap from="scan" to="camera/scan_depth_long"/>
        <rosparam>
            scan_height: 10
            scan_time: 0.167
        </rosparam>
    </node>
</launch>
\end{lstlisting}
\caption{Launchfile \textit{DepthImage\_to\_LaserScan} para obstáculos lejanos\footnotemark.}
\end{code}
\footnotetext{\hypersetup{urlcolor=black}
Fuente: \href{https://github.com/danimtb/pioneer3at_ETSIDI/blob/master/pioneer_utils/sensors/kinect_to_laser_long.launch}{\textit{pioneer\_utils/sensors/kinect\_to\_laser\_long.launch}}
\hypersetup{urlcolor=blue}}

\begin{code}[!htp]
\begin{lstlisting}[style=launch]
<launch>
    <node pkg="depthimage_to_laserscan" type="depthimage_to_laserscan" name="depthimage_to_laserscan_long">
        <remap from="image" to="/camera/depth/image_raw"/>
        <remap from="camera_info" to="/camera/depth/camera_info"/>
        <remap from="scan" to="camera/scan_depth_long"/>
        <rosparam>
            scan_height: 10
            scan_time: 0.167
        </rosparam>
    </node>
</launch>
\end{lstlisting}
\caption{Launchfile \textit{DepthImage\_to\_LaserScan} para obstáculos lejanos\footnotemark.}
\end{code}
\footnotetext{\hypersetup{urlcolor=black}
Fuente: \href{https://github.com/danimtb/pioneer3at_ETSIDI/blob/master/pioneer_utils/sensors/kinect_to_laser_long.launch}{\textit{pioneer\_utils/sensors/kinect\_to\_laser\_long.launch}}
\hypersetup{urlcolor=blue}}
\end{itemize}

Una vez solventados los problemas anteriores el ajuste de ambos costmaps se realizó mediante ensayos prueba error con el propio robot y con su modelo creado en el simulador Gazebo.

La configuración de los mismos se expone a continuación \ref{.

\begin{code}[!htp]
\begin{lstlisting}[style=launch]
global_costmap:
  global_frame: /map
  robot_base_frame: /base_link
  update_frequency: 2.0
  publish_frequency: 2.0
  static_map: true
  rolling_window: false
  track_unknown_space: true
  plugins:
      - {name: static_layer,       type: "costmap_2d::StaticLayer"}
      - {name: obstacle_layer_kinect,        type: "costmap_2d::VoxelLayer"}
      - {name: obstacle_layer_laser,       type: "costmap_2d::VoxelLayer"}
      - {name: inflation_layer,       type: "costmap_2d::InflationLayer"}

  obstacle_layer_kinect:
    observation_sources: kinect_laser kinect_laser_low kinect_laser_long
    kinect_laser: {sensor_frame: camera_link, data_type: LaserScan, topic: camera/scan_depth, marking: true, clearing: true, obstacle_range: 3.0, raytrace_range: 6.5, inf_is_valid: true}
    kinect_laser_low: {sensor_frame: camera_link, data_type: LaserScan, topic: camera/scan_depth_low, marking: true, clearing: true, obstacle_range: 3.0, raytrace_range: 6.5, inf_is_valid: true}
    kinect_laser_long: {sensor_frame: camera_link, data_type: LaserScan, topic: camera/scan_depth_long, marking: true, clearing: true, obstacle_range: 3.0, raytrace_range: 6.5, inf_is_valid: true}

  obstacle_layer_laser:
    observation_sources: sick_lms1xx
    sick_lms1xx: {sensor_frame: laser, data_type: LaserScan, topic: scan, marking: true, clearing: true, obstacle_range: 5.0, raytrace_range: 10.5, inf_is_valid: true}

  inflation_layer:
    inflation_radius: 0.55
    cost_scaling_factor: 4.0
\end{lstlisting}
\caption{Configuración del \textit{global\_costmap}\footnotemark.}
\end{code}
\footnotetext{\hypersetup{urlcolor=black}
Fuente: \href{https://github.com/danimtb/pioneer3at_ETSIDI/blob/master/pioneer_utils/navigation/global_navigation/global_costmap_params.yaml}{\textit{pioneer\_utils/navigation/global\_navigation/global\_costmap\_params.yaml}}
\hypersetup{urlcolor=blue}}

De especial interés la configuración de los parámetros de la capa \textit{costmap\_2d::InflationLayer} donde se ajusta el radio de "inflado" de los obstáculos así como un valor de escala en el cálculo del coste de cada celda.
Esto determina en gran medida el cálculo de trayectoria global, permitiendo trayectorias más suaves y alejadas de los obstáculos **referencia Imagen**.

**IMAGEN GLOBAL COSTMAP**

Para el mapa local la configuración es muy similar y lo más importante es una frecuencia de actualización del mapa mayor.

\begin{code}[!htp]
\begin{lstlisting}[style=launch]
local_costmap:
  global_frame: /odom
  robot_base_frame: /base_link
  update_frequency: 10.0
  publish_frequency: 10.0
  static_map: false
  rolling_window: true
  width: 6.0
  height: 6.0
  resolution: 0.05
  max_obstacle_height: 0.5
  plugins:
      - {name: obstacle_layer_laser, type: "costmap_2d::VoxelLayer"}
      - {name: obstacle_layer_kinect, type: "costmap_2d::VoxelLayer"}
      - {name: inflation_layer, type: "costmap_2d::InflationLayer"}

  obstacle_layer_kinect:
    observation_sources: kinect_laser kinect_laser_low kinect_laser_long
    kinect_laser: {sensor_frame: camera_link, data_type: LaserScan, topic: camera/scan_depth, marking: true, clearing: true, obstacle_range: 9.0, raytrace_range: 9.5, inf_is_valid: false}
    kinect_laser_low: {sensor_frame: camera_link, data_type: LaserScan, topic: camera/scan_depth_low, marking: true, clearing: true, obstacle_range: 9.0, raytrace_range: 9.5, inf_is_valid: false}
    kinect_laser_long: {sensor_frame: camera_link, data_type: LaserScan, topic: camera/scan_depth_long, marking: true, clearing: true, obstacle_range: 9.0, raytrace_range: 9.5, inf_is_valid: false}
    kinect: {sensor_frame: camera_link, data_type: PointCloud2, topic: camera/depth/points, marking: true, clearing: true, inf_is_valid: true}


  obstacle_layer_laser:
    observation_sources: sick_lms1xx
    sick_lms1xx: {sensor_frame: laser, data_type: LaserScan, topic: scan, marking: true, clearing: true, obstacle_range: 10.0, raytrace_range: 12.0, inf_is_valid: true}

  inflation_layer:
    inflation_radius: 0.3
\end{lstlisting}
\caption{Configuración del \textit{local\_costmap}\footnotemark.}
\end{code}
\footnotetext{\hypersetup{urlcolor=black}
Fuente: \href{https://github.com/danimtb/pioneer3at_ETSIDI/blob/master/pioneer_utils/navigation/common/local_costmap_params.yaml}{\textit{pioneer\_utils/navigation/common/local\_costmap\_params.yaml}}
\hypersetup{urlcolor=blue}}

\subsection{Configuración de los planificadores de trayectoria}
\subsection{Navegación con mapa}
\subsection{Navegación reactiva}
\section{Nodo de navegación por puntos}
\section{Nodo de comandos por voz}
\subsection{Reconocimiento de comandos de voz}
\subsection{Feedback mediante text-to-speech}

\section{Nodo de ejecución automática de nodos}
