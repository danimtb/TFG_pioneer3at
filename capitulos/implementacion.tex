\chapter{Implementación del sistema}\label{chapter:implementacion}

Este capítulo describe los cambios, ajustes y modificaciones que, basados en la información anterior expuesta, las características de ROS y el hardware del que disponemos, se han realizado para alcanzar los objetivos del proyecto.

\section{Configuraciones hardware}
Como ya se ha descrito, la navegación se basa en el sensor Kinect pero también se ha considerado integrar el sensor láser debido a la valiosa información que aporta y su disponibilidad.

Por ello, el robot deberá llevar incorporados estos sensores proporcionándoles alimentación y una interfaz de conexión adecuada.

\subsection{Pioneer 3 AT}
El robot Pioneer 3 AT de Adept Mobile Robots es la base de la plataforma robótica. El modelo disponible en el laborarotio de la Escuela Técnica Superior de Ingeniería y Diseño Industrial llevaba incorporado un ordenador de tipo **ORDENADOR PIONEER**. También, al comienzo de este proyecto ya existían algunas adaptaciones como la incorporación de un altavoz frontal, acceso a los puestos USB del ordenador interno y conexión para el sensor láser (Figura \ref{fig:robot_inicial}).

\begin{figure}[!htp]
\centering
\includegraphics[width=0.8\textwidth]{figuras/robot_inicial.png}
\caption{Estado del robot al comienzo del proyecto \cite{herrero2013}.}
\label{fig:robot_inicial}
\end{figure}

El robot había sido utilizado anteriormente mediante el software MRCore en el proyecto \cite{herrero2013} y el sistema operativo del ordenador interno era Ubuntu Server 10.

Para integrar la versión Indigo de ROS lo más recomendable era partir de la versión estable más actualizada de Ubuntu, por lo que se sustituyó el sistema operativo por Ubuntu 14.04 LTS en su versión de escritorio.

Una vez integrado el sistema operativo, la primera toma de contacto con el robot fue a partir de la librería Aria **referencia** para controlar el movimiento de los motores y comprobar que el robot se encontraba en buen estado.

A continuación, tras instalar ROS Indigo, se procedió a las pruebas mediante el paquete Rosaria de ROS. La conexión con el microcontrolador de la placa de motores fue exitosa y se comprobó que los valores de la odometría también funcionaban.

Llegados a este punto, el robot se encontraba en disposición para realizar las primeras pruebas.

\subsection{Sensor Láser}\label{subsection:implementacion_laser}
El sensor láser Sick ya había sido integrado en un proyecto anterior y sus conexiones de alimentación y datos vía Ethernet ya estaban preparadas para utilizarlo.

Para conectarlo a través del puerto Ethernet fue necesario ajustar su dirección IP a través del software del fabricante y ajustar la IP del ordenador del robot Pioneer (más información en el apéndice **TAL**).

El agarre mecánico del sensor se dejó tal y como había sido utilizado en ocasiones anteriores, situado en la parte frontal agarrado mediante un par de tornillos al chasis con tuercas de palometa para su fácil manipulación.

El sensor láser se conecta a la interfaz ROS mediante el paquete LMS1xx tal y como se describió en el apartado \ref{subsection:sicklms100}.

\subsection{Sensor Kinect}\label{subsection:implementacion_kinect}
La integración del sensor Kinect fue relativamente sencilla debido a que las entradas de los puestos USB del ordenador habían sido cableadas previamente. La adaptación a realizar era sobre la parte de alimentación, ya que este sensor trabaja a una tensión de 12 voltios.

En el manual del robot se encuentra una descripción detallada de la placa de alimentación a la cual pueden conectarse diferentes periféricos. Esta placa ofrece tomas de conexión de 5 voltios controlados por unos botones auxiliares y tomas de 12 voltios (ver apéndice **TAL**).

El sensor Kinect dispone de un adaptador USB, preparado para trabajar con la videoconsola XBOX 360, el cual suministra 12 voltios mediante un transformador conectado a una toma de corriente alterna de 220v e incorpora los cables de datos del propio sensor Kinect.

\begin{figure}[htp]
\centering
\includegraphics[width=0.6\textwidth]{figuras/kinect_usual.png}
\caption{Conexión usual del sensor Kinect a la consola Xbox.}
\label{fig:kinect_usual}
\end{figure}

Para integrar el sensor Kinect en el robot, se cortó el cable de alimentación del cable adaptador y se soldaron unas clavijas tipo Jack **REVISAR** macho-hembra para conectar el adaptador directamente a los 12 voltios de la placa del robot. También se realizó lo oportuno en el adaptador de corriente, para poder usar el sensor Kinect de la manera habitual (Figura \ref{fig:cables_kinect}).

\begin{figure}[!htp]
\centering
\includegraphics[width=0.45\textwidth]{figuras/adaptador_kinect.png}
\includegraphics[width=0.45\textwidth]{figuras/cable_kinect.png}
\caption{Adaptación de cables para la alimentación del sensor Kinect (izq.) y cable a 12V de la placa de alimentación del robot (dcha.).}
\label{fig:cables_kinect}
\end{figure}

Para anclar el sensor Kinect al robot se optó por situarlo en la parte superior del sensor Láser, para lo cual se diseñó una pieza que encajase en la base de la Kinect y en el sensor láser (figura **REF**).

**Imagen del diseño 3D**

El sensor Kinect se conecta a la interfaz ROS mediante el paquete freenect\_stack tal y como se describió en el apartado \ref{subsection:kinect}.

\subsection{Primera configuración hardware}
La primera configuración del robot consistió en ambos sensores situados en la parte frontal del mismo. Los sensores se encontraban colocados de manera vertical otro, de tal forma que no existieran interferencias entre uno y otro.

\begin{figure}[!htp]
\centering
\includegraphics[width=\textwidth]{figuras/primera_configuracion.jpg}
\caption{Primera configuración hardware del robot}
\label{fig:primera_configuracion}
\end{figure}

De esta forma conseguíamos una vista frontal despejada y contábamos con la información del láser para detectar obstáculos laterales.

\subsubsection{Primera configuración del sistema}
El ordenador interno corría todos los nodos de ROS, de modo que se disponía de la información de los sensores, el control sobre los motores y la lectura de la odometría para realizar las primeras pruebas con el paquete de navegación de ROS (Sección \ref{section:navigation_stack}).

Sin embargo, la primera implementación con los primeros ajustes a nuestro hardware de los sensores no fue posible debido a la sobrecarga de la CPU del ordenador interno del robot Pioneer y a problemas de memoria en la ejecución de nodos como AMCL.

\subsubsection{Segunda configuración del sistema}
La siguiente opción fue utilizar un ordenador externo que realizase los cálculos de navegación y enviase al robot las consignas de movimiento a través de una red WLAN. Esta idea no era la solución más ideal, ya que desde el principio la idea era que el robot fuese lo más autónomo posible sin depender de una infraestructura, sin embargo era una posibilidad directa que no suponía mucho esfuerzo.

\begin{figure}[!htp]
\centering
\includegraphics[width=\textwidth]{figuras/segunda_configuracion_sistema.png}
\caption{Esquema de la segunda configuración del sistema. Basado en \cite{herrero2013}.}
\label{fig:segunda_configuracion_sistema}
\end{figure}

Gracias a la filosofía de ejecución distribuida de nodos de ROS, ejecutar nodos en máquinas diferentes y compartir la información entre procesos es una tarea sencilla. Para configurarlo, tan solo es necesario indicar a las máquinas la IP del nodo MASTER. De esta forma, los nodos que se ejecuten en cada una de las máquinas tratarán de realizar la comunicación a través de IPs dentro de la misma red.

Este ajuste fue puesto en marcha utilizando un ordenador portátil con suficiente capacidad de procesamiento y memoria como para ejecutar la navegación, sin embargo aparecieron algunos inconvenientes. El primero de ellos fueron las direcciones IP en el ordenador interno del Pioneer.

\begin{itemize}

\item \textbf{Problemas con el sensor Láser: LM1xx}\\\\
Debido a que el sensor láser se conecta vía Ethernet a este ordenador, el nodo \textit{LMS1xx} debe obtener información a través de la IP del láser y enviarla a través del adaptador Wifi a la IP del nodo máster. El problema residía en que el nodo se saturaba al tener que lidiar con ambas interfaces de conexión y provocaba su detención.

Tras varias consultas a Clearpath Robotics a través de su repositorio de GitHub y preguntas en el foro ROS Answers **Enlaces de referencia**, la solución no estaba implementada en código y lo más inmediato era hacer un bridge en el ordenador del Pioneer 3 AT entre la interfaz Ethernet y la Wifi.

Los resultados de esta solución no fueron satisfactorios ya que el comportamiento era el mismo: el nodo \textit{LMS1xx} se saturaba e interrumpía a los pocos minutos de su ejecución.

Trantando de resolver este problema, se hicieron pruebas generando una red Wifi Ad-hoc desde el ordenador del robot, a la cual se conectaba el ordenador externo. Los resultados fueron buenos siempre y cuando las IPs del nodo master y del sensor Láser se encontrasen en el mismo subrango.

En la implementación final del sistema esta solución se sigue utilizando para conectarse desde un ordenador externo al ordenador que incorpora el robot Pioneer.
\end{itemize}

Una vez se pudo conectar el ordenador externo, la ejecución del nodo de navegación era la correcta y las consignas de movimiento se enviaban correctamente al robot, pudiendo realizar las primeras pruebas de navegación autónoma.

Sin embargo en ocasiones la recepción y envío de datos era demasiado alta y esto provocaba que existiese mucho retraso en la comunicación, haciendo que el robot reaccionase tarde para esquivar los obstáculos y el control del robot fuera impracticable.

\subsubsection{Tercera configuración del sistema}
Finalmente se optó por montar un ordenador más potente en el robot, para lo cual se utilizó un portátil externo al que se conectaba tanto e sensor Kinect como el sensor Láser y se utilizaba un convertidor de puesto USB a puerto serie RS-232 para controlar el movimiento del robot y leer la odometría. El ordenador del robot quedaba sustituido y así se mantuvo hasta la versión final.

\begin{figure}[!htp]
\centering
\includegraphics[width=0.5\textwidth]{figuras/tercera_configuracion.png}
\caption{Robot con portátil incorporado realizando navegación.}
\label{fig:tercera_configuracion}
\end{figure}


Llegados a este punto, ahora sí disponíamos de la plataforma robótica completa sobre la que trabajar en la navegación del robot. Las primeras pruebas fueron satisfactorias, logrando correr todos los nodos en el portátil, el cual se incorporó de manera provisional al robot por medio de unos agarres realizados con una impresora 3D.

\begin{figure}[!htp]
\centering
\includegraphics[width=0.5\textwidth]{figuras/agarre_portatil.png}
\caption{Soportes para el ordenador portátil.}
\label{fig:agarre_portatil}
\end{figure}

\subsection{Segunda configuración hardware}

La segunda configuración hardware vino dada tras las pruebas satisfactorias con el ordenador portátil como encargado de la ejecución del sistema ROS. Debido a ello se optó por utilizar el ordenador compacto Intel NUC de manera dedicada en el robot, sustituyendo al portátil, dejando más espacio para colocar los sensores y hacer un sistema más integrado.

Una de las principales desventajas con las que contaba la primera configuración hardware era la posición tan adelantada del sensor Kinect, ya que debido a sus características, si un objeto se situa a medio metro o menos delante de la cámara infrarroja, las proyecciones de su emisor de infrarrojos no pueden ser captadas y por tanto obtenemos una nube de puntos vacía (la cámara Kinect se queda ''ciega"). Es por tanto que esta nueva configuración hardware del robot vino motivada por el hecho de retrasar la posición de la cámara y obtener cierta distancia de margen para evitar el efecto anterior.

Tras varias pruebas en el simulador Gazebo (Sección \ref{Gazebo}) cambiando la posición del sensor Kinect, el ordenador Intel NUC y del sensor láser SICK, se aprovechó todo el área del robot de manera que ningún elemento estorbase a los haces de infrarrojos de los dos sensores. El sensor Kinect se situó más retrasado, hacia la mitad del robot, dejando tan solo sitio en la parte trasera para incorporar el resto de elementos. El sensor láser se situó en la parte trasera mirando hacia atrás ya que gracias a su amplio rango de 270º, podíamos obtener lecturas de prácticamente todo el perímetro del robot.

**imagen de la disposición del robot en freecad y pruebas reales**.

El ordenador Intel NUC se situó al lado izquierdo del sensor láser dejando el lado derecho para situar el cableado de los sensores y del ordenador.

Finalmente, para dejar todo el sistema integrado, se diseñaron unos paneles laterales, un sistema de varillas roscadas atornilladas a la base del robot y un panel superior para esconder el cableado interno. Se diseñó un modelo en 3D en el simulador Gazebo para comprobar su correcta compatibilidad con la disposición de los sensores.

\begin{figure}[!htp]
\centering
\includegraphics[width=0.45\textwidth]{figuras/modelo_3D_1.png}
\includegraphics[width=0.45\textwidth]{figuras/modelo_3D_2.png}
\caption{Modelo 3D de la segunda configuración hardware hecho en Gazebo}
\label{fig:modelo_3D}
\end{figure}

Tras comprobar que el diseño funcionaba y cumplía las características necesarias se mecanizó la parte superior del robot para anclar el sensor láser y las varillas roscadas, como se ve en la figura \ref{fig:petrois_laser1}.

\begin{figure}[!htp]
\centering
\includegraphics[width=0.6\textwidth]{figuras/petrois_laser1.jpg}
\caption{Posición retrasada del láser y nuevas varillas de soporte.}
\label{fig:petrois_laser1}
\end{figure}

Para evitar las vibraciones y que el sensor Kinect permaneciese inmóvil y situado a la altura necesaria, se diseñaron e imprimieron en 3D unas piezas de soporte (figura TAL)

\begin{figure}[!htp]
\centering
\includegraphics[width=0.45\textwidth]{figuras/soporte_kinect.png}
\includegraphics[width=0.45\textwidth]{figuras/petrois_kinect.jpg}
\caption{Pieza 3D para el soporte del sensor Kinect y su posición final.}
\label{fig:kinect_final}
\end{figure}

Los paneles laterales y el panel superior se mecanizaron en metacrilato de 3 y 4 milímetros respectivamente con una fresadora y se sujetaron al robot mediante remaches, tornillos y escuadras en forma de L, algunas de las cuales fueron diseñadas en 3D.

**Imagen de los paneles laterales, remaches, escuadra 3D**

El panel de control del robot quedó dividido en dos partes, la destinada al ordenador de abordo y la destinada al microcontrolador del robot. Ambas partes fueron resituadas en los laterales del robot.

**imagenes de los paneles de control laterales**

A continuación se muestran algunas imágenes del robot completo rediseñado.

\begin{figure}[htp]
\centering
\includegraphics[width=0.8\textwidth]{figuras/petrois1.jpg}
\includegraphics[width=0.45\textwidth]{figuras/petrois2.jpg}
\includegraphics[width=0.45\textwidth]{figuras/petrois3.jpg}
\caption{Petrois.}
\label{fig:petrois}
\end{figure}

\textbf{Intel NUC}\\
El ordenador compacto Intel NUC se eligió como unidad de procesamiento y ejecución del sistema ROS debido a sus altas capacidades de procesador y memoria y a su bajo consumo. Éste fue alimentado a 12 voltios de la misma forma que el sensor Kinect y se fijó al chasis del robot mediante tiras del velcro adhesivo.

A él se conectaron el vía puerto USB el sensor Kinect y el cable convertidor de control del robot, el jack de auriculares al altavoz frontal del robot y vía ethernet el sensor láser.


\subsubsection{Configuración del sistema final}

La configuración final del sistema quedó definida como se indica en la figura \ref{fig:esquema_robot_final}. El ordenador Intel NUC se configuró para que generase en su arranque una red Wifi ad-hoc propia, dentro del mismo subrango **revisar nomenclatura** que las direcciones IP del sensor láser para evitar el malfuncionamiento del mismo tal y como se ha indicado anteriormente.

\begin{figure}[htp]
\centering
\includegraphics[width=0.8\textwidth]{figuras/esquema_robot.jpg}
\caption{Esquema del sistema robótico final utilizado en el proyecto.} \label{fig:esquema_robot_final}
\end{figure}

El suministro de energía a todos los elementos que incorpora el robot se realiza a través de la placa de alimentación del robot, la cual se conecta al pack de 3 baterías alojado en su interior.

\section{Navegación}
La navegación es el propósito central de este proyecto. Como ya se ha indicado anteriormente la navegación se centra en el Navigation Stack de ROS.

En el capítulo \ref{chapter:navegacion} vimos una descripción de cada una de sus partes y unos ajustes generales para nuestro robot. En este apartado veremos unos ajustes más específicos y elaborados que son los ajustes finales para la navegación del robot.

El robot ha sido adaptado y configurado de manera óptima basándonos en las características de la navegación en dos dimensiones que ofrece ROS, como hemos visto en el apartado anterior.

\subsection{Configuración de los costmaps y los sensores}\label{subsection:configuracion_costmaps_sensores}
La navegación del robot se basa en la información recogida en los llamados "costmaps".

Para realizar la navegación disponemos de dos costmaps, el llamado "global\_costmap'' y el llamado "local\_costmap". El primero sirve de base al planificador de trayectoria global y el segundo lo hace para el planificador de trayectoria local.

Su comportamiento y posibilidades de configuración son las mismas con la excepción de que el mapa global toma información del mapa que se cargue para realizar navegación (en caso de utilizar uno) además de la información aportada por los sensores.

La misión de los mapas de coste es la tomar la información de los sensores e incorporarla a un mapa de celdillas y marcar o borrar los obstáculos pertinentes. A partir de esa información se calcula un gradiente de coste que asigna un valor a cada una de ellas.

Los mapas de coste tal y como está implementados en ROS integran la información de los sensores en una misma capa, de tal forma que no existe distinción entre el tipo de información que está tomando un sensor u otro. Esto supone un problema añadido en el caso de este proyecto, ya que se pretendía utilizar la información de los dos sensores: Kinect y láser.

\begin{itemize}
\item \textbf{La problemática de los dos sensores}\\\\
El problema consiste en que el sensor Kinect es capaz de tomar información de la posición de os obstáculos a diferente altura pero con un alcance más reducido. Sin embargo, el sensor láser dispone de un alcance mayor, pero no puede detectar obstáculos que queden por encima o por debajo de su haz.

Esto hacía que si se configuraban los mapas de coste con una sola capa de obstáculos, en caso de que el sensor Kinect incorporase un obstáculo al mapa que quedaba por debajo del haz láser, si ese obstáculo dejaba de ser visto por el sensor Kinect pero se encontraba dentro del rango del láser, al quedar por debajo del haz de este último no se detectaba ningún obstáculo y se borraba del mapa.

Esta problemática se producía tanto para el mapa de coste global como para el local.

Esta problemática es bien conocida dentro del mundo de la robótica y se denomina "Fusión sensorial" **referencia** donde una de las técnicas más conocidas es la de fusión mediante Filtro de Kalman **refrencia**.

La solución a esta problemática más adecuada era realizar una composición de los puntos obtenidos por el sensor Láser y el sensor Kinect. Sin embargo, el coste computacional de crear una nueva nube de puntos a partir de dos tipos de datos diferentes a una frecuencia adecuada se antojaba elevado, por lo que la opción más adecuada consistió en utilizar capas de obstáculos diferentes (\textit{costmap\_2d::VoxelLayer}) para cada uno de los sensores.

Así, cada sensor sería capaz de incorporar o borrar obstáculos del mapa solo si eran detectados o no por ese mismo sensor y no por el otro. Si bien es cierto que de esta forma existen duplicidades de los obstáculos al tener que ser incorporados o borrados del mapa por cada sensor de manera independiente, esto nos permite salvar el caso en el que exista un obstáculo y este no se tenga en cuenta por interferencias de los sensores.

\item \textbf{La problemática de la nube de puntos}\\\\
Otro de los problemas a solventar fue la manera en la que gestionar los obstáculos del sensor Kinect.

El gran número de puntos disponibles incrementa mucho el cálculo de los obstáculos si se analiza toda la nube directamente, por lo que es mejor recurrir a analizarla por partes.

Para hacer esta operación se probaron nodos de ros que realizaban la conversión del dato tipo PointCloud2 a tipo LaserScan definiendo parámetros como la altura, distancia...

El nodo \textbf{\textit{PointCloud\_to\_LaserScan}} **referencia** realiza este filtrado y conversión de tipo de datos a partir del análisis de la nube de puntos basándose en la librería PCL \cite{PCL}. Su funcionamiento es correcto y el coste computacional se reduce, sin embargo no se consigue la frecuencia adecuada para que los datos se actualicen a tiempo a medida que el robot navega.

Otro de los nodos utilizados para este propósito es \textbf{\textit{DepthImage\_to\_LaserScan}} **referencia**. En este caso su enfoque es diferente, ya que utiliza los píxeles de la imagen para analizar la nube de puntos de tal modo que solo analiza los puntos correspondientes a un determinado rango de píxeles. Este nodo es mucho más eficiente en el cálculo pero no tiene en cuenta aspectos como la inclinación del sensor o la detección del suelo como obstáculo.

Las pruebas realizadas utilizando este nodo fueron correctas, ya que el procesado de la nube de puntos era más rápido y permitía realizar los cáculos a una frecuencia adecuada. Sin embargo requiere más de una instancia de estos nodos para poder detectar obstáculos bajos a corta (hasta 1.5 metros) y media distancia (hasta unos 3 metros) para que no se produzcan interferencias con el suelo.

De este modo, se crearon 3 instancias diferentes de este nodo: uno para obstáculos situados a una distancia mayor, un segundo para obstáculos a distancias medias, y un tercero para obstáculos a distancias cortas.

\begin{code}[!htp]
\begin{lstlisting}[style=launch]
<launch>
    <node pkg="depthimage_to_laserscan" type="depthimage_to_laserscan" name="depthimage_to_laserscan_long">
        <remap from="image" to="/camera/depth/image_raw"/>
        <remap from="camera_info" to="/camera/depth/camera_info"/>
        <remap from="scan" to="camera/scan_depth_long"/>
        <rosparam>
            scan_height: 10
            scan_time: 0.167
        </rosparam>
    </node>
</launch>
\end{lstlisting}
\caption{Launchfile \textit{DepthImage\_to\_LaserScan} para obstáculos lejanos.}\footnotemark
\end{code}
\footnotetext{\hypersetup{urlcolor=black}
Fuente: \href{https://github.com/danimtb/pioneer3at_ETSIDI/blob/master/pioneer_utils/sensors/kinect_to_laser_long.launch}{\textit{pioneer\_utils/sensors/kinect\_to\_laser\_long.launch}}
\hypersetup{urlcolor=blue}}

\begin{code}[!htp]
\begin{lstlisting}[style=launch]
<launch>
    <node pkg="depthimage_to_laserscan" type="depthimage_to_laserscan" name="depthimage_to_laserscan_long">
        <remap from="image" to="/camera/depth/image_raw"/>
        <remap from="camera_info" to="/camera/depth/camera_info"/>
        <remap from="scan" to="camera/scan_depth_long"/>
        <rosparam>
            scan_height: 10
            scan_time: 0.167
        </rosparam>
    </node>
</launch>
\end{lstlisting}
\caption{Launchfile \textit{DepthImage\_to\_LaserScan} para obstáculos lejanos.}\footnotemark
\end{code}
\footnotetext{\hypersetup{urlcolor=black}
Fuente: \href{https://github.com/danimtb/pioneer3at_ETSIDI/blob/master/pioneer_utils/sensors/kinect_to_laser_long.launch}{\textit{pioneer\_utils/sensors/kinect\_to\_laser\_long.launch}}
\hypersetup{urlcolor=blue}}

\begin{code}[!htp]
\begin{lstlisting}[style=launch]
<launch>
    <node pkg="depthimage_to_laserscan" type="depthimage_to_laserscan" name="depthimage_to_laserscan_long">
        <remap from="image" to="/camera/depth/image_raw"/>
        <remap from="camera_info" to="/camera/depth/camera_info"/>
        <remap from="scan" to="camera/scan_depth_long"/>
        <rosparam>
            scan_height: 10
            scan_time: 0.167
        </rosparam>
    </node>
</launch>
\end{lstlisting}
\caption{Launchfile \textit{DepthImage\_to\_LaserScan} para obstáculos lejanos.}\footnotemark
\end{code}
\footnotetext{\hypersetup{urlcolor=black}
Fuente: \href{https://github.com/danimtb/pioneer3at_ETSIDI/blob/master/pioneer_utils/sensors/kinect_to_laser_long.launch}{\textit{pioneer\_utils/sensors/kinect\_to\_laser\_long.launch}}
\hypersetup{urlcolor=blue}}
\end{itemize}

Una vez solventados los problemas anteriores el ajuste de ambos costmaps se realizó mediante ensayos prueba error con el propio robot y con su modelo creado en el simulador Gazebo.

La configuración de los mismos se expone a continuación.

\begin{code}[!htp]
\begin{lstlisting}[style=launch]
global_costmap:
  global_frame: /map
  robot_base_frame: /base_link
  update_frequency: 2.0
  publish_frequency: 2.0
  static_map: true
  rolling_window: false
  track_unknown_space: true
  plugins:
      - {name: static_layer,       type: "costmap_2d::StaticLayer"}
      - {name: obstacle_layer_kinect,        type: "costmap_2d::VoxelLayer"}
      - {name: obstacle_layer_laser,       type: "costmap_2d::VoxelLayer"}
      - {name: inflation_layer,       type: "costmap_2d::InflationLayer"}

  obstacle_layer_kinect:
    observation_sources: kinect_laser kinect_laser_low kinect_laser_long
    kinect_laser: {sensor_frame: camera_link, data_type: LaserScan, topic: camera/scan_depth, marking: true, clearing: true, obstacle_range: 3.0, raytrace_range: 6.5, inf_is_valid: true}
    kinect_laser_low: {sensor_frame: camera_link, data_type: LaserScan, topic: camera/scan_depth_low, marking: true, clearing: true, obstacle_range: 3.0, raytrace_range: 6.5, inf_is_valid: true}
    kinect_laser_long: {sensor_frame: camera_link, data_type: LaserScan, topic: camera/scan_depth_long, marking: true, clearing: true, obstacle_range: 3.0, raytrace_range: 6.5, inf_is_valid: true}

  obstacle_layer_laser:
    observation_sources: sick_lms1xx
    sick_lms1xx: {sensor_frame: laser, data_type: LaserScan, topic: scan, marking: true, clearing: true, obstacle_range: 5.0, raytrace_range: 10.5, inf_is_valid: true}

  inflation_layer:
    inflation_radius: 0.55
    cost_scaling_factor: 4.0
\end{lstlisting}
\caption{Configuración del \textit{global\_costmap}.}\footnotemark
\end{code}
\footnotetext{\hypersetup{urlcolor=black}
Fuente: \href{https://github.com/danimtb/pioneer3at_ETSIDI/blob/master/pioneer_utils/navigation/global_navigation/global_costmap_params.yaml}{\textit{pioneer\_utils/navigation/global\_navigation/global\_costmap\_params.yaml}}
\hypersetup{urlcolor=blue}}

De especial interés la configuración de los parámetros de la capa \textit{costmap\_2d::InflationLayer} donde se ajusta el radio de "inflado" de los obstáculos así como un valor de escala en el cálculo del coste de cada celda.
Esto determina en gran medida el cálculo de trayectoria global, permitiendo trayectorias más suaves y alejadas de los obstáculos **referencia Imagen**.

**IMAGEN GLOBAL COSTMAP**

Para el mapa local la configuración es muy similar y lo más importante es una frecuencia de actualización del mapa mayor y la ausencia de la capa estática.

\begin{code}[!htp]
\begin{lstlisting}[style=launch]
local_costmap:
  global_frame: /odom
  robot_base_frame: /base_link
  update_frequency: 10.0
  publish_frequency: 10.0
  static_map: false
  rolling_window: true
  width: 6.0
  height: 6.0
  resolution: 0.05
  max_obstacle_height: 0.5
  plugins:
      - {name: obstacle_layer_laser, type: "costmap_2d::VoxelLayer"}
      - {name: obstacle_layer_kinect, type: "costmap_2d::VoxelLayer"}
      - {name: inflation_layer, type: "costmap_2d::InflationLayer"}

  obstacle_layer_kinect:
    observation_sources: kinect_laser kinect_laser_low kinect_laser_long
    kinect_laser: {sensor_frame: camera_link, data_type: LaserScan, topic: camera/scan_depth, marking: true, clearing: true, obstacle_range: 9.0, raytrace_range: 9.5, inf_is_valid: false}
    kinect_laser_low: {sensor_frame: camera_link, data_type: LaserScan, topic: camera/scan_depth_low, marking: true, clearing: true, obstacle_range: 9.0, raytrace_range: 9.5, inf_is_valid: false}
    kinect_laser_long: {sensor_frame: camera_link, data_type: LaserScan, topic: camera/scan_depth_long, marking: true, clearing: true, obstacle_range: 9.0, raytrace_range: 9.5, inf_is_valid: false}
    kinect: {sensor_frame: camera_link, data_type: PointCloud2, topic: camera/depth/points, marking: true, clearing: true, inf_is_valid: true}


  obstacle_layer_laser:
    observation_sources: sick_lms1xx
    sick_lms1xx: {sensor_frame: laser, data_type: LaserScan, topic: scan, marking: true, clearing: true, obstacle_range: 10.0, raytrace_range: 12.0, inf_is_valid: true}

  inflation_layer:
    inflation_radius: 0.55
    cost_scaling_factor: 4.0
\end{lstlisting}
\caption{Configuración del \textit{local\_costmap}.}\footnotemark
\end{code}
\footnotetext{\hypersetup{urlcolor=black}
Fuente: \href{https://github.com/danimtb/pioneer3at_ETSIDI/blob/master/pioneer_utils/navigation/common/local_costmap_params.yaml}{\textit{pioneer\_utils/navigation/common/local\_costmap\_params.yaml}}
\hypersetup{urlcolor=blue}}

\subsection{Configuración de los planificadores de trayectoria}
Los planificadores de trayectoria por defecto en ROS utilizan algoritmos como Dijsktra o A* además de algunos ajustes para el cálculo y parámetros especiales en el caso del planificador de trayectoria local.

\subsubsection{Parámetros del global\_planner}
En ROS, el nodo encargado de realizar el cálculo de la trayectoria global es el denominado global\_planner. Este nodo dispone de los algoritmos de planificación ya implementados que realizan los cálculos de trayectoria.

La configuración del planificador global se realiza a través de parámetros que podemos configurar, distinguiendo entre el uso del algoritmo de Dijsktra o el de A*, utilizar un camino definido por rejilla, etc.

A continuación podemos ver el comportamiento del planificador con diferente configuración en sus parámetros.

\begin{itemize}
\item \textbf{Algoritmo A*}:
\\El planificador de trayectoria A* ofrece los siguientes resultados.

\begin{figure}[htp]
\centering
\includegraphics[width=0.7\textwidth]{figuras/rviz-astar.png}
\caption{Planificador de trayectoria A* visualizado en RViz.}
\label{fig:rviz-astar}
\end{figure}

El espacio analizado es menor que en el caso de Dijkstra, sin embargo el camino final no es el adecuado, ya que es sinuoso y se acerca demasiado a obstáculos y paredes. Eso a simple vista parece un mal funcionamiento del algoritmo, ya que el comportamiento es exagerado.


\item \textbf{Algoritmo de Dijkstra}:
\\La configuración por defecto del planeador realiza el cálculo de trayectoria mediante el algoritmo de Dijsktra.

\begin{figure}[htp]
\centering
\includegraphics[width=0.7\textwidth]{figuras/rviz-dijkstra.png}
\caption{Planificador de trayectoria Dijkstra visualizado en RViz.}
\label{fig:rviz-dijkstra}
\end{figure}

Como se ve en la imagen, el comportamiento de este planificador es más adecuado, trazando una trayectoria que se mantiene equidistante a los obstáculos y mucho más suavizada respecto a A* a pesar de su exploración mucho más masiva del espacio.

\end{itemize}

El algoritmo que se ha utilizado finalmente es el de Dijkstra debido a las pruebas anteriores y a su buen comportamiento en pruebas con el robot.

\begin{code}[!htp]
\begin{lstlisting}[style=launch]
GlobalPlanner:
  old_navfn_behavior: false
  use_quadratic: true
  use_dijkstra: true
  use_grid_path: false
  
  allow_unknown: true 
                      
  planner_window_x: 0.0 
  planner_window_y: 0.0
  default_tolerance: 0.1
  
  publish_scale: 100
  planner_costmap_publish_frequency: 0.0
\end{lstlisting}
\caption{Configuración del \textit{global\_planner}.}\footnotemark
\end{code}
\footnotetext{\hypersetup{urlcolor=black}
Fuente: \href{https://github.com/danimtb/pioneer3at_ETSIDI/blob/master/pioneer_utils/navigation/common/global_planner_params.yaml}{\textit{pioneer\_utils/navigation/common/global\_planner\_params.yaml}}
\hypersetup{urlcolor=blue}}

\subsubsection{Parámetros del local\_planner}
EL planificador de trayectoria local que se ha utilizado es \textit{Trajectory Rollout} por sus buenos resultados en robot con bajas capacidades de aceleración.

Las pruebas realizadas tanto en el simulador como en el propio robot han servido para ajustar los parámetros de velocidad y aceleración así como para mantener un compromiso entre la correcta reacción ante obstáculos locales y el ajuste a la trayectoria global.

\begin{figure}[htp]
\centering
\includegraphics[width=0.5\textwidth]{figuras/rviz-local.png}
\caption{Planificador de trayectoria local \textit{Trajectory Rollout} en RViz.}
\label{fig:rviz-local}
\end{figure}

Los parámetros utilizados son los que se presentan a continuación.

\begin{code}[!htp]
\begin{lstlisting}[style=launch]
TrajectoryPlannerROS:
  max_vel_x: 0.6
  min_vel_x: 0.1
  max_vel_theta: 0.8
  min_in_place_vel_theta: 0.4

  acc_lim_theta: 3.2
  acc_lim_x: 2.5
  acc_lim_y: 2.5

  holonomic_robot: false

  yaw_goal_tolerance: 3.1415

  sim_granularity: 0.025
  sim_time: 2.0
  meter_scoring: true
  pdist_scale: 0.9
  gdist_scale: 0.6
\end{lstlisting}
\caption{Configuración de \textit{base\_local\_planner}.}\footnotemark
\end{code}
\footnotetext{\hypersetup{urlcolor=black}
Fuente: \href{https://github.com/danimtb/pioneer3at_ETSIDI/blob/master/pioneer_utils/navigation/common/base_local_planner_params.yaml}{\textit{pioneer\_utils/navigation/common/base\_local\_planner\_params.yaml}}
\hypersetup{urlcolor=blue}}

\subsection{Navegación con mapa}
Usualmente cuando hablamos de navegación nos referimos a una navegación basada en un mapa previo que se carga en la memoria del robot.

Los mapas utilizados para la navegación han sido todos creados utilizando el paquete \textit{gmapping} de ROS, utilizando el sensor láser del robot para obtener un rango y precisión mayor (Figura \ref{fig:creacion_mapa}).

\begin{figure}[htp]
\centering
\includegraphics[width=0.5\textwidth]{figuras/creacion_mapa.png}
\caption{Creación de un mapa mediante SLAM.}
\label{fig:creacion_mapa}
\end{figure}

Para realizar una navegación con mapa se utiliza un mapa del tipo anterior cargado en memoria acompañado del ya mencionado \textit{global\_costmap} de manera estática.

Esta es la configuración que se ha venido utilizando de manera general y el uso de todos sus elementos queda reflejado en el archivo launch de navegación global

\begin{code}[!htp]
\begin{lstlisting}[style=launch]
<launch>
    <!-- Run the map server -->
    <node name="map_server" pkg="map_server" type="map_server" args="$(find pioneer_utils)/maps/floor_zero-map.yaml"/>

    <!--- Run AMCL -->
    <include file="$(find pioneer_utils)/navigation/common/amcl.launch"/>
	
	<node pkg="move_base" type="move_base" respawn="false" name="move_base" output="screen">
        <rosparam file="$(find pioneer_utils)/navigation/common/costmap_common_params_p3at.yaml" command="load" ns="global_costmap" />
        <rosparam file="$(find pioneer_utils)/navigation/common/costmap_common_params_p3at.yaml" command="load" ns="local_costmap" />
        <rosparam file="$(find pioneer_utils)/navigation/common/local_costmap_params.yaml" command="load" />
        <rosparam file="$(find pioneer_utils)/navigation/global_navigation/global_costmap_params.yaml" command="load" />
        <rosparam file="$(find pioneer_utils)/navigation/common/base_local_planner_params.yaml" command="load"/>
        <rosparam file="$(find pioneer_utils)/navigation/common/global_planner_params.yaml" command="load" />
        <rosparam file="$(find pioneer_utils)/navigation/common/recovery_behaviors.yaml" command="load" />
        <rosparam>
            planner_frequency: 1.0 
        </rosparam>
        <param name="base_global_planner" value="global_planner/GlobalPlanner"/>
    </node>
</launch>
\end{lstlisting}
\caption{Configuración de navegación global.}\footnotemark
\end{code}
\footnotetext{\hypersetup{urlcolor=black}
Fuente: \href{https://github.com/danimtb/pioneer3at_ETSIDI/blob/master/pioneer_utils/navigation/global_navigation/global_navigation_p3at.launch}{\textit{pioneer\_utils/navigation/global\_navigation/global\_navigation\_p3at.launch}}
\hypersetup{urlcolor=blue}}

\subsection{Navegación reactiva}
La navegación reactiva es la que se conoce por carecer de un mapa previo cargado en la memoria del robot. En su caso el robot percibe el entorno a medida que navega construyendo un mapa global de manera dinámica al igual que sucede con la configuración del mapa local.

En este caso la configuración del mapa global carece de capa estática, por lo que el mapa se desplaza junto con el robot.

\begin{code}[!htp]
\begin{lstlisting}[style=launch]
global_costmap:
  global_frame: /odom
  robot_base_frame: /base_link
  update_frequency: 2.0
  publish_frequency: 2.0
  static_map: false
  rolling_window: true
  track_unknown_space: true
  width: 15.0
  height: 15.0
  origin_x: 0.0
  origin_y: 0.0

  plugins:
      - {name: obstacle_layer_kinect,        type: "costmap_2d::VoxelLayer"}
      - {name: obstacle_layer_laser,       type: "costmap_2d::VoxelLayer"}
      - {name: inflation_layer,       type: "costmap_2d::InflationLayer"}

  obstacle_layer_kinect:
    observation_sources: kinect_laser kinect_laser_low kinect_laser_long
    kinect_laser: {sensor_frame: camera_link, data_type: LaserScan, topic: camera/scan_depth, marking: true, clearing: true, obstacle_range: 3.0, raytrace_range: 6.5, inf_is_valid: true}
    kinect_laser_low: {sensor_frame: camera_link, data_type: LaserScan, topic: camera/scan_depth_low, marking: true, clearing: true, obstacle_range: 3.0, raytrace_range: 6.5, inf_is_valid: true}
    kinect_laser_long: {sensor_frame: camera_link, data_type: LaserScan, topic: camera/scan_depth_long, marking: true, clearing: true, obstacle_range: 3.0, raytrace_range: 6.5, inf_is_valid: true}

  obstacle_layer_laser:
    observation_sources: sick_lms1xx
    sick_lms1xx: {sensor_frame: laser, data_type: LaserScan, topic: scan, marking: true, clearing: true, obstacle_range: 5.0, raytrace_range: 12.5, inf_is_valid: true}

  inflation_layer:
    inflation_radius: 0.55
    cost_scaling_factor: 4.0
\end{lstlisting}
\caption{Configuración de \textit{global\_costmap} para navegación reactiva.}\footnotemark
\end{code}
\footnotetext{\hypersetup{urlcolor=black}
Fuente: \href{https://github.com/danimtb/pioneer3at_ETSIDI/blob/master/pioneer_utils/navigation/local_navigation/global_costmap_params.yaml}{\textit{pioneer\_utils/navigation/local\_navigation/global\_costmap\_params.yaml}}
\hypersetup{urlcolor=blue}}

La configuración de la navegación carece mapa y por tanto tampoco es preciso el nodo AMCL para situar al robot en el mismo. La orientación y posición del robot queda determinada por su odometría.

\begin{code}[!htp]
\begin{lstlisting}[style=launch]
<launch>
	<node pkg="move_base" type="move_base" respawn="false" name="move_base" output="screen">
        <rosparam file="$(find pioneer_utils)/navigation/common/costmap_common_params_p3at.yaml" command="load" ns="global_costmap" />
        <rosparam file="$(find pioneer_utils)/navigation/common/costmap_common_params_p3at.yaml" command="load" ns="local_costmap" />
        <rosparam file="$(find pioneer_utils)/navigation/common/local_costmap_params.yaml" command="load" />
        <rosparam file="$(find pioneer_utils)/navigation/local_navigation/global_costmap_params.yaml" command="load" />
        <rosparam file="$(find pioneer_utils)/navigation/common/base_local_planner_params.yaml" command="load"/>
        <rosparam file="$(find pioneer_utils)/navigation/common/global_planner_params.yaml" command="load" />
        <rosparam file="$(find pioneer_utils)/navigation/common/recovery_behaviors.yaml" command="load" />
        <rosparam>
            planner_frequency: 1.0 
        </rosparam>
        <param name="base_global_planner" value="global_planner/GlobalPlanner"/>
    </node>
</launch>
\end{lstlisting}
\caption{Configuración del launchfile para navegación reactiva.}\footnotemark
\end{code}
\footnotetext{\hypersetup{urlcolor=black}
Fuente: \href{https://github.com/danimtb/pioneer3at_ETSIDI/blob/master/pioneer_utils/navigation/local_navigation/local_navigation_p3at.launch}{\textit{pioneer\_utils/navigation/local\_navigation/local\_navigation\_p3at.launch}}
\hypersetup{urlcolor=blue}}

\section{Nodo de navegación por puntos}
El nodo de navegación por puntos ofrece a posibilidad de enviar diferentes puntos de meta al robot tanto en el modo global como en el modo local. esto quiere decir, que ofrece unas funciones capaces de manda al robot a un punto deseado del mapa o hacer que este avance cierta distancia determinada.

El nodo de navegación por puntos \textit{nav-waypoints} está desarrollado en C++ y ofrece un ejemplo de las posibilidades de uso de la navegación del robot. Este nodo se subscribe al estado del nodo de navegación del robot y publica puntos de meta en el \textit{frame} apropiado dependiendo de si es una meta global o local.

\begin{code}[!htp]
\begin{lstlisting}[style=C++]
...
bool setGlobalGoal(const float &x, const float &y, const float &angle)
{
	//tell the action client that we want to spin a thread by default
	MoveBaseClient ac("move_base", true);
	
	//wait for the action server to come up
	while(!ac.waitForServer(ros::Duration(5.0)))
	{
		ROS_INFO("Waiting for the move_base action server to come up");
	}
	
	move_base_msgs::MoveBaseGoal goal;
	goal.target_pose.header.frame_id = "/map";
	goal.target_pose.header.stamp = ros::Time::now();
	
	goal.target_pose.pose.position.x = x;
	goal.target_pose.pose.position.y = y;
	goal.target_pose.pose.orientation = tf::createQuaternionMsgFromYaw(angle);
	ROS_INFO("Sending GLOBAL goal");
	ac.sendGoal(goal);
	
	ac.waitForResult();
	
	if(ac.getState() == actionlib::SimpleClientGoalState::SUCCEEDED)
	{
		ROS_INFO("Hooray, the base moved 1 meter forward");
		return true;
	}
    else
    {
		ROS_INFO("The base failed to move forward 1 meter for some reason");
		return false;
	}
}

int main(int argc, char** argv){
  ros::init(argc, argv, "simple_navigation_goals");
  setGlobalGoal(-0.671, 1.938, 1.0); //rosa
  setGlobalGoal(0.193, -1.520, 1.0); //invernadero
  setGlobalGoal(2.118, -8.223, 1.0); //comau
  setGlobalGoal(-1.073, -9.271, 1.0); //puerta principal
  
  return 0;
}
\end{lstlisting}
\caption{Fragmento de código del nodo \textit{nav-waypoints}.}
\end{code}

Este nodo se encuentra separado del directorio habitual \textit{pioneer\_utils} debido a dependencias adicionales que pueden causar conflicto. Por ello, se aloja en el paquete \textit{navigation\_goals}.

\subsection{Endurance test}
El nodo \textit{endurance\_test} es un nodo de navegación similar al descrito en el apartado anterior pero desarrollado con la API de Python.

Este nodo sirve para realizar un test de resistencia en la navegación del robot con un mapa, mandando al robot diferentes puntos de meta y llevando un registro de las metas alcanzadas, el tiempo transcurrido y los metros recorridos.

Los puntos de meta son leídos desde un archivo de texto plano donde se indica el nombre del punto de meta y las coordenadas X, Y del mismo en el mapa utilizado. Adicionalmente y para dotarlo de utilidad a otros robots es posible configurar el Topic de la odometría, el Topic de velocidad y el tiempo de esperas entre metas.

\begin{table}[!h]
\centering
\resizebox{\textwidth}{!}{
\begin{tabular}{c c c}
& {\bf endurance\_test API} & \\
{\bf Topics suscritos} & {\bf Mensaje} & {\bf Descripción}\\ \hline
odom & nav\_msgs/Odometry & Odometría del robot\\
{\bf Topics publicados} & {\bf Mensaje} & {\bf Descripción}\\ \hline
cmd\_vel & geometry\_msgs/Twist & Publica comandos de velocidad \\
{\bf Parámetros} & {\bf Tipo} & {\bf Descripción}\\
\hline
rest\_time & int & Tiempo de espera (segundos)\\
map\_locations & file & Archivo con los puntos de meta\\
odometry\_topic & string & Nombre del Topic de odometría\\
cmd\_vel\_topic & string & Nombre del Topic de velocidad\\
\end{tabular}
}
\caption{API de endurance\_test}
\label{tabla:endurance_test}
\end{table}

Para lanzar este nodo se utiliza un archivo \textit{launchfile} en el que se indican los parámetros.

\begin{code}[!htp]
\begin{lstlisting}[style=C++]
<launch>
  <node name="endurance_test" pkg="pioneer_utils" type="endurance_test.py" output="screen">
    <param name="map_locations" value="$(find pioneer_utils)/main/locations_lab.txt"/>
    <rosparam>
       odometry_topic: rosaria/pose
       cmd_vel_topic: cmd_vel
       rest_time: 1
     </rosparam>
  </node>
</launch>
\end{lstlisting}
\caption{Archivo \textit{launchfile} para el nodo \textit{endurance\_test}.}
\end{code}



\section{Nodo de guiado (follower)}

El nodo de guiado consiste en el análisis de la nube de puntos que capta el sensor Kinect para detectar un objeto delante y dirigir al robot ajustando sus velocidades para que mantenga la posición hacia ese objeto. Su funcionamiento se basa en el procesamiento de la nube de puntos obtenida a través de los nodos del paquete \textit{freenect\_stack}.

El nodo está originalmente desarrollado para el robot Turtlebot pero es fácil adaptable a otros robots con el propósito de que el robot siga a personas, a otros robot o a objetos en movimiento.

Requiere un Topic de tipo \textit{sensor\_msgs/PointCloud2} al que suscribirse para leer la nube de puntos y un Topic de tipo \textit{geometry\_msgs/Twist} al que publicar los movimientos de giro, avance y retroceso.

\begin{table}[!h]
\centering
\resizebox{\textwidth}{!}{
\begin{tabular}{c c c}
& {\bf turtlebot\_follower API} & \\
{\bf Topics suscritos} & {\bf Mensaje} & {\bf Descripción}\\ \hline
camera/depth/points & sensor\_msgs/PointCloud2 & Recibe la nube de puntos\\
{\bf Topics publicados} & {\bf Mensaje} & {\bf Descripción}\\ \hline
cmd\_vel & geometry\_msgs/Twist & Publica comandos de velocidad\\
{\bf Parámetros} & {\bf Tipo} & {\bf Descripción}\\
\hline
min\_y & double & Posición mínima de puntos en Y\\
max\_y & double & Posición máxima de puntos en Y\\
min\_x & double & Posición mínima de puntos en X\\
max\_x & double & Posición máxima de puntos en Y\\
max\_z & double & Posición máxima de puntos en Y\\
goal\_z & double & Distancia mantenida en el seguimiento\\
z\_scale & double & Factor de escala velocidad trans.\\
x\_scale & double & Factor de escala en velocidad de rot.\\
enabled & bool & Hanilita los movimientos\\
\end{tabular}
}
\caption{API de turtlebot\_follower}
\label{tabla:tabla_follower}
\end{table}

El tratamiento de la nube de puntos se realiza con la librería PCL (PointCloud Library \cite{PCL}) y su funcionamiento es el siguiente:

\begin{enumerate}[1.-]
\item Busca puntos dentro de los límites establecidos.
\item Calcula las dimensiones de los puntos encontrados.
\item Calcula el centroide del la zona destacada.
\item Mueve el robot de manera acorde hasta que alcanza la distancia establecida.
\end{enumerate}

\begin{code}[!htp]
\begin{lstlisting}[style=launch]
<launch>
    
<!--  Load turtlebot follower into the 3d sensors nodelet manager to avoid pointcloud serializing -->
    <node pkg="nodelet" type="nodelet" name="turtlebot_follower" args="load turtlebot_follower/TurtlebotFollower camera/camera_nodelet_manager">
      <remap from="turtlebot_follower/cmd_vel" to="/cmd_vel"/>
      <remap from="depth/points" to="camera/depth/points"/>
      <param name="enabled" value="true" />
      <param name="x_scale" value="10.0" />
      <param name="z_scale" value="10.0" />
      <param name="min_x" value="-0.35" />
      <param name="max_x" value="0.35" />
      <param name="min_y" value="0.1" />
      <param name="max_y" value="0.5" />
      <param name="max_z" value="1.2" />
      <param name="goal_z" value="0.6" />
    </node>
</launch>
\end{lstlisting}
\caption{Launchfile para \textit{turtlebot\_follower} en el robot Pioneer 3 AT\footnotemark.}
\end{code}
\footnotetext{\hypersetup{urlcolor=black}
Fuente: \href{https://github.com/danimtb/pioneer3at_ETSIDI/blob/master/pioneer_utils/follower/simple-follower.launch}{\textit{pioneer\_utils/follower/simple-follower.launch}}
\hypersetup{urlcolor=blue}}

\section{Feedback mediante text-to-speech}
Como ya se ha visto, la variedad de paquetes de software robótico en ROS es notable y ofrece una amplia variedad de características gracias a los aportes de la comunidad. A medida que evolucionaba este proyecto y debido a que el robot lleva incorporado su propio altavoz, apareció la idea de dotar de sonidos al robot de tal manera que existiera un feedback hacia las personas que se encuentren en su entorno.

Existe un nodo en ROS llamado \textit{sound\_play} \cite{soundplay} que permite, mediante la publicación de mensajes, reproducir sonidos preincorporados, archivos de sonido OGG/WAV o incluso realizar síntesis de voz a partir de un texto, conocido como \textit{text-to-speech} (TTS), utilizando voces del \textit{Festival Speech Synthesis System} desarrollado por \textit{The Centre for Speech Technology Research} de la Universidad de Edinburgo \cite{festival2014}.

En este caso se ha utilizado la API de Python para interactuar con el nodo de tal modo que tan solo es necesario intercambiar mensajes con el nodo \textit{soundplay\_node}. Para ponerlo en marcha se incia el nodo desde el archivo de \textit{launch}.

\begin{code}[!htp]
\begin{lstlisting}[style=launch]
  <node name="sound_play" pkg="sound_play" type="soundplay_node.py"/>
\end{lstlisting}
\caption{Nodo soundplay\_node para reproducir sonidos.}
\end{code}

Para reproducir sonidos se utiliza un ''handle'' de sonido que facilita e paso de mensajes en caso de reproducir sonidos o voz con las funciones \textit{.play()} y \textit{.say()} respectivamente.
A continuación se presenta un fragmento del nodo \textit{voice\_cmd} de desarrollo propio y que se describirá más adelante

\begin{code}[!htp]
\begin{lstlisting}[style=launch]
  from sound_play.libsoundplay import SoundClient
  
   # Create the sound client object
          self.soundhandle = SoundClient()
         
          rospy.sleep(1)
          self.soundhandle.stopAll()
          
           # Subscribe to the move_base action server
          self.move_base = actionlib.SimpleActionClient("move_base", MoveBaseAction)
          
          rospy.loginfo("Waiting for move_base action server...")
          self.soundhandle.play(1)
          
          # Wait 60 seconds for the action server to become available
          self.move_base.wait_for_server(rospy.Duration(60))
          
          rospy.loginfo("Connected to move base server")
         
          # Announce that we are ready for input
          rospy.sleep(1)
          self.soundhandle.say('Hi, my name is Petrois')
          rospy.sleep(2)
          self.soundhandle.say("Say one of the navigation commands")
\end{lstlisting}
\caption{Fragmento del nodo \textit{voice\_cmd} utilizando el cliente de sonido de \textit{soundplay\_node}.}
\end{code}

\section{Reconocimiento de comandos de voz}
Siguiendo con la idea de utilizar todos los elementos que incorpora el robot y de manera adicional a los objetivos del proyecto, se exploró la idea de la interacción con el robot mediante comandos de voz, gracias una vez más a los paquetes de ROS.

\textit{pocketsphinx} \cite{pocketsphinx2012} es un paquete que actúa como \textit{wrapper} del motor de reconocimiento de voz del mismo nombre, que utiliza el framework multimedia \textit{GStreamer} \cite{gstreamer2001}. La implementación de este nodo está basada en los estudios e investigaciones sobre reconocimiento de voz y patrones en el habla de la Universidad de Carnegie Mellon dentro del proyecto CMU Sphinx \cite{sphinx}.

\section{Nodo de ejecución automática de nodos}