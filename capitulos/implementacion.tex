\chapter{Implementación del sistema}\label{chapter:implementacion}

Este capítulo describe los cambios, ajustes y modificaciones que, basados en la información anterior expuesta, las características de ROS y el hardware del que disponemos, se han realizado para alcanzar los objetivos del proyecto.

\section{Configuraciones hardware}
Como ya se ha descrito, la navegación se basa en el sensor Kinect y en el sensor láser Sick. Estos sensores han sido incorporados al robot de manera que disponen de alimentación desde las baterías del mismo y su interfaz de conexión es conexión la correspondiente en cada caso.

En esta sección se describen las implementaciones que se han llevado a cabo en el hardware utilizado en este proyecto.

\subsection{Pioneer 3 AT}
El robot Pioneer 3 AT de Adept Mobile Robots es la base de la plataforma robótica. El modelo que ha sido utilizado en el laborarotio de la Escuela Técnica Superior de Ingeniería y Diseño Industrial llevaba incorporado un ordenador de tipo dual-core\footnote{Ordenadores integrados en los robots de Mobile Robots: \url{http://www.mobilerobots.com/Accessories/EmbeddedComputers.aspx}}. Al comienzo de este proyecto ya existían algunas adaptaciones como la incorporación de un altavoz frontal, acceso a los puestos USB del ordenador interno y conexión para el sensor láser (Figura \ref{fig:robot_inicial}).

\begin{figure}[!htp]
\centering
\includegraphics[width=0.8\textwidth]{figuras/robot_inicial.png}
\caption{Estado del robot al comienzo del proyecto \cite{herrero2013}.}
\label{fig:robot_inicial}
\end{figure}

El robot había sido utilizado mediante el software MRCore \cite{mrcore} en el proyecto anterior \cite{herrero2013} y el sistema operativo del ordenador interno era Ubuntu Server 10.

Para integrar la versión Indigo de ROS lo más recomendable era partir de la versión estable más actualizada de Ubuntu, por lo que se sustituyó el sistema operativo por Ubuntu 14.04 LTS en su versión de escritorio.

Una vez integrado el sistema operativo, la primera toma de contacto con el robot fue a partir de la librería Aria para controlar el movimiento de los motores y comprobar que el robot se encontraba en buen estado.

A continuación, tras instalar ROS Indigo, se procedió a las pruebas mediante el paquete Rosaria de ROS. La conexión con el microcontrolador de la placa de motores fue exitosa y se comprobó que los valores de la odometría también funcionaban.

Llegados a este punto, el robot se encontraba en disposición para realizar las primeras pruebas.

\subsection{Sensor Láser}\label{subsection:implementacion_laser}
El sensor láser Sick también había sido integrado en un proyecto anterior y sus conexiones de alimentación y datos vía Ethernet ya estaban preparadas para utilizarlo.

Para conectarlo a través del puerto Ethernet fue necesario ajustar su dirección IP a través del software del fabricante y ajustar la IP del ordenador del robot Pioneer (más información en el apéndice \ref{subsection:sicklms100_apendice}).

El agarre mecánico del sensor se dejó tal y como había sido utilizado en ocasiones anteriores, situado en la parte frontal agarrado mediante un par de tornillos al chasis con tuercas de palometa para su fácil manipulación.

El sensor láser se conecta a la interfaz ROS mediante el paquete \textit{LMS1xx} tal y como se describió en el apartado \ref{subsection:sicklms100}.

\subsection{Sensor Kinect}\label{subsection:implementacion_kinect}
La integración del sensor Kinect fue relativamente sencilla debido a que las entradas de los puestos USB del ordenador habían sido cableadas previamente. La adaptación a realizar era sobre la parte de alimentación, ya que este sensor trabaja a una tensión de 12 voltios.

En el manual del robot se encuentra una descripción detallada de la placa de alimentación a la cual pueden conectarse diferentes periféricos. Esta placa ofrece tomas de conexión de 5 voltios controlados por unos botones auxiliares y tomas de 12 voltios (ver apéndice \label{section:placa_alimentacion}).

El sensor Kinect dispone de un adaptador USB, preparado para trabajar con la videoconsola XBOX 360, el cual suministra 12 voltios mediante un transformador conectado a una toma de corriente alterna de 220v e incorpora los cables de datos del propio sensor Kinect.

\begin{figure}[htp]
\centering
\includegraphics[width=0.6\textwidth]{figuras/kinect_usual.png}
\caption{Conexión usual del sensor Kinect a la consola Xbox.}
\label{fig:kinect_usual}
\end{figure}

Para integrar el sensor Kinect en el robot, se adaptó el cable de alimentación soldando unas clavijas de conexión para obtener directamente alimentación a 12 voltios de la placa del robot. También se realizó lo oportuno en el adaptador de corriente, para poder usar el sensor Kinect de la manera habitual (Figura \ref{fig:cables_kinect}).

\begin{figure}[!htp]
\centering
\includegraphics[width=0.45\textwidth]{figuras/adaptador_kinect.png}
\includegraphics[width=0.45\textwidth]{figuras/cable_kinect.png}
\caption{Adaptación de cables para la alimentación del sensor Kinect (izq.) y cable a 12V de la placa de alimentación del robot (dcha.).}
\label{fig:cables_kinect}
\end{figure}

Para anclar el sensor Kinect al robot se optó por situarlo en la parte superior del sensor Láser, para lo cual se diseñó una pieza que encajase en la base de la Kinect y en el sensor láser (Figura )\ref{fig:primera_configuracion}).

El sensor Kinect se conecta a la interfaz ROS mediante el paquete freenect\_stack tal y como se describió en el apartado \ref{subsection:kinect}.

\subsection{Primera configuración hardware}
La primera configuración del robot consistió en ambos sensores situados en la parte frontal del mismo (Figura )\ref{fig:primera_configuracion}). Los sensores se encontraban colocados de manera vertical otro, de tal forma que no existieran interferencias en su rango de detección.

\begin{figure}[!htp]
\centering
\includegraphics[width=\textwidth]{figuras/primera_configuracion.jpg}
\caption{Primera configuración hardware del robot: Kinect y láser en la parte frontal.}
\label{fig:primera_configuracion}
\end{figure}

De esta forma se consigue una vista despejada y contamos con la información del láser para detectar obstáculos laterales.

\subsubsection{Primera configuración del sistema}
El ordenador interno corría todos los nodos de ROS, de modo que se disponía de la información de los sensores, el control sobre los motores y la lectura de la odometría para realizar las primeras pruebas con el paquete de navegación de ROS (Sección \ref{section:navigation_stack}).

Sin embargo, la primera implementación con los primeros ajustes de los sensores no fue posible debido a la sobrecarga de la CPU del ordenador interno del robot Pioneer y a problemas de memoria en la ejecución de nodos como \textit{amcl}.

\subsubsection{Segunda configuración del sistema}
La siguiente opción ha consistido en utilizar un ordenador externo para realizar los cálculos de navegación y enviar al robot las consignas de movimiento a través de una red inalámbrica (Figura \ref{fig:segunda_configuracion_sistema}). Esta idea no era la solución más ideal, ya que desde el principio la idea era que el robot fuese lo más autónomo posible sin depender de una infraestructura, sin embargo esta configuración no suponía mucho esfuerzo debido a que los nodos pueden ejecutarse de manera distribuida.

\pagebreak

\begin{figure}[!htp]
\centering
\includegraphics[width=\textwidth]{figuras/segunda_configuracion_sistema.png}
\caption{Esquema de la segunda configuración del sistema. Basado en \cite{herrero2013}.}
\label{fig:segunda_configuracion_sistema}
\end{figure}

Esta configuración fue probada utilizando un ordenador portátil con suficiente capacidad de procesamiento y memoria como para ejecutar la navegación, sin embargo aparecieron algunos inconvenientes relacionados con la comunicación. Estos problemas se detallan a continuación.

\begin{itemize}

\item \textbf{Problemas con el sensor Láser: LMS1xx}\\\\
Debido a que el sensor láser se conecta vía Ethernet al ordenador del robot, el nodo \textit{LMS1xx} debe obtener información a través de la IP del láser y enviarla a través del adaptador Wifi a la IP del nodo MASTER. El problema reside en que el nodo se saturaba al tener que lidiar con ambas interfaces de conexión y provocaba su detención.

Tras varias consultas a \textit{Clearpath Robotics} a través de su repositorio de \textit{GitHub} y preguntas en el foro \textit{ROS Answers} (ver sección \ref{section:preguntas}), la solución no estaba implementada en código y lo más inmediato era hacer un \textit{bridge} en el ordenador del Pioneer 3 AT entre la interfaz Ethernet y la Wifi.

Los resultados de esta solución no fueron satisfactorios ya que el comportamiento era el mismo: el nodo \textit{LMS1xx} se saturaba e interrumpía a los pocos minutos de su ejecución.

Tratando de resolver este problema, se hicieron pruebas generando una red Wifi Ad-hoc desde el ordenador del robot, a la cual se conectaba el ordenador externo. Con esta configuración resultados son satisfactorios siempre y cuando las IPs del nodo MASTER y del sensor Láser se encuentren en el mismo subrango.

Esta solución es la implementada en el sistema final.
\end{itemize}

Una vez conectado el ordenador externo, la ejecución del nodo de navegación es correcta y las consignas de movimiento se envían correctamente al robot, pudiendo realizar las primeras pruebas de navegación autónoma.

Sin embargo en ocasiones la recepción y el envío de datos era demasiado alta y esto provocaba que existiese mucho retraso en la comunicación, haciendo que el robot reaccionase tarde para esquivar los obstáculos y el control del robot fuera impracticable.

\subsubsection{Tercera configuración del sistema}
Finalmente se optó por montar un ordenador más potente en el robot, para lo cual se utilizó un portátil externo al que se conectaba tanto el sensor Kinect como el sensor láser. Para controlar el movimiento del robot y leer la odometría se utilizaba un convertidor de puesto USB a puerto serie RS-232. El ordenador del robot quedó por tanto apartado, sustituido por la incorporación del ordenador portátil mediante unos soportes realizados a medida en impresión 3D. Con esta configuración se realizaron las primeras pruebas de navegación autónoma del robot (Figura \ref{fig:tercera_configuracion}).

\begin{figure}[!htp]
\centering
\includegraphics[width=0.5\textwidth]{figuras/tercera_configuracion.png}
\includegraphics[width=0.28\textwidth]{figuras/agarre_portatil.png}
\caption{Robot con portátil incorporado realizando navegación y soporte realizado con la impresora 3D.}
\label{fig:tercera_configuracion}
\end{figure}


\subsection{Segunda configuración hardware}

La segunda configuración hardware vino dada tras las pruebas satisfactorias con el ordenador portátil como encargado de la ejecución del sistema ROS. Debido a ello se optó por utilizar el ordenador compacto Intel NUC de manera dedicada en el robot, sustituyendo al portátil, dejando más espacio para colocar los sensores y hacer un sistema más integrado.

Una de las principales desventajas con las que contaba la primera configuración hardware era la posición tan adelantada del sensor Kinect, ya que debido a sus características, si un objeto se situaba a medio metro o menos delante, las proyecciones de su emisor infrarrojo no pueden ser captadas por la cámara por lo que obtenemos una nube de puntos vacía (se dice que el sensor Kinect se queda ''ciego"). La nueva configuración hardware del robot vino dada por el hecho de retrasar la posición de del sensor y obtener cierta distancia de margen para evitar el efecto anterior.

Tras varias pruebas en el simulador Gazebo (Sección \ref{Gazebo}) cambiando la posición del sensor Kinect, el ordenador Intel NUC y del sensor láser Sick, se aprovechó todo el área del robot de manera que ningún elemento estorbase a los haces de infrarrojos de ambos sensores.

En esta nueva configuración el sensor Kinect se ha situado más retrasado, hacia la mitad del robot, dejando tan solo sitio en la parte trasera para incorporar el resto de elementos. El sensor láser se ha situado en la parte trasera mirando hacia atrás ya que gracias a su rango de 270º se obtienen lecturas de prácticamente todo el perímetro del robot (Figura \ref{fig:configuracion3}).

\begin{figure}[!htp]
\centering
\includegraphics[width=0.8\textwidth]{figuras/configuracion3.png}
\caption{Esquema de la nueva configuración hardware.}
\label{fig:configuracion3}
\end{figure}

El ordenador Intel NUC se ha situado al lado izquierdo del sensor láser dejando el lado derecho para situar el cableado de los sensores y del ordenador.

Finalmente, para dejar todo el sistema integrado, se han diseñado unos paneles laterales, un sistema de varillas roscadas atornilladas a la base del robot y un panel superior para esconder el cableado interno. Previamente se diseñó un modelo en 3D en el simulador \textit{Gazebo} para comprobar su correcta compatibilidad con la disposición de los sensores.

\begin{figure}[!htp]
\centering
\includegraphics[width=0.45\textwidth]{figuras/modelo_3D_1.png}
\includegraphics[width=0.45\textwidth]{figuras/modelo_3D_2.png}
\caption{Modelo 3D de la segunda configuración hardware hecho en Gazebo}
\label{fig:modelo_3D}
\end{figure}

Tras comprobar que el diseño funcionaba y cumplía las características necesarias se mecanizó la parte superior del robot para anclar el sensor láser y las varillas roscadas, como se ve en la figura \ref{fig:petrois_laser1}.

\begin{figure}[!htp]
\centering
\includegraphics[width=0.6\textwidth]{figuras/petrois_laser1.jpg}
\caption{Posición retrasada del láser y nuevas varillas de soporte.}
\label{fig:petrois_laser1}
\end{figure}

Además, para evitar vibraciones, anclar el sensor Kinect a la base del robot y situarlo a la altura necesaria, se diseñaron e imprimieron en 3D unas piezas de soporte (Figura \ref{fig:kinect_final}).

\begin{figure}[!htp]
\centering
\includegraphics[width=0.45\textwidth]{figuras/soporte_kinect.png}
\includegraphics[width=0.45\textwidth]{figuras/petrois_kinect.jpg}
\caption{Pieza 3D para el soporte del sensor Kinect y su posición final.}
\label{fig:kinect_final}
\end{figure}

Los paneles laterales y el panel superior se mecanizaron en metacrilato de 3 y 4 milímetros respectivamente con una fresadora y se sujetaron al robot mediante remaches, tornillos y escuadras en forma de L, algunas de las cuales fueron diseñadas también en 3D.

El panel de control del robot quedó dividido en dos partes, la destinada al ordenador de abordo y la destinada al microcontrolador del robot. Ambas partes fueron recolocadas en los laterales del robot para facilitar su acceso.

\pagebreak

\begin{figure}[htp]
\centering
\includegraphics[width=0.45\textwidth]{figuras/panel_lateral.jpg}
\caption{Panel lateral.}
\label{fig:panel_lateral}
\end{figure}

A continuación se muestran algunas imágenes del rediseño del robot.

\begin{figure}[htp]
\centering
\includegraphics[width=0.8\textwidth]{figuras/petrois1.jpg}
\includegraphics[width=0.45\textwidth]{figuras/petrois2.jpg}
\includegraphics[width=0.45\textwidth]{figuras/petrois3.jpg}
\caption{Imágenes de la implementación final.}
\label{fig:petrois}
\end{figure}

\subsubsection{Intel NUC}
El ordenador compacto Intel NUC se eligió como unidad de procesamiento y ejecución del sistema ROS debido a sus altas capacidades de procesador y memoria y a su bajo consumo. Éste fue alimentado a 12 voltios de la misma forma que el sensor Kinect y se fijó al chasis del robot mediante tiras del velcro adhesivo.

A él se conectaron el vía puerto USB el sensor Kinect y el cable convertidor de control del robot, el jack de auriculares al altavoz frontal del robot y vía ethernet el sensor láser.


\subsubsection{Configuración del sistema final}

La configuración final del sistema quedó definida como se indica en la figura \ref{fig:esquema_robot_final}. El ordenador Intel NUC se configuró para que generase en su arranque una red Wifi ad-hoc propia, dentro del mismo subrango que las direcciones IP del sensor láser para evitar el malfuncionamiento del mismo tal y como se ha indicado anteriormente.

\begin{figure}[htp]
\centering
\includegraphics[width=0.8\textwidth]{figuras/esquema_robot.jpg}
\caption{Esquema del sistema robótico final utilizado en el proyecto.} \label{fig:esquema_robot_final}
\end{figure}

El suministro de energía a todos los elementos que incorpora el robot se realiza a través de la placa de alimentación del robot, la cual se conecta al pack de 3 baterías alojado en su interior.

\section{Navegación}
La navegación es el propósito central de este proyecto. Como ya se ha indicado anteriormente la navegación se centra en el Navigation Stack de ROS.

En el capítulo \ref{chapter:navegacion} vimos una descripción de cada una de sus partes y unos ajustes generales para nuestro robot. En este apartado veremos unos ajustes más específicos y elaborados que son los ajustes finales para la navegación del robot.

El robot ha sido adaptado y configurado de manera óptima basándonos en las características de la navegación en dos dimensiones que ofrece ROS, como hemos visto en el apartado anterior.

\subsection{Configuración de los costmaps y los sensores}\label{subsection:configuracion_costmaps_sensores}
La navegación del robot se basa en la información recogida en los llamados "costmaps".

Para realizar la navegación disponemos de dos costmaps, el llamado "global\_costmap'' y el llamado "local\_costmap". El primero sirve de base al planificador de trayectoria global y el segundo lo hace para el planificador de trayectoria local.

Su comportamiento y posibilidades de configuración son las mismas con la excepción de que el mapa global toma información del mapa que se cargue para realizar navegación (en caso de utilizar uno) además de la información aportada por los sensores.

La misión de los mapas de coste es la tomar la información de los sensores e incorporarla a un mapa de celdillas y marcar o borrar los obstáculos pertinentes. A partir de esa información se calcula un gradiente de coste que asigna un valor a cada una de ellas.

Los mapas de coste tal y como está implementados en ROS integran la información de los sensores en una misma capa, de tal forma que no existe distinción entre el tipo de información que está tomando un sensor u otro. Esto supone un problema añadido en el caso de este proyecto, ya que se pretendía utilizar la información de los dos sensores: Kinect y láser.

\begin{itemize}
\item \textbf{La problemática de los dos sensores}\\\\
El problema consiste en que el sensor Kinect es capaz de tomar información de la posición de os obstáculos a diferente altura pero con un alcance más reducido. Sin embargo, el sensor láser dispone de un alcance mayor, pero no puede detectar obstáculos que queden por encima o por debajo de su haz.

Esto hacía que si se configuraban los mapas de coste con una sola capa de obstáculos, en caso de que el sensor Kinect incorporase un obstáculo al mapa que quedaba por debajo del haz láser, si ese obstáculo dejaba de ser visto por el sensor Kinect pero se encontraba dentro del rango del láser, al quedar por debajo del haz de este último no se detectaba ningún obstáculo y se borraba del mapa.

Esta problemática se producía tanto para el mapa de coste global como para el local.

Esta problemática es bien conocida dentro del mundo de la robótica y se denomina "Fusión sensorial" **referencia** donde una de las técnicas más conocidas es la de fusión mediante Filtro de Kalman **refrencia**.

La solución a esta problemática más adecuada era realizar una composición de los puntos obtenidos por el sensor Láser y el sensor Kinect. Sin embargo, el coste computacional de crear una nueva nube de puntos a partir de dos tipos de datos diferentes a una frecuencia adecuada se antojaba elevado, por lo que la opción más adecuada consistió en utilizar capas de obstáculos diferentes (\textit{costmap\_2d::VoxelLayer}) para cada uno de los sensores.

Así, cada sensor sería capaz de incorporar o borrar obstáculos del mapa solo si eran detectados o no por ese mismo sensor y no por el otro. Si bien es cierto que de esta forma existen duplicidades de los obstáculos al tener que ser incorporados o borrados del mapa por cada sensor de manera independiente, esto nos permite salvar el caso en el que exista un obstáculo y este no se tenga en cuenta por interferencias de los sensores.

\item \textbf{La problemática de la nube de puntos}\\\\
Otro de los problemas a solventar fue la manera en la que gestionar los obstáculos del sensor Kinect.

El gran número de puntos disponibles incrementa mucho el cálculo de los obstáculos si se analiza toda la nube directamente, por lo que es mejor recurrir a analizarla por partes.

Para hacer esta operación se probaron nodos de ros que realizaban la conversión del dato tipo PointCloud2 a tipo LaserScan definiendo parámetros como la altura, distancia...

El nodo \textbf{\textit{PointCloud\_to\_LaserScan}} **referencia** realiza este filtrado y conversión de tipo de datos a partir del análisis de la nube de puntos basándose en la librería PCL \cite{PCL}. Su funcionamiento es correcto y el coste computacional se reduce, sin embargo no se consigue la frecuencia adecuada para que los datos se actualicen a tiempo a medida que el robot navega.

Otro de los nodos utilizados para este propósito es \textbf{\textit{DepthImage\_to\_LaserScan}} **referencia**. En este caso su enfoque es diferente, ya que utiliza los píxeles de la imagen para analizar la nube de puntos de tal modo que solo analiza los puntos correspondientes a un determinado rango de píxeles. Este nodo es mucho más eficiente en el cálculo pero no tiene en cuenta aspectos como la inclinación del sensor o la detección del suelo como obstáculo.

Las pruebas realizadas utilizando este nodo fueron correctas, ya que el procesado de la nube de puntos era más rápido y permitía realizar los cáculos a una frecuencia adecuada. Sin embargo requiere más de una instancia de estos nodos para poder detectar obstáculos bajos a corta (hasta 1.5 metros) y media distancia (hasta unos 3 metros) para que no se produzcan interferencias con el suelo.

De este modo, se crearon 3 instancias diferentes de este nodo: uno para obstáculos situados a una distancia mayor, un segundo para obstáculos a distancias medias, y un tercero para obstáculos a distancias cortas.

\begin{code}[!htp]
\begin{lstlisting}[style=launch]
<launch>
    <node pkg="depthimage_to_laserscan" type="depthimage_to_laserscan" name="depthimage_to_laserscan_long">
        <remap from="image" to="/camera/depth/image_raw"/>
        <remap from="camera_info" to="/camera/depth/camera_info"/>
        <remap from="scan" to="camera/scan_depth_long"/>
        <rosparam>
            scan_height: 10
            scan_time: 0.167
        </rosparam>
    </node>
</launch>
\end{lstlisting}
\caption{Launchfile \textit{DepthImage\_to\_LaserScan} para obstáculos lejanos.}\footnotemark
\end{code}
\footnotetext{\hypersetup{urlcolor=black}
Fuente: \href{https://github.com/danimtb/pioneer3at_ETSIDI/blob/master/pioneer_utils/sensors/kinect_to_laser_long.launch}{\textit{pioneer\_utils/sensors/kinect\_to\_laser\_long.launch}}
\hypersetup{urlcolor=blue}}

\begin{code}[!htp]
\begin{lstlisting}[style=launch]
<launch>
    <node pkg="depthimage_to_laserscan" type="depthimage_to_laserscan" name="depthimage_to_laserscan_long">
        <remap from="image" to="/camera/depth/image_raw"/>
        <remap from="camera_info" to="/camera/depth/camera_info"/>
        <remap from="scan" to="camera/scan_depth_long"/>
        <rosparam>
            scan_height: 10
            scan_time: 0.167
        </rosparam>
    </node>
</launch>
\end{lstlisting}
\caption{Launchfile \textit{DepthImage\_to\_LaserScan} para obstáculos lejanos.}\footnotemark
\end{code}
\footnotetext{\hypersetup{urlcolor=black}
Fuente: \href{https://github.com/danimtb/pioneer3at_ETSIDI/blob/master/pioneer_utils/sensors/kinect_to_laser_long.launch}{\textit{pioneer\_utils/sensors/kinect\_to\_laser\_long.launch}}
\hypersetup{urlcolor=blue}}

\begin{code}[!htp]
\begin{lstlisting}[style=launch]
<launch>
    <node pkg="depthimage_to_laserscan" type="depthimage_to_laserscan" name="depthimage_to_laserscan_long">
        <remap from="image" to="/camera/depth/image_raw"/>
        <remap from="camera_info" to="/camera/depth/camera_info"/>
        <remap from="scan" to="camera/scan_depth_long"/>
        <rosparam>
            scan_height: 10
            scan_time: 0.167
        </rosparam>
    </node>
</launch>
\end{lstlisting}
\caption{Launchfile \textit{DepthImage\_to\_LaserScan} para obstáculos lejanos.}\footnotemark
\end{code}
\footnotetext{\hypersetup{urlcolor=black}
Fuente: \href{https://github.com/danimtb/pioneer3at_ETSIDI/blob/master/pioneer_utils/sensors/kinect_to_laser_long.launch}{\textit{pioneer\_utils/sensors/kinect\_to\_laser\_long.launch}}
\hypersetup{urlcolor=blue}}
\end{itemize}

Una vez solventados los problemas anteriores el ajuste de ambos costmaps se realizó mediante ensayos prueba error con el propio robot y con su modelo creado en el simulador Gazebo.

La configuración de los mismos se expone a continuación.

\begin{code}[!htp]
\begin{lstlisting}[style=launch]
global_costmap:
  global_frame: /map
  robot_base_frame: /base_link
  update_frequency: 2.0
  publish_frequency: 2.0
  static_map: true
  rolling_window: false
  track_unknown_space: true
  plugins:
      - {name: static_layer,       type: "costmap_2d::StaticLayer"}
      - {name: obstacle_layer_kinect,        type: "costmap_2d::VoxelLayer"}
      - {name: obstacle_layer_laser,       type: "costmap_2d::VoxelLayer"}
      - {name: inflation_layer,       type: "costmap_2d::InflationLayer"}

  obstacle_layer_kinect:
    observation_sources: kinect_laser kinect_laser_low kinect_laser_long
    kinect_laser: {sensor_frame: camera_link, data_type: LaserScan, topic: camera/scan_depth, marking: true, clearing: true, obstacle_range: 3.0, raytrace_range: 6.5, inf_is_valid: true}
    kinect_laser_low: {sensor_frame: camera_link, data_type: LaserScan, topic: camera/scan_depth_low, marking: true, clearing: true, obstacle_range: 3.0, raytrace_range: 6.5, inf_is_valid: true}
    kinect_laser_long: {sensor_frame: camera_link, data_type: LaserScan, topic: camera/scan_depth_long, marking: true, clearing: true, obstacle_range: 3.0, raytrace_range: 6.5, inf_is_valid: true}

  obstacle_layer_laser:
    observation_sources: sick_lms1xx
    sick_lms1xx: {sensor_frame: laser, data_type: LaserScan, topic: scan, marking: true, clearing: true, obstacle_range: 5.0, raytrace_range: 10.5, inf_is_valid: true}

  inflation_layer:
    inflation_radius: 0.55
    cost_scaling_factor: 4.0
\end{lstlisting}
\caption{Configuración del \textit{global\_costmap}.}\footnotemark
\end{code}
\footnotetext{\hypersetup{urlcolor=black}
Fuente: \href{https://github.com/danimtb/pioneer3at_ETSIDI/blob/master/pioneer_utils/navigation/global_navigation/global_costmap_params.yaml}{\textit{pioneer\_utils/navigation/global\_navigation/global\_costmap\_params.yaml}}
\hypersetup{urlcolor=blue}}

De especial interés la configuración de los parámetros de la capa \textit{costmap\_2d::InflationLayer} donde se ajusta el radio de "inflado" de los obstáculos así como un valor de escala en el cálculo del coste de cada celda.
Esto determina en gran medida el cálculo de trayectoria global, permitiendo trayectorias más suaves y alejadas de los obstáculos **referencia Imagen**.

**IMAGEN GLOBAL COSTMAP**

Para el mapa local la configuración es muy similar y lo más importante es una frecuencia de actualización del mapa mayor y la ausencia de la capa estática.

\begin{code}[!htp]
\begin{lstlisting}[style=launch]
local_costmap:
  global_frame: /odom
  robot_base_frame: /base_link
  update_frequency: 10.0
  publish_frequency: 10.0
  static_map: false
  rolling_window: true
  width: 6.0
  height: 6.0
  resolution: 0.05
  max_obstacle_height: 0.5
  plugins:
      - {name: obstacle_layer_laser, type: "costmap_2d::VoxelLayer"}
      - {name: obstacle_layer_kinect, type: "costmap_2d::VoxelLayer"}
      - {name: inflation_layer, type: "costmap_2d::InflationLayer"}

  obstacle_layer_kinect:
    observation_sources: kinect_laser kinect_laser_low kinect_laser_long
    kinect_laser: {sensor_frame: camera_link, data_type: LaserScan, topic: camera/scan_depth, marking: true, clearing: true, obstacle_range: 9.0, raytrace_range: 9.5, inf_is_valid: false}
    kinect_laser_low: {sensor_frame: camera_link, data_type: LaserScan, topic: camera/scan_depth_low, marking: true, clearing: true, obstacle_range: 9.0, raytrace_range: 9.5, inf_is_valid: false}
    kinect_laser_long: {sensor_frame: camera_link, data_type: LaserScan, topic: camera/scan_depth_long, marking: true, clearing: true, obstacle_range: 9.0, raytrace_range: 9.5, inf_is_valid: false}
    kinect: {sensor_frame: camera_link, data_type: PointCloud2, topic: camera/depth/points, marking: true, clearing: true, inf_is_valid: true}


  obstacle_layer_laser:
    observation_sources: sick_lms1xx
    sick_lms1xx: {sensor_frame: laser, data_type: LaserScan, topic: scan, marking: true, clearing: true, obstacle_range: 10.0, raytrace_range: 12.0, inf_is_valid: true}

  inflation_layer:
    inflation_radius: 0.55
    cost_scaling_factor: 4.0
\end{lstlisting}
\caption{Configuración del \textit{local\_costmap}.}\footnotemark
\end{code}
\footnotetext{\hypersetup{urlcolor=black}
Fuente: \href{https://github.com/danimtb/pioneer3at_ETSIDI/blob/master/pioneer_utils/navigation/common/local_costmap_params.yaml}{\textit{pioneer\_utils/navigation/common/local\_costmap\_params.yaml}}
\hypersetup{urlcolor=blue}}

\subsection{Configuración de los planificadores de trayectoria}
Los planificadores de trayectoria por defecto en ROS utilizan algoritmos como Dijsktra o A* además de algunos ajustes para el cálculo y parámetros especiales en el caso del planificador de trayectoria local.

\subsubsection{Parámetros del global\_planner}
En ROS, el nodo encargado de realizar el cálculo de la trayectoria global es el denominado global\_planner. Este nodo dispone de los algoritmos de planificación ya implementados que realizan los cálculos de trayectoria.

La configuración del planificador global se realiza a través de parámetros que podemos configurar, distinguiendo entre el uso del algoritmo de Dijsktra o el de A*, utilizar un camino definido por rejilla, etc.

A continuación podemos ver el comportamiento del planificador con diferente configuración en sus parámetros.

\begin{itemize}
\item \textbf{Algoritmo A*}:
\\El planificador de trayectoria A* ofrece los siguientes resultados.

\begin{figure}[htp]
\centering
\includegraphics[width=0.7\textwidth]{figuras/rviz-astar.png}
\caption{Planificador de trayectoria A* visualizado en RViz.}
\label{fig:rviz-astar}
\end{figure}

El espacio analizado es menor que en el caso de Dijkstra, sin embargo el camino final no es el adecuado, ya que es sinuoso y se acerca demasiado a obstáculos y paredes. Eso a simple vista parece un mal funcionamiento del algoritmo, ya que el comportamiento es exagerado.


\item \textbf{Algoritmo de Dijkstra}:
\\La configuración por defecto del planeador realiza el cálculo de trayectoria mediante el algoritmo de Dijsktra.

\begin{figure}[htp]
\centering
\includegraphics[width=0.7\textwidth]{figuras/rviz-dijkstra.png}
\caption{Planificador de trayectoria Dijkstra visualizado en RViz.}
\label{fig:rviz-dijkstra}
\end{figure}

Como se ve en la imagen, el comportamiento de este planificador es más adecuado, trazando una trayectoria que se mantiene equidistante a los obstáculos y mucho más suavizada respecto a A* a pesar de su exploración mucho más masiva del espacio.

\end{itemize}

El algoritmo que se ha utilizado finalmente es el de Dijkstra debido a las pruebas anteriores y a su buen comportamiento en pruebas con el robot.

\begin{code}[!htp]
\begin{lstlisting}[style=launch]
GlobalPlanner:
  old_navfn_behavior: false
  use_quadratic: true
  use_dijkstra: true
  use_grid_path: false
  
  allow_unknown: true 
                      
  planner_window_x: 0.0 
  planner_window_y: 0.0
  default_tolerance: 0.1
  
  publish_scale: 100
  planner_costmap_publish_frequency: 0.0
\end{lstlisting}
\caption{Configuración del \textit{global\_planner}.}\footnotemark
\end{code}
\footnotetext{\hypersetup{urlcolor=black}
Fuente: \href{https://github.com/danimtb/pioneer3at_ETSIDI/blob/master/pioneer_utils/navigation/common/global_planner_params.yaml}{\textit{pioneer\_utils/navigation/common/global\_planner\_params.yaml}}
\hypersetup{urlcolor=blue}}

\subsubsection{Parámetros del local\_planner}
EL planificador de trayectoria local que se ha utilizado es \textit{Trajectory Rollout} por sus buenos resultados en robot con bajas capacidades de aceleración.

Las pruebas realizadas tanto en el simulador como en el propio robot han servido para ajustar los parámetros de velocidad y aceleración así como para mantener un compromiso entre la correcta reacción ante obstáculos locales y el ajuste a la trayectoria global.

\begin{figure}[htp]
\centering
\includegraphics[width=0.5\textwidth]{figuras/rviz-local.png}
\caption{Planificador de trayectoria local \textit{Trajectory Rollout} en RViz.}
\label{fig:rviz-local}
\end{figure}

Los parámetros utilizados son los que se presentan a continuación.

\begin{code}[!htp]
\begin{lstlisting}[style=launch]
TrajectoryPlannerROS:
  max_vel_x: 0.6
  min_vel_x: 0.1
  max_vel_theta: 0.8
  min_in_place_vel_theta: 0.4

  acc_lim_theta: 3.2
  acc_lim_x: 2.5
  acc_lim_y: 2.5

  holonomic_robot: false

  yaw_goal_tolerance: 3.1415

  sim_granularity: 0.025
  sim_time: 2.0
  meter_scoring: true
  pdist_scale: 0.9
  gdist_scale: 0.6
\end{lstlisting}
\caption{Configuración de \textit{base\_local\_planner}.}\footnotemark
\end{code}
\footnotetext{\hypersetup{urlcolor=black}
Fuente: \href{https://github.com/danimtb/pioneer3at_ETSIDI/blob/master/pioneer_utils/navigation/common/base_local_planner_params.yaml}{\textit{pioneer\_utils/navigation/common/base\_local\_planner\_params.yaml}}
\hypersetup{urlcolor=blue}}

\subsection{Navegación con mapa}
Usualmente cuando hablamos de navegación nos referimos a una navegación basada en un mapa previo que se carga en la memoria del robot.

Los mapas utilizados para la navegación han sido todos creados utilizando el paquete \textit{gmapping} de ROS, utilizando el sensor láser del robot para obtener un rango y precisión mayor (Figura \ref{fig:creacion_mapa}).

\begin{figure}[htp]
\centering
\includegraphics[width=0.5\textwidth]{figuras/creacion_mapa.png}
\caption{Creación de un mapa mediante SLAM.}
\label{fig:creacion_mapa}
\end{figure}

Para realizar una navegación con mapa se utiliza un mapa del tipo anterior cargado en memoria acompañado del ya mencionado \textit{global\_costmap} de manera estática.

Esta es la configuración que se ha venido utilizando de manera general y el uso de todos sus elementos queda reflejado en el archivo launch de navegación global

\begin{code}[!htp]
\begin{lstlisting}[style=launch]
<launch>
    <!-- Run the map server -->
    <node name="map_server" pkg="map_server" type="map_server" args="$(find pioneer_utils)/maps/floor_zero-map.yaml"/>

    <!--- Run AMCL -->
    <include file="$(find pioneer_utils)/navigation/common/amcl.launch"/>
	
	<node pkg="move_base" type="move_base" respawn="false" name="move_base" output="screen">
        <rosparam file="$(find pioneer_utils)/navigation/common/costmap_common_params_p3at.yaml" command="load" ns="global_costmap" />
        <rosparam file="$(find pioneer_utils)/navigation/common/costmap_common_params_p3at.yaml" command="load" ns="local_costmap" />
        <rosparam file="$(find pioneer_utils)/navigation/common/local_costmap_params.yaml" command="load" />
        <rosparam file="$(find pioneer_utils)/navigation/global_navigation/global_costmap_params.yaml" command="load" />
        <rosparam file="$(find pioneer_utils)/navigation/common/base_local_planner_params.yaml" command="load"/>
        <rosparam file="$(find pioneer_utils)/navigation/common/global_planner_params.yaml" command="load" />
        <rosparam file="$(find pioneer_utils)/navigation/common/recovery_behaviors.yaml" command="load" />
        <rosparam>
            planner_frequency: 1.0 
        </rosparam>
        <param name="base_global_planner" value="global_planner/GlobalPlanner"/>
    </node>
</launch>
\end{lstlisting}
\caption{Configuración de navegación global.}\footnotemark
\end{code}
\footnotetext{\hypersetup{urlcolor=black}
Fuente: \href{https://github.com/danimtb/pioneer3at_ETSIDI/blob/master/pioneer_utils/navigation/global_navigation/global_navigation_p3at.launch}{\textit{pioneer\_utils/navigation/global\_navigation/global\_navigation\_p3at.launch}}
\hypersetup{urlcolor=blue}}

\subsection{Navegación reactiva}
La navegación reactiva es la que se conoce por carecer de un mapa previo cargado en la memoria del robot. En su caso el robot percibe el entorno a medida que navega construyendo un mapa global de manera dinámica al igual que sucede con la configuración del mapa local.

En este caso la configuración del mapa global carece de capa estática, por lo que el mapa se desplaza junto con el robot.

\begin{code}[!htp]
\begin{lstlisting}[style=launch]
global_costmap:
  global_frame: /odom
  robot_base_frame: /base_link
  update_frequency: 2.0
  publish_frequency: 2.0
  static_map: false
  rolling_window: true
  track_unknown_space: true
  width: 15.0
  height: 15.0
  origin_x: 0.0
  origin_y: 0.0

  plugins:
      - {name: obstacle_layer_kinect,        type: "costmap_2d::VoxelLayer"}
      - {name: obstacle_layer_laser,       type: "costmap_2d::VoxelLayer"}
      - {name: inflation_layer,       type: "costmap_2d::InflationLayer"}

  obstacle_layer_kinect:
    observation_sources: kinect_laser kinect_laser_low kinect_laser_long
    kinect_laser: {sensor_frame: camera_link, data_type: LaserScan, topic: camera/scan_depth, marking: true, clearing: true, obstacle_range: 3.0, raytrace_range: 6.5, inf_is_valid: true}
    kinect_laser_low: {sensor_frame: camera_link, data_type: LaserScan, topic: camera/scan_depth_low, marking: true, clearing: true, obstacle_range: 3.0, raytrace_range: 6.5, inf_is_valid: true}
    kinect_laser_long: {sensor_frame: camera_link, data_type: LaserScan, topic: camera/scan_depth_long, marking: true, clearing: true, obstacle_range: 3.0, raytrace_range: 6.5, inf_is_valid: true}

  obstacle_layer_laser:
    observation_sources: sick_lms1xx
    sick_lms1xx: {sensor_frame: laser, data_type: LaserScan, topic: scan, marking: true, clearing: true, obstacle_range: 5.0, raytrace_range: 12.5, inf_is_valid: true}

  inflation_layer:
    inflation_radius: 0.55
    cost_scaling_factor: 4.0
\end{lstlisting}
\caption{Configuración de \textit{global\_costmap} para navegación reactiva.}\footnotemark
\end{code}
\footnotetext{\hypersetup{urlcolor=black}
Fuente: \href{https://github.com/danimtb/pioneer3at_ETSIDI/blob/master/pioneer_utils/navigation/local_navigation/global_costmap_params.yaml}{\textit{pioneer\_utils/navigation/local\_navigation/global\_costmap\_params.yaml}}
\hypersetup{urlcolor=blue}}

La configuración de la navegación carece mapa y por tanto tampoco es preciso el nodo AMCL para situar al robot en el mismo. La orientación y posición del robot queda determinada por su odometría.

\begin{code}[!htp]
\begin{lstlisting}[style=launch]
<launch>
	<node pkg="move_base" type="move_base" respawn="false" name="move_base" output="screen">
        <rosparam file="$(find pioneer_utils)/navigation/common/costmap_common_params_p3at.yaml" command="load" ns="global_costmap" />
        <rosparam file="$(find pioneer_utils)/navigation/common/costmap_common_params_p3at.yaml" command="load" ns="local_costmap" />
        <rosparam file="$(find pioneer_utils)/navigation/common/local_costmap_params.yaml" command="load" />
        <rosparam file="$(find pioneer_utils)/navigation/local_navigation/global_costmap_params.yaml" command="load" />
        <rosparam file="$(find pioneer_utils)/navigation/common/base_local_planner_params.yaml" command="load"/>
        <rosparam file="$(find pioneer_utils)/navigation/common/global_planner_params.yaml" command="load" />
        <rosparam file="$(find pioneer_utils)/navigation/common/recovery_behaviors.yaml" command="load" />
        <rosparam>
            planner_frequency: 1.0 
        </rosparam>
        <param name="base_global_planner" value="global_planner/GlobalPlanner"/>
    </node>
</launch>
\end{lstlisting}
\caption{Configuración del launchfile para navegación reactiva.}\footnotemark
\end{code}
\footnotetext{\hypersetup{urlcolor=black}
Fuente: \href{https://github.com/danimtb/pioneer3at_ETSIDI/blob/master/pioneer_utils/navigation/local_navigation/local_navigation_p3at.launch}{\textit{pioneer\_utils/navigation/local\_navigation/local\_navigation\_p3at.launch}}
\hypersetup{urlcolor=blue}}

\section{Nodo de navegación por puntos}
El nodo de navegación por puntos ofrece a posibilidad de enviar diferentes puntos de meta al robot tanto en el modo global como en el modo local. esto quiere decir, que ofrece unas funciones capaces de manda al robot a un punto deseado del mapa o hacer que este avance cierta distancia determinada.

El nodo de navegación por puntos \textit{nav-waypoints} está desarrollado en C++ y ofrece un ejemplo de las posibilidades de uso de la navegación del robot. Este nodo se subscribe al estado del nodo de navegación del robot y publica puntos de meta en el \textit{frame} apropiado dependiendo de si es una meta global o local.

\begin{code}[!htp]
\begin{lstlisting}[style=C++]
...
bool setGlobalGoal(const float &x, const float &y, const float &angle)
{
	//tell the action client that we want to spin a thread by default
	MoveBaseClient ac("move_base", true);
	
	//wait for the action server to come up
	while(!ac.waitForServer(ros::Duration(5.0)))
	{
		ROS_INFO("Waiting for the move_base action server to come up");
	}
	
	move_base_msgs::MoveBaseGoal goal;
	goal.target_pose.header.frame_id = "/map";
	goal.target_pose.header.stamp = ros::Time::now();
	
	goal.target_pose.pose.position.x = x;
	goal.target_pose.pose.position.y = y;
	goal.target_pose.pose.orientation = tf::createQuaternionMsgFromYaw(angle);
	ROS_INFO("Sending GLOBAL goal");
	ac.sendGoal(goal);
	
	ac.waitForResult();
	
	if(ac.getState() == actionlib::SimpleClientGoalState::SUCCEEDED)
	{
		ROS_INFO("Hooray, the base moved 1 meter forward");
		return true;
	}
    else
    {
		ROS_INFO("The base failed to move forward 1 meter for some reason");
		return false;
	}
}

int main(int argc, char** argv){
  ros::init(argc, argv, "simple_navigation_goals");
  setGlobalGoal(-0.671, 1.938, 1.0); //rosa
  setGlobalGoal(0.193, -1.520, 1.0); //invernadero
  setGlobalGoal(2.118, -8.223, 1.0); //comau
  setGlobalGoal(-1.073, -9.271, 1.0); //puerta principal
  
  return 0;
}
\end{lstlisting}
\caption{Fragmento de código del nodo \textit{nav-waypoints}.}
\end{code}

Este nodo se encuentra separado del directorio habitual \textit{pioneer\_utils} debido a dependencias adicionales que pueden causar conflicto. Por ello, se aloja en el paquete \textit{navigation\_goals}.

\subsection{Endurance test}
El nodo \textit{endurance\_test} es un nodo de navegación similar al descrito en el apartado anterior pero desarrollado con la API de Python.

Este nodo sirve para realizar un test de resistencia en la navegación del robot con un mapa, mandando al robot diferentes puntos de meta y llevando un registro de las metas alcanzadas, el tiempo transcurrido y los metros recorridos.

Los puntos de meta son leídos desde un archivo de texto plano donde se indica el nombre del punto de meta y las coordenadas X, Y del mismo en el mapa utilizado. Adicionalmente y para dotarlo de utilidad a otros robots es posible configurar el Topic de la odometría, el Topic de velocidad y el tiempo de esperas entre metas.

\begin{table}[!h]
\centering
\resizebox{\textwidth}{!}{
\begin{tabular}{c c c}
& {\bf endurance\_test API} & \\
{\bf Topics suscritos} & {\bf Mensaje} & {\bf Descripción}\\ \hline
odom & nav\_msgs/Odometry & Odometría del robot\\
{\bf Topics publicados} & {\bf Mensaje} & {\bf Descripción}\\ \hline
cmd\_vel & geometry\_msgs/Twist & Publica comandos de velocidad \\
{\bf Parámetros} & {\bf Tipo} & {\bf Descripción}\\
\hline
rest\_time & int & Tiempo de espera (segundos)\\
map\_locations & file & Archivo con los puntos de meta\\
odometry\_topic & string & Nombre del Topic de odometría\\
cmd\_vel\_topic & string & Nombre del Topic de velocidad\\
\end{tabular}
}
\caption{API de endurance\_test}
\label{tabla:endurance_test}
\end{table}

Para lanzar este nodo se utiliza un archivo \textit{launchfile} en el que se indican los parámetros.

\begin{code}[!htp]
\begin{lstlisting}[style=C++]
<launch>
  <node name="endurance_test" pkg="pioneer_utils" type="endurance_test.py" output="screen">
    <param name="map_locations" value="$(find pioneer_utils)/main/locations_lab.txt"/>
    <rosparam>
       odometry_topic: rosaria/pose
       cmd_vel_topic: cmd_vel
       rest_time: 1
     </rosparam>
  </node>
</launch>
\end{lstlisting}
\caption{Archivo \textit{launchfile} para el nodo \textit{endurance\_test}.}
\end{code}



\section{Nodo de guiado (follower)}

El nodo de guiado consiste en el análisis de la nube de puntos que capta el sensor Kinect para detectar un objeto delante y dirigir al robot ajustando sus velocidades para que mantenga la posición hacia ese objeto. Su funcionamiento se basa en el procesamiento de la nube de puntos obtenida a través de los nodos del paquete \textit{freenect\_stack}.

El nodo está originalmente desarrollado para el robot Turtlebot pero es fácil adaptable a otros robots con el propósito de que el robot siga a personas, a otros robot o a objetos en movimiento.

Requiere un Topic de tipo \textit{sensor\_msgs/PointCloud2} al que suscribirse para leer la nube de puntos y un Topic de tipo \textit{geometry\_msgs/Twist} al que publicar los movimientos de giro, avance y retroceso.

\begin{table}[!h]
\centering
\resizebox{\textwidth}{!}{
\begin{tabular}{c c c}
& {\bf turtlebot\_follower API} & \\
{\bf Topics suscritos} & {\bf Mensaje} & {\bf Descripción}\\ \hline
camera/depth/points & sensor\_msgs/PointCloud2 & Recibe la nube de puntos\\
{\bf Topics publicados} & {\bf Mensaje} & {\bf Descripción}\\ \hline
cmd\_vel & geometry\_msgs/Twist & Publica comandos de velocidad\\
{\bf Parámetros} & {\bf Tipo} & {\bf Descripción}\\
\hline
min\_y & double & Posición mínima de puntos en Y\\
max\_y & double & Posición máxima de puntos en Y\\
min\_x & double & Posición mínima de puntos en X\\
max\_x & double & Posición máxima de puntos en Y\\
max\_z & double & Posición máxima de puntos en Y\\
goal\_z & double & Distancia mantenida en el seguimiento\\
z\_scale & double & Factor de escala velocidad trans.\\
x\_scale & double & Factor de escala en velocidad de rot.\\
enabled & bool & Hanilita los movimientos\\
\end{tabular}
}
\caption{API de \textit{turtlebot\_follower}}
\label{tabla:tabla_follower}
\end{table}

El tratamiento de la nube de puntos se realiza con la librería PCL (PointCloud Library \cite{PCL}) y su funcionamiento es el siguiente:

\begin{enumerate}[1.-]
\item Busca puntos dentro de los límites establecidos.
\item Calcula las dimensiones de los puntos encontrados.
\item Calcula el centroide del la zona destacada.
\item Mueve el robot de manera acorde hasta que alcanza la distancia establecida.
\end{enumerate}

\begin{code}[!htp]
\begin{lstlisting}[style=launch]
<launch>
    
<!--  Load turtlebot follower into the 3d sensors nodelet manager to avoid pointcloud serializing -->
    <node pkg="nodelet" type="nodelet" name="turtlebot_follower" args="load turtlebot_follower/TurtlebotFollower camera/camera_nodelet_manager">
      <remap from="turtlebot_follower/cmd_vel" to="/cmd_vel"/>
      <remap from="depth/points" to="camera/depth/points"/>
      <param name="enabled" value="true" />
      <param name="x_scale" value="10.0" />
      <param name="z_scale" value="10.0" />
      <param name="min_x" value="-0.35" />
      <param name="max_x" value="0.35" />
      <param name="min_y" value="0.1" />
      <param name="max_y" value="0.5" />
      <param name="max_z" value="1.2" />
      <param name="goal_z" value="0.6" />
    </node>
</launch>
\end{lstlisting}
\caption{Launchfile para \textit{turtlebot\_follower} en el robot Pioneer 3 AT.}
\end{code}

\footnotemark
\footnotetext{\hypersetup{urlcolor=black}
Fuente: \href{https://github.com/danimtb/pioneer3at_ETSIDI/blob/master/pioneer_utils/follower/simple-follower.launch}{\textit{pioneer\_utils/follower/simple-follower.launch}}
\hypersetup{urlcolor=blue}}

\section{Feedback mediante text-to-speech}
Como ya se ha visto, la variedad de paquetes de software robótico en ROS es notable y ofrece una amplia variedad de características gracias a los aportes de la comunidad. A medida que evolucionaba este proyecto y debido a que el robot lleva incorporado su propio altavoz, apareció la idea de dotar de sonidos al robot de tal manera que existiera un feedback hacia las personas que se encuentren en su entorno.

Existe un nodo en ROS llamado \textit{sound\_play} \cite{soundplay} que permite, mediante la publicación de mensajes, reproducir sonidos preincorporados, archivos de sonido OGG/WAV o incluso realizar síntesis de voz a partir de un texto, conocido como \textit{text-to-speech} (TTS), utilizando voces del \textit{Festival Speech Synthesis System} desarrollado por \textit{The Centre for Speech Technology Research} de la Universidad de Edinburgo \cite{festival2014}.

En este caso se ha utilizado la API de Python para interactuar con el nodo de tal modo que tan solo es necesario intercambiar mensajes con el nodo \textit{soundplay\_node}.

\begin{table}[!h]
\centering
\resizebox{\textwidth}{!}{
\begin{tabular}{c c c}
& {\bf \textit{soundplay\_node} API} & \\
{\bf Topics suscritos} & {\bf Mensaje} & {\bf Descripción}\\ \hline
robotsound & sound\_play/SoundRequest & Sonido a reproducir\\
\end{tabular}
}
\caption{API del nodo \textit{soundplay\_node}.}
\label{tabla:soundplay_node}
\end{table}

Para ponerlo en marcha se incia el nodo desde el archivo de \textit{launch}.

\begin{code}[!htp]
\begin{lstlisting}[style=launch]
  <node name="sound_play" pkg="sound_play" type="soundplay_node.py"/>
\end{lstlisting}
\caption{Nodo soundplay\_node para reproducir sonidos.}
\end{code}

Para reproducir sonidos se utiliza un ''handle'' de sonido que facilita e paso de mensajes en caso de reproducir sonidos o voz con las funciones \textit{.play()} y \textit{.say()} respectivamente.
A continuación se presenta un fragmento del nodo \textit{voice\_cmd\_vel} de desarrollo propio y que se describirá más adelante

\begin{code}[!htp]
\begin{lstlisting}[style=launch]
  from sound_play.libsoundplay import SoundClient
  
   # Create the sound client object
          self.soundhandle = SoundClient()
         
          rospy.sleep(1)
          self.soundhandle.stopAll()
          
           # Subscribe to the move_base action server
          self.move_base = actionlib.SimpleActionClient("move_base", MoveBaseAction)
          
          rospy.loginfo("Waiting for move_base action server...")
          self.soundhandle.play(1)
          
          # Wait 60 seconds for the action server to become available
          self.move_base.wait_for_server(rospy.Duration(60))
          
          rospy.loginfo("Connected to move base server")
         
          # Announce that we are ready for input
          rospy.sleep(1)
          self.soundhandle.say('Hi, my name is Petrois')
          rospy.sleep(2)
          self.soundhandle.say("Say one of the navigation commands")
\end{lstlisting}
\caption{Fragmento del nodo \textit{voice\_cmd} utilizando el cliente de sonido de \textit{soundplay\_node}.}
\end{code}

El uso y aplicación del nodo de sonido se expone con más detalle en la sección \ref{section:ejecucion_automatica}.

\section{Reconocimiento de comandos de voz}
Siguiendo con la idea de utilizar todos los elementos que incorpora el robot y de manera adicional a los objetivos del proyecto, se exploró la idea de la interacción con el robot mediante comandos de voz, gracias una vez más a los paquetes de ROS.

\textit{pocketsphinx} \cite{pocketsphinx2012} es un paquete que actúa como \textit{wrapper} del motor de reconocimiento de voz del mismo nombre, que utiliza el framework multimedia \textit{GStreamer} \cite{gstreamer2001}. La implementación de este nodo está basada en los estudios e investigaciones sobre reconocimiento de voz y patrones en el habla de la Universidad de Carnegie Mellon dentro del proyecto CMU Sphinx \cite{sphinx}.

EL objetivo del paquete \textit{pocketsphinx}, mediante el nodo \textit{recognizer}, es realizar el reconocimiento y procesado de voz, comparar la voz con un diccionario de palabras y pronunciación y devolver una cadena de texto con las palabras que han sido detectadas \cite{rosbyexample}.

\begin{table}[!h]
\centering
\resizebox{\textwidth}{!}{
\begin{tabular}{c c c}
& {\bf recognizer API} & \\
{\bf Topics publicados} & {\bf Mensaje} & {\bf Descripción}\\ \hline
output & std\_msgs/String & Palabras detectadas\\
{\bf Parámetros} & {\bf Tipo} & {\bf Descripción}\\
\hline
lm & string & Archivo de pronunciación\\
dict & string & Diccionario de palabras\\
\end{tabular}
}
\caption{API del nodo \textit{recognizer}.}
\label{tabla:tabla_pocketsphinx}
\end{table}

La entrada de audio se realiza a través de las entradas de micrófono configuradas en Ubuntu. En este caso, para sacar mayor partido al sensor Kinect, se pretende utilizar el array de micrófonos que incorpora. La entrada de audio del sensor Kinect no se reconoce por defecto en Ubuntu, ni siquiera a través de los drivers \textit{libfreenect} por lo que es necesario instalar el paquete \textit{kinect-audio-setup}.

\begin{code}[!htp]
\begin{lstlisting}[style=consola]
sudo apt-get install kinect-audio-setup
\end{lstlisting}
\caption{Paquete necesario para reconocer el micrófono del sensor Kinect.}
\end{code}

Una vez instalado se selecciona el micrófono del sensor en el panel de ajustes de audio de Ubuntu. El resto de instalaciones necesarias para el paquete \textit{pocketsphinx} se detallan en el apéndice \ref{subsection:dependencias}.

Para utilizar el nodo \textit{recognizer} deben indicarse los parámetros del siguiente modo:

\begin{code}[!htp]
\begin{lstlisting}[style=launch]
<launch>
  <node name="recognizer" pkg="pocketsphinx" type="recognizer.py">
    <param name="lm" value="$(find pioneer_utils)/voice_audio/dic/commands.lm"/>
    <param name="dict" value="$(find pioneer_utils)/voice_audio/dic/commands.dic"/>
  </node>
<\launch>
\end{lstlisting}
\caption{Archivo \textit{launchfile} para el nodo \textit{recognizer}.}
\end{code}

Para realizar acciones con los comandos de voz se utiliza un nodo alternativo que se suscribe al Topic \textit{recognizer/output}, el cual devuelve las palabras reconocidas. A continuación se expone un fragmento de su uso en Python:

\begin{code}[!htp]
\begin{lstlisting}[style=C++]
...
rospy.Subscriber('recognizer/output', String, self.speechCb)
        
    def speechCb(self, msg):
        rospy.loginfo(msg.data)

        if msg.data.find("fast") > -1:
            if self.speed != 0.3:
                self.soundhandle.say('Speeding up')
                self.set_speed(0.3)
...
\end{lstlisting}
\caption{Fragmento del nodo \textit{voide\_cmd} utilizando las palabras reconocidas por el nodo \textit{recognizer}.}
\end{code}

El uso y aplicación del nodo de reconocimiento de voz se expone con más detalle en la sección \ref{section:ejecucion_automatica}.

\subsection{Crear una lista de vocabulario}
Como ya se ha mostrado, este nodo precisa de un archivo de pronunciación y un archivo de dicionario. Estos archivos actúan como base de datos de vocabulario del motor de reconocimiento de voz.

Para crear estos archivos de vocabulario y pronunciación se utiliza la herramienta \textit{Sphinx Knowledge Base Tool}\footnote{http://www.speech.cs.cmu.edu/tools/lmtool-new.html} que genera archivos de pronunciación gracias a la herramienta \textit{lmtool}\footnote{http://www.speech.cs.cmu.edu/tools/lmtool.html}. Una de las limitaciones de la herramienta es que su desarrollo está basado principalmente en el reconocimiento de palabras en Inglés, y aunque se pueden incluir palabra de otros lenguajes su pronunciación es probable que no sea la correcta.

Accediendo a la herramienta en web (Figura \ref{fig:lmtool}), tenemos la posibilidad de subir un archivo con las palabras que queramos que nuestro sistema reconozca. En este caso hemos utilizado palabras en Inglés. La herramienta nos genera los archivos necesarios \textit{.lm} y \textit{.dic}.

\begin{figure}[htp]
\centering
\includegraphics[width=0.8\textwidth]{figuras/lmtool.png}
\caption{Herramienta web Sphinx Knowledge Base Tool.}
\label{fig:lmtool}
\end{figure}

\section{Nodo de ejecución automática de nodos}\label{section:ejecucion_automatica}
Para dar integración a todas las funcionalidades de ROS incorporadas en el robot de este proyecto se ha implementado un nodo que ejecuta o detiene cada uno de los nodos dependiendo de las tareas que se envíen al robot mediante comandos de voz.

El nombre de este nodo es \textit{voice\_cmd\_vel}. Es un nodo desarrollado en Python debido a su fácil utilización de los nodos \textit{regonizer} (para reconocer comandos de voz) y del nodo \textit{soundplay\_node} (para reproducir sonidos).

Las fucionalidades que integra son las siguientes:
\begin{itemize}
\item Reconocimiento de comandos de voz con el micrófono del sensor Kinect.
\item Respuesta con voz sintetizada a través del altavoz.
\item Capacidad para realizar movimientos de desplazamiento básico: forward, backward, right, left, stop.
\item Indicar al robot que active el nodo de guiado (follower).
\item Generar mapas mediante SLAM y guardarlos en memoria.
\item Navegar hacia puntos del mapa preestablecidos (navegación con mapa).
\end{itemize}

Este nodo es un desarrollo específico y por tanto se encuentra muy acoplado a las configuraciones que se han realizado durante este trabajo.

La API de \textit{voice\_cmd\_vel} (Tabla \ref{tabla:voice_cmd_vel}) sirve para conectarse a los nodos de reconocimiento de voz, al nodo de navegación y al nodo de sonido. Además recibe los mismos parámetros que el nodo \textit{endurance\_test} ya que incorpora su funcionalidad para leer puntos guardados del el mapa para después utilizarlos como objetivos de meta.

\begin{table}[!h]
\centering
\resizebox{\textwidth}{!}{
\begin{tabular}{c c c}
& {\bf \textit{voice\_cmd\_vel} API} & \\
{\bf Topics suscritos} & {\bf Mensaje} & {\bf Descripción}\\ \hline
recognizer/output & std\_msgs/String & Comandos de voz detectados\\
{\bf Topics publicados} & {\bf Mensaje} & {\bf Descripción}\\ \hline
cmd\_vel & geometry\_msgs/Twist & Publica comandos de velocidad \\
move\_base/goal & move\_base\_msgs/MoveBaseActionGoal & Punto de meta \\
robotsound & sound\_play/SoundRequest & Sonido a reproducir\\
{\bf Parámetros} & {\bf Tipo} & {\bf Descripción}\\
\hline
map\_locations & file & Archivo con los puntos de meta\\
cmd\_vel\_topic & string & Nombre del Topic de velocidad\\
\end{tabular}
}
\caption{API del nodo \textit{voice\_cmd\_vel}}
\label{tabla:voice_cmd_vel}
\end{table}

\subsection{Ejecución de nodos}
Para la ejecución y para automática de los nodos existe el paquete \textit{roslaunch}\footnote{\url{http://wiki.ros.org/roslaunch/API\%20Usage}}, sin embargo la API del mismo ofrece una funcionalidad limitada ya que no se pueden ejecutar archivos \textit{launchfile} completos con todos los nodos y parámetros necesarios.

Por tanto, se ha recurrido a los la funcionalidad \textit{Subprocess} de Python, que permite ejecutar subprocesos con determinados comandos de consolas. Con esta forma de proceder podemos ejecutar archivos \textit{launch} completos y después para los utilizando directamente los comandos de ROS "rosnode kill".

\begin{code}[!htp]
\begin{lstlisting}[style=C++]
subprocess.Popen(['roslaunch', 'pioneer_utils', 'simple-follower.launch'])
...
subprocess.Popen(['rosnode', 'kill', 'turtlebot_follower'])
\end{lstlisting}
\caption{Ejemplo de uso de \textit{subprocess} en Python lanzando y parando el nodo \textit{turtlebot\_follower}.}
\end{code}

La funcinalidad subprocess genera procesos por detrás, que sería el equivalente a abrir manualmente una terminal y lanzar el \textit{launchfile}. Este proceso se mantiene abierto hasta que se ejecuta "rosnode kill", que abre un proceso que se encarga de parar el proceso anterior y después de hacerlo muere automáticamente.

Para que este nodo funcione correctamente requiere que los siguientes nodos se ejecuten al inicio del mismo:
\begin{itemize}
\item Nodo de reconocimiento de voz: \textit{recognizer}.
\item Nodo de reproducción de sonidos: \textit{soundplay\_node}.
\item Nodos de navegación: \textit{move\_base}, \textit{amcl} y \textit{map\_server}.
\item Nodos de bajo nivel: \textit{RosAria}, \textit{LMS1xx}, \textit{freenect\_launch} y \textit{depthimage\_to\_laserscan}.
\end{itemize}

Adicionalmente, controla algunos nodos que no se ejecutan a inicio:
\begin{itemize}
\item \textit{turtlebot\_follower}: Para seguir a personas.
\item \textit{slam\_gmapping}: Para capturar mapas mediante SLAM.
\item \textit{map\_saver}: Para guardar mapas en memoria.
\end{itemize}

\subsection{Funcionamiento}
El funcionamiento del nodo \textit{voice\_cmd\_vel} es el siguiente:
\begin{enumerate}
\item Se conecta al Topic de velocidad (\textit{cmd\_vel}) y lee los puntos de meta del archivo indicado en \textit{map\_locations}.
\item Se conecta al Topic \textit{move\_base/goal} para poder publicar puntos de meta.
\item Se conecta al Topic \textit{robotsound} para enviar comandos de voz y sonidos.
\item Se suscribe al Topic \textit{recognizer/output} para leer las palabras del nodo de reconocimiento de voz.
\item Entra en un bucle esperando algún comando de voz reconocido.
\item En caso de reconocer alguno de los comandos, ejecuta la acción y responde mediante voz o un sonido.
\end{enumerate}

Los comandos de voz que puede reconocer el robot son los siguientes:
\begin{itemize}
\item Comandos de velocidad "fast", "half", "slow": Cambia la velocidad de desplazamiento del robot.
\item Comandos de movimiento básico "forward", "back", "right", "left", "stop": El robot se mueve de la forma indicada.
\item Comandos de guiado "follow me", "stop follower": Inicia o detiene el nodo de guiado (follower).
\item Comandos de mapa "build map", "save map", "stop map": Construye un mapa mediante SLAM, guarda el mapa o detiene el SLAM.
\item Comandos de navegación: "Navigate to..." seguido del nombre de un punto de meta guardado en el archivo \textit{map\_locations}: Indica al robot que navegue hasta el punto deseado.
\end{itemize}

El archivo \textit{voice\_cmd.launch} ejecuta el inicio del nodo de reconocimiento de voz, el nodo de reproducción de sonido y el nodo \textit{voice\_cmd\_vel} con sus respectivos parámetros.

\begin{code}[!htp]
\begin{lstlisting}[style=launch]
<launch>
  <node name="recognizer" pkg="pocketsphinx" type="recognizer.py">
    <param name="lm" value="$(find pioneer_utils)/voice_audio/dic/commands.lm"/>
    <param name="dict" value="$(find pioneer_utils)/voice_audio/dic/commands.dic"/>
  </node>

  <node name="sound_play" pkg="sound_play" type="soundplay_node.py"/>

  <node name="voice_cmd_vel" pkg="pioneer_utils" type="voice_cmd_vel.py" output="screen">
    <param name="map_locations" value="$(find pioneer_utils)/main/locations.txt"/>
    <rosparam>
       cmd_vel_topic: cmd_vel
     </rosparam>
  </node>
</launch>
\end{lstlisting}
\caption{Archivo \textit{voice\_cmd.launch}.}
\end{code}
\footnotemark
\footnotetext{\hypersetup{urlcolor=black}
Fuente: \href{https://github.com/danimtb/pioneer3at_ETSIDI/blob/master/pioneer_utils/voice_audio/launch/voice_cmd.launch}{\textit{pioneer\_utils/voice\_audio/launch/voice\_cmd.launch}}
\hypersetup{urlcolor=blue}}

El código fuente de este nodo se ha omitido por no alargar la explicación, sin embargo puede consultarse de forma externa\footnote{\hypersetup{urlcolor=black}
Fuente: \href{https://github.com/danimtb/pioneer3at_ETSIDI/blob/master/pioneer_utils/voice_audio/node/voice_cmd_vel.py}{\textit{pioneer\_utils/voice\_audio/node/voice\_cmd\_vel.py}}
\hypersetup{urlcolor=blue}}.