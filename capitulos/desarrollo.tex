\chapter{Desarrollo del proyecto}

En esta capítulo se expone cuál ha sido el planteamiento del proyecto y los pasos que se han seguido para conseguir los objetivos y llegar a unos resultados óptimos.

\section{Planteamiento}
El robots sobre el que se pretende trabajar es el Pioneer 3 AT, de la empresa Adept Mobile Robots, cuyas características se detallarán más adelante. La configuración del sistema motriz es de tipo skid-steer y será determinante a la hora de realizar el control del desplazamiento.

Para realizar la teleoperación del robot, utilizaremos las herramientas de comunicación de ROS, que hacen que la ejecución de los diferentes nodos de forma distribuida entre equipos se realice de forma transparente para el usuario. Con esta caracterítica podremos desarrollar con facilidad un sistema de telecontrol sin preocuparnos en exceso por la implementación de la comunicación entre equipos.

Para la navegación se pretende que el robot base sus movimientos en un sistema reactivo, es decir, que el robot base su navegación principalmente en la información captada por sus sensores y no en un mapa preestablecido. El control en navegación del robot se basará en la funcionalidad "Navigation Stack" de ROS, que también será explicada en detalle más delante.

El desarrollo principal para la navegación se basa en la infomación aportada por el sensor Kinect, sin embargo, el sensor láser proporciona una información muy potente para robots y también ha sido incluido en el dasarrollo de este proyecto, utilizándolo en conjunto con el sensor Kinect.

Seguidamente, se han realizado los ajustes pertinentes en la navegación del robot, para la cual se ha seguido el concepto de mapas de coste y descomposición en celdas. También se ha valorado la disposición de ambos sensores para capturar el entorno, así como el tratamiento dispar de los datos capturados por cada uno de ellos. De esta forma logramos que no se produzcan detecciones de objetos de manera duplicada y que no haya discrepancias entre los obstáculos que detecta un sensor respecto al otro \footnote{De especial interés en la icorporación de obstáculos al mapa mediante el uso de Costmaps en el sistema de navegación de ROS}.

Finalmente, se han incorporado características adicionales que aportan valor al desarrollo del proyecto, como el uso del simulador Gazebo o la interacción con el robot mediante comandos de voz y sintetizado de voz.

\section{Planificación del proyecto}

En este apartado se desarrollan las fase por las que ha pasado ete proyecto y realizaremos un análisis de tiempos.

Fase inicial: Familiarización con el entrono ROS y elección de herramientas.

\begin{enumerate}[i.]
  \item Utilizaremos las herramientas proporcionadas por ROS para evaluar los datos que pueda manejar el robot: RViz, rqt\_grapth, map\_server, rostopics...
  
  \item Para el control del movimiento del robot utilizaremos el nodo RoSAria debido a sus amplias posibilidades.
  
  \item Para acceder a la información de la Kinect utilizaremos los driver libfreenect, por ser librerías de código libre y utilizadas ampliamente.
\end{enumerate}

Segunda fase: Realización del nodo de teleoperación y comunicación entre equipos conectados a la misma red.

\begin{enumerate}[i.]
  \item Utilizamos la configuración de equipos en red para acceder a la información publicada por nodos que se ejecuten en varias máquinas **referencia**
  
  \item Partindo del nodo de teleoperacion de "Turtlesim" *refrencia*, realizamos un nodo similar para nuestro robot.
\end{enumerate}

Tercera fase: Incorporación de los sensores al robot y acceso a los datos.

\begin{enumerate}[i.]
  \item Para el sensor Kinect, realizamos un adaptador para conectarlo a la alimetnación del robot. Utilizando el nodo "freenect\_stack" *REFERENCIA*, accedemos a la nube de puntos y la imagen.
  
  \item Utilizamos el nodo LMS1xx **Referencia** para la puesta en marcha del láser y el acceso a los datos.
  
  \item Incorporación de sistemas de referencia "base\_link", "laser", ''camera\_link'' y sus transformadas mediante el paquete "tf" **referencia**. 
  
  \item Visualización del conjunto de datos junto con los ejes de referencia en RViz.
\end{enumerate}

Cuarta fase: Incorporación del sistema de navegación y ajuste de los parámetros

\begin{enumerate}[i.]
  \item Calibrado de los encoders de las ruedas del robot y ajuste de la odometría mediante RosAria.
  
  \item Incorporación del sistema de navegación ROS de forma básica.
  
  \item Navegación utilizando el sensor Kinect y el sensor Sick y generado de mapas mediante "slam\_gmapping".
  
  \item Ajuste de los planeadores de taryectoria del robot y parámetros de giro y control.
\end{enumerate}

Quinta fase: Ajuste de la navegación y simulación mediante gazebo

\begin{enumerate}[i.]
  \item Navegación en modo global (utilizando un mapa guardado) y en modo local (completamente reactivo).
  
  \item Puesta en marcha del simulador Gazebo y configuración del robot en el entorno.
  
  \item Disposición de los sensores de manera óptima y remodelado de la estructura física del robot.
\end{enumerate}

Sexta fase: Nuevas funcionalidades y toma de datos.

\begin{enumerate}[i.]
  \item Incorporación de la funcionalidad "follower", adaptada a partir del robot Turtlebot **Refeencia**, para el guiado del robot.
  
  \item Interfaz de comandos por voz y sintetizador de texto a voz.
  
  \item Pruebas físicas, recogida y análisis de los datos.
\end{enumerate}


Análisis de tiempos:

Este proyecto fin de grado comenzó en Noviembre de 2014 y terminó en Febrero de 2015.

Durante el primer mes de Noviembre se estuvo recopilando información sobre ROS y su funcionamiento, los desarrollos existentes aplicados a robots reales y la filosofía del sistema.

En el mes de Diciembre se comenzó a trabajar con el robot, comprobando que todos los elementos se encontraban en correcto funcionamiento y se instaló el sistema operativo en su ordenador de abordo

Duarante el mes de Enero se pudo avanzar menos debido a los exámenes y trabajos de las últimas asignaturas.

En el mes de Febrero se retomó el trabajo, empezando por una primera toma de contacto con la librería Aria y la ejecución de movimientos desde un ordenador externo conectado vía puerto serie.

Durante los meses de Marzo y Abril, el robot comenzó a funcionar con ROS, realizando los primeros movimientos con control por teclado. Seguidamente se realizó el nodo de telecontrol y un nodo para realizar movimientos basados tan solo en la odometría.

En el mes de mayo, se comenzaron a probar la compatibilidad con ROS de la cámara Kinect y el sensor Láser. Acto seguido, comenzaron las primeras pruebas de navegación autónoma.

En los meses de Junio y Julio, siguieron los ajustes en la navegación, tanto en el planificador de trayectoria como en los mapas de coste, así como en el sensor Kinect para la detección de obstáculos a diferente altura. Además se incorporó la funcionalidad de seguimiento.

En Julio también comenzaron las primeras pruebas de comandos de voz y la sintetización de voz.

Durante ese mes y el mes de Agosto, se comenzó a redactar gran parte del trabajo en esta memoria, donde se organizó la estructura del proyecto y la información a incluir.

En el mes de Septiembre se decidió incorporar un ordenador más potente al robot y reestructurar su chasis para dejar el sistema desarrollado integrado de manera permanente. También se utilizó el array de micrófonos del sensor Kinect para los comandos de voz.

Duante el mes de Octubre se organizó la estructura del proyecto y se puso en marcha el simulador Gazebo. A continuación se realizó el ajuste de los sensores y la optimización del sistema de navegación. En paralelo se realizadon las modificaciones mecánicas y estructurales para la integración de los sensores y el ordenador en el robot.

Durante el mes de Novimebre se realizó un pequeño parón a nivel de software, se continuó con la parte mecánica y con la redacción de la memoria de este proyecto.

A continuación, comenzaron a realizarse las pruebas reales con la nueva configuración en el robot.

A continiación se muestra un diagrama de Gantt con el análisis de tiempos de las diferentes tareas.

\begin{landscape} 
\begin{ganttchart}{1}{48}
  \gantttitle{2014}{6}
  \gantttitle{2015}{36}
  \gantttitle{2016}{6} \\
  \gantttitlelist{11,...,12}{3}
  \gantttitlelist{1,...,12}{3}
  \gantttitlelist{1,2}{3} \\
  \ganttgroup{Entorno ROS}{1}{3} \\
  \ganttbar{Recabar informacion}{1}{2} \\
  \ganttlinkedbar{Realizar tutoriales}{2}{3} \\
  \ganttgroup{Pruebas robot}{4}{12} \ganttnewline
  \ganttbar{Sistema Operativo}{4}{4} \\
  \ganttlinkedbar{Librería Aria}{7}{8} \\
  \ganttlinkedbar{Primeros movimientos}{9}{12} \\
  \ganttgroup{Pruebas ROS}{13}{18} \ganttnewline
  \ganttbar{Instalacion ROS}{13}{14} \\
  \ganttlinkedbar{Telecontrol}{15}{16} \\
  \ganttlinkedbar{Movimientos odometría}{17}{19} \\
  \ganttgroup{Navegacion}{19}{40} \ganttnewline
  \ganttbar{Navegacion básica}{19}{20} \\
  \ganttlinkedbar{Navegacion}{21}{28}{29}{30} \\
  
  %\ganttlink{elem2}{elem3}
  %\ganttlink{elem3}{elem4}
\end{ganttchart}
\end{landscape}

\section{Tecnologías y herramientas empleadas en el proyecto}

En esta sección se describen las tanto las tecnologías como las herramientas utilizadas en el desarrollo del proyecto. 

\subsection{Robot Operating System}

El Sistema Operativo Robótico \cite{ROS} (conocido en inglés como Robot Operating System o ROS) es un framework para el desarrollo de software para robots que provee la funcionalidad de un sistema operativo \cite{quigley2009ros}. ROS fue desarrollado originalmente en 2007 por el Laboratorio de Inteligencia Artificial de Stanford para dar soporte al proyecto del Robot con Inteligencia Artificial de Stanford \cite{stair}. Desde 2008, el desarrollo continua primordialmente en Willow Garage, un instituto de investigación robótico con más de veinte instituciones que colaboran conjuntamente.

ROS provee los servicios estándar de un sistema operativo como abstracción del hardware, control de dispositivos de bajo nivel, implementación de funcionalidad de uso común, paso de mensajes entre procesos y mantenimiento de paquetes. Está basado en una arquitectura de nodos interconectados que que pueden mandar, recibir y multiplexar mensajes de sensores, control, estados, planificaciones y actuadores, entre otros. La librería está orientada para un sistema UNIX (Ubuntu (Linux)) aunque también se está adaptando a otros sistemas operativos como Fedora, Mac OS X, Arch, Gentoo, OpenSUSE, Slackware, Debian o Microsoft Windows, considerados como 'experimentales'.

ROS consta de dos partes básicas: la parte del sistema operativo, ros, como se ha descrito anteriormente y ros-pkg, una suite de paquetes aportados por la contribución de usuarios (organizados en conjuntos llamado en inglés "stacks") que implementan la funcionalidades tales como localización y mapeo simultáneo, planificación, percepción, simulación, etc.

ROS ofrece principlamente dos lenguajes de programación para acceder a su API (Application Programming Interface) completa. Esos lesnguajes son C++ y Python \cite{rosapi}. 

ROS es software libre bajo términos de licencia BSD. Esta licencia permite libertad para uso comercial e investigador. Las contribuciones de los paquetes en ros-pkg están bajo una gran variedad de licencias diferentes.

Actualemtente ROS es mantenido y desarrollado de manera Open Source por Open Source robotics Foundation \cite{osrf}, una organización independiente sin ánimo de lucro fundada por miembros de la comunidad robótica a nivel global.

\subsection{Lenguaje de programación C++}

El lenguaje C++ es un lenguaje orientado a objetos, y como tal, tiene como objetivo la reducción del tiempo de desarrollo aumentando la eficacia del proceso de generación de los programas.

Como consecuencia, los programas tienden a tener menos líneas de código y con más facilidad de introducir elementos nuevos escritos
por otras personas.

Al tratarse de un lenguaje compilado, presenta una buena eficiencia en tiempo de ejecución frente a los lenguajes interpretados.

En sistemas operativos basados en Linux, el lenguaje C++ se compila bajo el compilador GCC (GNU Compiler Collection).

Dentro del desarrollo software en C++ para ROS (roscpp \cite{roscpp}), existe una amplia interfaz para acceder a las diferentes funcionalidades y comunicarse con nodos desarrollados tanto en C++ como en Python.

\subsection{Lenguaje de programación Python}
Python es un lenguaje de programación interpretado cuya principal característica es que utiiza una sintaxis que favorece el código legible.

Se trata de un lenguaje de programación multiparadigma, ya que soporta orientación a objetos, programación imperativa y, en menor medida, programación funcional. Es un lenguaje interpretado, usa tipado dinámico y es multiplataforma.

Gracias a sus características, su uso es totalmente flexible y permite un tiempo de desarrollo menor principalmente por su tipado dinámico¡ y su sintaxis. Sin embargo, al tratarse de un lenguaje interpretado, el tiempo de ejecución es más alto lo cual no lo hace adecuado para tareas que requieran altos niveles de eficiencia.

Desntro del desarrollo en Python para ROS (rospy \cite{rospy}), existe una interfaz completa para comunicarse con los nodos y otras funcionalidades de ROS desarrolladas en Python o C++.

\subsection{Controlador de versiones git y repositiorios github}

Git es un software de control de versiones libre. Es decir, git
gestiona los archivos y directorios y los cambios hechos en ellos a lo largo del tiempo. Esto te permite recuperar antiguas revisiones del proyecto o ver tu historial de cambios.

Git fue creado pensando en la eficiencia y la confiabilidad del mantenimiento e versiones cuando estas tienen un gran número de archivos de código fuente. Tiene la capacidad de poder trabajar varias personas con el mismo paquete siempre que no modifiquen el mismo archivo, en ese caso, sería posible ver las diferencias entre ambas versiones, y unirlas o crear unarama del proyecto principal si fuera necesario tener las dos versiones.

GitHub es un sistema de almacenamiento público de código fuente (de cualquier tipo) o un servicio de respositorios. Su principal característica es la de ofrecer una plataforma de interacción social \cite{dabbish2012social}en la que distintas personas pueden trabajar en conjunto. Esto permite que varios desarrolladores contribuyan a un proyecto y trabajen de manera coordinada.

Tanto para el desarrollo software de este proyecto como para la redacción de esta memoria se han utilizado estas herramientas, y el acceso a los respositorios se ecuentra en las siguientes direcciones:

\begin{itemize}
	\item Desarrollo software del proyecto \url{https://github.com/danimtb/pioneer3at_ETSIDI}
	\item Memoria del proyecto \url{https://github.com/danimtb/TFG_pioneer3at}
\end{itemize}

\subsection{Simulador de robótica Gazebo}

Gazebo \cite{gazebo} es un simulador de robótica en tres dimensiones que ofrece la simulación de complejos entornos de diversas características, así como robots de todo tipo, su interacción con el entorno y la representación visual de datos obtenidos por diversos sensores como cámaras, láseres, ultrasonidos...

Un buen simulador de robótica es esencial para cualquier tipo de desarrollo robótico, ya que podemos realizar las pruebas software o la viabilidad de un sistema antes de construirlo. Gazebo cuenta con un potente motor de física simulada, interacción con objetos y dinámica de los mismos \cite{koenig2004design}.

Gazebo permite una integación completa con ROS, gestiona modelos físicos de robots utilizando el formato URDF (Unified Robot Description Format) \cite{urdf} y añade características específicas como el tipo de material, los momentos de inercia o el modelo de colisión. Además, incorpora plug-ins (funcionalidades añadidas) que permiten la simulación de robots de tipo diferencial, simulación de sensores y el cálculo de tranformadas entre los distintos sistemas de referencia.

Gazebo es mantenido y desarrollado actualmente por la Open Source Robotics Foundation \cite{osrf}.

\subsection{RViz: Robot Visualization tool}


\section{Hardware}
En esta parte se explica detalladamente el hardware empleado en el desarrollo del proyecto.

\subsection{Pioneer 3 AT}

El robot Pioneer 3 AT (Figura \ref{fig:pioneer3at}), perteneciente a la empresa Adept MobileRobots, es un robot de cuatro ruedas en configuración skid-steer y todo terreno (AT, All Terrain) de operación e investigación en laboratorio.\\

\begin{figure}[htp]
\centering
\includegraphics[width=0.4\textwidth]{figuras/pioneer_3_at.jpg}
\caption{Robot Pioneer 3-AT} \label{fig:pioneer3at}
\end{figure}

Su configuración en skid-steer permite un control relativamente simple utilizando el modo diferencial para poder realizar giros con gran maniobrabilidad, sin embargo, esta configuración depende mucho del tipo de suelo, con lo que se pierde precisión.

Este robot dispone de baterías, interruptor con parada de emergencia, dos motores de corriente continua para cada par de ruedas con transmisión mediante correa, encoders para leer la odometría y un microcontrolador con firmware ARCOS.

Ademas cuenta con un pequeño computador interno conectado al microcontrolador que puede utilizarse para realizar operaciones de manera autónoma.

El cuerpo del robot es de aluminio y su parte delantera así como superior es fácilmente desmontable para realizar las conexiones pertinentes y acceder al ordenador de a bordo y la placa microcontroladora. En la plataforma superior se sitúa el panel de control (Figura \ref{fig:panel_control})para acceder al ordenador de abordo conectando un monitor, teclado y ratón, puerto serial RS-232, botones de encendido y reset varios leds indicadores de estado y de envío y recepción de datos.

\begin{figure}[htp]
\centering
\includegraphics[width=0.6\textwidth]{figuras/panel_control.png}
\caption{Panel de control del robot Pioneer 3-AT} \label{fig:panel_control}
\end{figure}

En la siguiente tabla (Tabla \ref{tabla_pioneer3at}) se describen las principales características del robot.

% Please add the following required packages to your document preamble:
% \usepackage{graphicx}
\begin{table}[!h]
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{c c}
\hline
{\bf Especificaciones} & {\bf Pioneer 3 AT} \\ \hline
Largo & 508 mm \\
Ancho & 497 mm \\
Alto & 277 mm \\
Distancia al suelo & 80 mm \\
Peso & 12 kg \\
Carga útil & 32 kg \\
Cuerpo & Aluminio de 1.6 mm \\
Baterías & 3 de 12 V ~Ah, estancas, plomo-ácido \\
Autonomía & 4-8 horas \\
Sistema motriz & 4 ruedas motrices \\
Ruedas & Neumáticos de Nylon \\
Diámetro de rueda & 222 mm \\
Ancho de rueda & 88 mm \\
Sistema de giro & Diferencial \\
radio máxima curvatura & 40 cm \\
Radio de giro & 0 cm \\
Máxima velocidad de avance & 1.2 m/s \\
Máximo escalón & 10 cm \\
Máximo hueco & 15.2 cm \\
Terreno & Asfalto, Tierra, Césped, etc. \\
Encoders & 500 pulsos \\
Procesador & Hitachi H8S \\ \hline
\end{tabular}
}
\caption{Especificaciones del robot Pioneer 3 AT}
\label{tabla_pioneer3at}
\end{table}


\subsection{Sensor Kinect}

Kinect es un conjunto de sensores de bajo coste que lo convierte en una
herramienta excepcional (Figura \ref{fig:sensor_kinect}). Este dispositivo incluye una cámara de vídeo RGB, una cámara infrarroja de profundidad, un array de micrófonos y altavoces, un acelerómetro y un pequeño motor que le permite hacer movimientos de inclinación.

\begin{figure}[htp]
\centering
\includegraphics[width=0.6\textwidth]{figuras/sensor_kinect.png}
\caption{Sensor Kinect}
\label{fig:sensor_kinect}
\end{figure}

Su función principal es la de percibir el entorno captando una serie de puntos que se ubican en las tres dimensiones. Su funcionamiento a grandes rasgos se basa en un emisor de infrarrojos a 830 nm que interactñua con los objetos y una cámara infrarroja que etecta la diferencia entre la proyección anterior y la actual, obteniendo la distancia a cada objeto.

En primer lugar, el laser infrarrojo es emitido por Kinect con un patrón
determinado (Projected textures **REFERENCIA**), el cual no es simétrico sino que tiene puntos aleatorios que se dispersa gracias a unas lentes de proyección. Estos puntos aleatorios se reflejan en los objetos, los cuales sería posible verlos con una cámara externa.

A continuación, al sensor de Kinect MT9M001C12STM, que no es más que el
sensor CMOS de una cámara en la que se le trata para que observe solo el
infrarrojo, obteniendo los puntos infrarrojos en el plano 2D. El motivo por el que podemos medir la profundidad de los objetos (su distancia) es porque sabemos el patrón de cómo emite el laser emisor \cite{konolige2010projected}, por tanto sabremos que si un punto no está en el sitio que corresponde, se ha trasladado respecto al punto inicial y se le aplica la correspondiente transformación (Figura \ref{fig:proyecciones_kinect}), obteniendo finalmente los puntos de toda la nube en coordenadas cartesianas XYZ.\\

\begin{figure}[htp]
\centering
\includegraphics[width=0.8\textwidth]{figuras/proyecciones_kinect.png}
\caption{Proyección de infrarrojos y obtención de la nube de puntos}
\label{fig:proyecciones_kinect}
\end{figure}

La siguiente tabla (Tabla \ref{tabla_kinect}) muestra las especificaciones del sensor Kinect.

\begin{table}[!h]
\centering
\resizebox{\textwidth}{!}{
\begin{tabular}{c c}
\hline
{\bf Especificaciones} & {\bf Sensor Kinect} \\ \hline
Dimensiones del conjunto & 270mm x 50mm x 70mm \\
Fuente infrarroja & 830nm \\
Potencia & 60 mW \\
Cámara Infrarroja & MT9M001C12STM \\
Resolución cámara infrarroja & 1200x960 pixeles \\
Frecuencia & 30 Hz \\
Tamaño pixel & 5.2um x 5.2um  \\
Pixeles activos & 1280H x 1024V \\
Campo de visión & 58º H, 45º V, 70º D \\
Resolución espacial & 3mm (a 2 metros de distancia) \\
Resolución de profundidad & 1cm (a 2 metros de distancia) \\
Distancia de operación & 0.45m ? 6.5m \\
Cámara RGB & MT9M112 \\
Resolución cámara RGB & 640 x 480) \\
Audio & TAS1020B (Controlador de Audio) \\
Formato & 16kHz, 16-bit mono, modulación por
codificación de pulso (PCM)\\
Entrada de audio & 4 micrófonos con conversión analógico
digital de 24bits \\
Acelerómetro & KXSD9-2050\\ \hline
\end{tabular}
}
\caption{Características del sensor Kinect}
\label{tabla_kinect}
\end{table}

\subsection{Láser SICK LMS100}

Aunque el planteamiento incial del proyecto planteaba la navegación basada únicamente en el sensor kinect, debemos mencionar el uso del sensor láser Sick LMS100 (Figura \ref{fig:sensor_sicklms100}).

\begin{figure}[htp]
\centering
\includegraphics[width=0.6\textwidth]{figuras/SICK_LMS100.jpg}
\caption{Sensor escaner láser Sick LMS100}
\label{fig:sensor_sicklms100}
\end{figure}

Este es un sensor láser por infrarrojos de clase I (Inofensivo para el ojo humano), que obtiene la medida de distancias con gran preción y rapidez en un solo plano y realizando un barrido de 270º (Figura \ref{fig:sicklms100_rango}).

\begin{figure}[htp]
\centering
\includegraphics[width=0.6\textwidth]{figuras/sicklms100_rango.png}
\caption{Campo de visión del sensor láser Sick LMS100} \label{fig:sicklms100_rango}
\end{figure}

Este sensor está colocado en la parte trasera del robot, enfocando hacia atrás para cubrir un mayor rango y conocer todo el entorno alrededor del robot.

En la siguiente tabla (Tabla \ref{tabla_sicklms100}) se recogen sus características principales.

\begin{table}[!h]
\centering
\resizebox{\textwidth}{!}{
\begin{tabular}{c c}
\hline
{\bf Especificaciones} & {\bf Sick LMS100} \\ \hline
Campo de aplicación & Interno \\
Fuente infrarroja & 905 nm \\
Clase Láser & 1 (IEC 60825-1) \\
Campo de visión & 270º \\
Frecuencia de escaneo & 25Hz/50Hz \\
Resolución angular & 0.25º/0.5º \\
Distancia de operación & 0.05 - 20 m  \\
Tiempo de respuesta & 20 ms \\
Error & 30 mm \\
Interfaz de datos & Ethernet \\
Tensión de operación & 10.8V - 20V DC \\
Consumo & 20 W \\
Peso & 1.1 Kg \\
Dimensiones & 105mm x 102mm x 152mm\\ \hline
\end{tabular}
}
\caption{Características del sensor láser Sick LMS100. Basado en \cite{sicklms100}}
\label{tabla_sicklms100}
\end{table}

\subsection{Intel NUC NUC5i7RYH}

El ordenador Intel NUC NUC5i7RYH, es un ordenador de altas prestaciones y de tamaño compacto que ofrece unas buenas características para procesar datos y realizar la algoritmia adecuada para tareas de robótica.

Está equipado con un procesador Intel i7-5557U de quinta generación que ofrece una frecuencai de reloj de 3.1 GHz. Está incorporado con un discoduro de estado sólido que permite una alta velocidad de lectura y escritura en disco, así como una tarjeta RAM de tipo DDR3L de 8GB que permitirá el intercambio de información entre los nodos ROS de una manera fluida.

Su cometido será el de procesar la información de los sensores, generar los mapas incorporando los obstáculos, generar las trayectorias de navegación y comandar los motores del robot para realizar movimientos.

Dispone de tamaño compacto y un consumo bajo, juto con una alimentación a partir de los 12 voltios, lo que lo hace ideal para incorporarlo en robots móviles que requieran realizar tareas sin depender de una infraestructura.

En la tabla \ref{tabla_intelnuc} pueden consultarse sus características principales.

\begin{table}[!h]
\centering
\resizebox{\textwidth}{!}{
\begin{tabular}{c c}
\hline
{\bf Especificaciones} & {\bf Intel NUC NUC5i7RYH} \\ \hline
Procesador & Intel Core i7-5557U, dual-core \\
Frecuencia de reloj & 3.1 GHz hasta 3.4 GHz \\
Memoria RAM & DDR3L1 **DATO** \\
Disco duro & M.2 SSD **DATO** \\
Gráficos & Iris Graphics 6100 \\
Conectividad de periféricos & 2 x USB 3.0 en el panel posterior\\
 & 2 x USB 3.0 en el panel frontal\\
 & 2 x USB 2.0 internos vía colector\\
Conectividad de red & Intel 10/100/1000 Mbps\\
 & Intel® Wireless-AC 7265 M.2, antenas inalámbricas (IEEE 802.11ac)\\
Alimentación & 12-19V DC \\
Consumo & 65 W \\
Dimensiones & 115mm x 111mm x 48.7mm\\ \hline
\end{tabular}
}
\caption{Características del ordenador Intel NUC NUC5i7RYH}
\label{tabla_intelnuc}
\end{table}